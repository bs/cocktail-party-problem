1.
TITLE: The Cocktail Party Problem: What Is It? How Can It Be Solved? And Why Should Animal Behaviorists Study It?
AUTHORS: M. Bee, C. Micheyl
YEAR: 2008
SOURCE: Journal of Comparative Psychology
ABSTRACT: Animals often use acoustic signals to communicate in groups or social aggregations in which multiple individuals signal within a receiver's hearing range. Consequently, receivers face challenges related to acoustic interference and auditory masking that are not unlike the human cocktail party problem, which refers to the problem of perceiving speech in noisy social settings. Understanding the sensory solutions to the cocktail party problem has been a goal of research on human hearing and speech communication for several decades. Despite a general interest in acoustic signaling in groups, animal behaviorists have devoted comparatively less attention toward understanding how animals solve problems equivalent to the human cocktail party problem. After illustrating how humans and nonhuman animals experience and overcome similar perceptual challenges in cocktail-party-like social environments, this article reviews previous psychophysical and physiological studies of humans and nonhuman animals to describe how the cocktail party problem can be solved. This review also outlines several basic and applied benefits that could result from studies of the cocktail party problem in the context of animal acoustic communication.

2.
TITLE: How the Brain Separates Sounds
AUTHORS: R.P. Carlyon
YEAR: 2004
SOURCE: Trends in Cognitive Sciences
ABSTRACT: In everyday life we often listen to one sound, such as someone’s voice, in a background of competing sounds. To do this, we must assign simultaneously occurring frequency components to the correct source, and organize sounds appropriately over time. The physical cues that we exploit to do so are well-established; more recent research has focussed on the underlying neural bases, where most progress has been made in the study of a form of sequential organization known as ‘auditory streaming’. Listeners’ sensitivity to streaming cues can be captured in the responses of neurons in the primary auditory cortex, and in EEG wave components with a short latency (<200 ms). However, streaming can be strongly affected by attention, suggesting that this early processing either receives input from non-auditory areas, or feeds into processes that do.

3.
TITLE: Neural Basis of Hearing in Real-World Situations
AUTHORS: A.S. Feng, R. Ratnam
YEAR: 2000
SOURCE: Annual Review of Psychology
ABSTRACT: In real-world situations animals are exposed to multiple sound sources originating from different locations. Most vertebrates have little difficulty in attending to selected sounds in the presence of distractors, even though sounds may overlap in time and frequency. This chapter selectively reviews behavioral and physiological data relevant to hearing in complex auditory environments. Behavioral data suggest that animals use spatial hearing and integrate information in spectral and temporal domains to determine sound source identity. Additionally, attentional mechanisms help improve hearing performance when distractors are present. On the physiological side, although little is known of where and how auditory objects are created in the brain, studies show that neurons extract behaviorally important features in parallel hierarchically arranged pathways. At the highest levels in the pathway these features are often represented in the form of neural maps. Further, it is now recognized that descending auditory pathways can modulate information processing in the ascending pathway, leading to improvements in signal detectability and response selectivity, perhaps even mediating attention. These issues and their relevance to hearing in real- world conditions are discussed with respect to several model systems for which both behavioral and physiological data are available.

4.
TITLE: Lungs Contribute to Solving the Frog’s Cocktail Party Problem by Enhancing the Spectral Contrast of Conspecific Vocal Signals
AUTHORS: N. Lee, J. Christensen-Dalsgaard, L.A. White, K. Schrode, M. Bee
YEAR: 2020
SOURCE: bioRxiv
ABSTRACT: Noise impairs signal perception and is a major source of selection on animal communication. Identifying adaptations that enable receivers to cope with noise is critical to discovering how animal sensory and communication systems evolve. We integrated biophysical and bioacoustic measurements with physiological modeling to demonstrate that the lungs of frogs serve a heretofore unknown noise-control function in vocal communication. Lung resonance enhances the signal-to-noise ratio for communication by selectively reducing the tympanum’s sensitivity at critical frequencies where the tuning of two inner ear organs overlaps. Social network analysis of citizen-science data on frog calling behavior indicates the calls of other frog species in multi-species choruses are a prominent source of environmental noise attenuated by the lungs. These data reveal that an ancient adaptation for detecting sound via the lungs has been evolutionarily co-opted to create spectral contrast enhancement that contributes to solving a multi-species cocktail party problem.

5.
TITLE: Streaming of Repeated Noise in Primary and Secondary Fields of Auditory Cortex
AUTHORS: D. Saderi, B. Buran, S. David
YEAR: 2020
SOURCE: The Journal of Neuroscience
ABSTRACT: Statistical regularities in natural sounds facilitate the perceptual segregation of auditory sources, or streams. Repetition is one cue that drives stream segregation in humans, but the neural basis of this perceptual phenomenon remains unknown. We demonstrated a similar perceptual ability in animals by training ferrets of both sexes to detect a stream of repeating noise samples (foreground) embedded in a stream of random samples (background). During passive listening, we recorded neural activity in primary auditory cortex (A1) and secondary auditory cortex (posterior ectosylvian gyrus, PEG). We used two context-dependent encoding models to test for evidence of streaming of the repeating stimulus. The first was based on average evoked activity per noise sample and the second on the spectro-temporal receptive field. Both approaches tested whether differences in neural responses to repeating versus random stimuli were better modeled by scaling the response to both streams equally (global gain) or by separately scaling the response to the foreground versus background stream (stream-specific gain). Consistent with previous observations of adaptation, we found an overall reduction in global gain when the stimulus began to repeat. However, when we measured stream-specific changes in gain, responses to the foreground were enhanced relative to the background. This enhancement was stronger in PEG than A1. In A1, enhancement was strongest in units with low sparseness (i.e., broad sensory tuning) and with tuning selective for the repeated sample. Enhancement of responses to the foreground relative to the background provides evidence for stream segregation that emerges in A1 and is refined in PEG. SIGNIFICANCE STATEMENT To interact with the world successfully, the brain must parse behaviorally important information from a complex sensory environment. Complex mixtures of sounds often arrive at the ears simultaneously or in close succession, yet they are effortlessly segregated into distinct perceptual sources. This process breaks down in hearing-impaired individuals and speech recognition devices. By identifying the underlying neural mechanisms that facilitate perceptual segregation, we can develop strategies for ameliorating hearing loss and improving speech recognition technology in the presence of background noise. Here, we present evidence to support a hierarchical process, present in primary auditory cortex and refined in secondary auditory cortex, in which sound repetition facilitates segregation.

6.
TITLE: Song Overlapping, Noise, and Territorial Aggression in Great Tits
AUTHORS: C. Akcay, Y.K. Porsuk, A. Avşar, D. Sabuk, C.C. Bilgin
YEAR: 2020
SOURCE: International Society for Behavioral Ecology
ABSTRACT: Communication often happens in noisy environments where interference from the ambient noise and other signalers may reduce the effectiveness of signals which may lead to more conflict between interacting individuals. Signalers may also evolve behaviors to interfere with signals of opponents, for example, by temporally overlapping them with their own, such as the song overlapping behavior that is seen in some songbirds during aggressive interactions. Song overlapping has been proposed to be a signal of aggressive intent, but few studies directly examined the association between song overlapping and aggressive behaviors of the sender. In the present paper, we examined whether song overlapping and ambient noise are associated positively with aggressive behaviors. We carried out simulated territorial intrusions in a population of great tits (Parus major) living in an urban–rural gradient to assess signaling and aggressive behaviors. Song overlapping was associated negatively with aggressive behaviors males displayed against a simulated intruder. This result is inconsistent with the hypothesis that song overlapping is an aggressive signal in this species. Ambient noise levels were associated positively with aggressive behaviors but did not correlate with song rate, song duration, or song overlapping. Great tits in noisy urban habitats may display higher levels of aggressive behaviors due to either interference of noise in aggressive communication or another indirect effect of noise.

7.
TITLE: What Can Computational Models Learn From Human Selective Attention? A Review From an Audiovisual Unimodal and Crossmodal Perspective
AUTHORS: D. Fue, et al.
YEAR: 2020
SOURCE: Frontiers in Integrative Neuroscience
ABSTRACT: Selective attention plays an essential role in information acquisition and utilization from the environment. In the past 50 years, research on selective attention has been a central topic in cognitive science. Compared with unimodal studies, crossmodal studies are more complex but necessary to solve real-world challenges in both human experiments and computational modeling. Although an increasing number of findings on crossmodal selective attention have shed light on humans' behavioral patterns and neural underpinnings, a much better understanding is still necessary to yield the same benefit for intelligent computational agents. This article reviews studies of selective attention in unimodal visual and auditory and crossmodal audiovisual setups from the multidisciplinary perspectives of psychology and cognitive neuroscience, and evaluates different ways to simulate analogous mechanisms in computational models and robotics. We discuss the gaps between these fields in this interdisciplinary review and provide insights about how to use psychological findings and theories in artificial intelligence from different perspectives.

8.
TITLE: Signal Synchrony and Alternation Among Neighbor Males in a Japanese Stream Breeding Treefrog, Buergeria japonica
AUTHORS: H.D. Legett, I. Aihara, X.E. Bernal
YEAR: 2020
SOURCE: Current Herpetology
ABSTRACT: Animals that aggregate in leks to attract mates often time the production of their mating signals against the signals of neighboring conspecifics. Such signal timing usually falls into general patterns within these aggregations, which can be categorized based on the amount of overlap between the signals. In many species, individuals produce signals in an alternating pattern, avoiding signal overlap to reduce interference and increase mate attraction. In contrast, individuals in some species produce signals in synchrony, maximizing overlap and interference. The prevalence and function of signal synchronization is still unknown in many species. Here we examine the call timing strategies of the Ryukyu Kajika frog (Buergeria japonica). Using acoustic playback experiments we characterize a divergence in timing patterns between the two call types in this species, one produced in alternation and one in synchrony. Specifically, male B. japonica responded to playbacks of the first call type (Type I calls) with delayed Type I calls, avoiding overlap with the playbacks. In contrast, males responded to playbacks of the second call type (Type II calls) with synchronized Type II calls, overlapping their calls with the playbacks. Such variation in temporal signaling strategies within a species provides insights into how social and environmental pressures shape signal timings.

9.
TITLE: Insect Acoustic Communication: The Role of Transmission Channel and the Sensory System and Brain of Receivers
AUTHORS: H. Romer
YEAR: 2020
SOURCE: Functional Ecology
ABSTRACT: For decades, acoustic insects have been used as model organisms for behavioural neurobiologists to understand mate choice or predator avoidance, because behaviour can easily and reliably be elicited in the laboratory, and behaviourally relevant, identified nerve cells be studied under these conditions. However, signalling often takes place in complex environments, in which the signal perceived by the receiver may differ greatly from the one broadcast due to the biotic and abiotic properties of the sound transmission channel. Thus, the key challenge is to transfer the insights of these laboratory‐oriented experiments to more natural settings. Signal detection, identification and discrimination, as well as localization, are complicated by the transmission channel in several ways. Here, I review the empirical evidence from outdoor studies, demonstrating how excess attenuation reduces the active space and the information of a signal at some distance from the sender. At the same time, these frequency‐dependent processes allow to maintain acoustic distances to neighbours in a population. Insects often communicate within choruses of signallers of the same and different species, giving rise to high levels of acoustic masking interference. I discuss the evidence found for temporal or spatial partitioning of species in multispecies assemblages, and I show that solutions to the masking problem are based on a combination of adaptations in the behaviour of signallers and in the sensory system of receivers. Whether or not the perceived signal elicits a behaviour in receivers will depend on the design of the sensory system and the brain. I give examples for active mechanical processes in insect sensory receptors that influence the responses to external stimuli. In addition, neuronal filters in the frequency, intensity or time domain, and even the memory of individual receivers, provide the basis for adaptive receiver decision‐making in mate choice scenarios. Finally, I describe the advantages of having access to the relatively simple nervous systems of insects and how this access, combined with the use of a variety of behavioural tests, allows new insights into acoustic communication and its evolution. A free Plain Language Summary can be found within the Supporting Information of this article.

10.
TITLE: Integrating Form and Function in the Songbird Auditory Forebrain
AUTHORS: S.C. Woolley, S.M.N. Woolley
YEAR: 2020
SOURCE: Springer Handbook of Auditory Research
ABSTRACT: Vocal communication is critical for reproduction and survival across a wide range of species. For vocal communication systems to function, receivers must perform a range of auditory tasks to decode and process acoustic signals. In songbirds, learned vocal signals (songs) can be used by receivers to gain information about the species, sex, identity, and even motivation of the singer. Moreover, young songbirds must hear and memorize songs during development to use them as templates for song learning. This chapter reviews research on the structure and function of the songbird auditory system. In particular, the relationships between the organization, connections, and information-coding properties of the auditory pallium are described and how the functions of those circuits allow birds to perform a range of auditory tasks is considered, including individual recognition, tutor song learning, auditory memory, and mate choice processes.

11.
TITLE: Improving End-to-End Single-Channel Multi-Talker Speech Recognition
AUTHORS: W. Zhang, X. Chang, Y. Qian, S. Watanabe
YEAR: 2020
SOURCE: IEEE/ACM Transactions on Audio, Speech, and Language Processing
ABSTRACT: Although significant progress has been made in single-talker automatic speech recognition (ASR), there is still a large performance gap between multi-talker and single-talker speech recognition systems. In this article, we propose an enhanced end-to-end monaural multi-talker ASR architecture and training strategy to recognize the overlapped speech. The single-talker end-to-end model is extended to a multi-talker architecture with permutation invariant training (PIT). Several methods are designed to enhance the system performance, including speaker parallel attention, scheduled sampling, curriculum learning and knowledge distillation. More specifically, the speaker parallel attention extends the basic single shared attention module into multiple attention modules for each speaker, which can enhance the tracing and separation ability. Then the scheduled sampling and curriculum learning are proposed to make the model better optimized. Finally the knowledge distillation transfers the knowledge from an original single-speaker model to the current multi-speaker model in the proposed end-to-end multi-talker ASR structure. Our proposed architectures are evaluated and compared on the artificially mixed speech datasets generated from the WSJ0 reading corpus. The experiments demonstrate that our proposed architectures can significantly improve the multi-talker mixed speech recognition. The final system obtains more than 15% relative performance gains in both character error rate (CER) and word error rate (WER) compared to the basic end-to-end multi-talker ASR system.

12. 
TITLE: Modeling Active Sensing Reveals Echo Detection Even in Large Groups of Bats
AUTHORS: T. Beleyur, H. Goerlitz
YEAR: 2019
SOURCE: Proceedings of the National Academy of Sciences
ABSTRACT: Close-by active sensing animals may interfere with each other. We investigated what echolocators flying in a group hear—can they detect each other after all? We modeled perceptual and acoustic properties in group echolocation to quantify neighbor detection probability as group size increases. Echolocating bats can detect at least 1 of their closest neighbors per call up to group sizes of even 100 bats. Call parameters such as call rate and call duration also play a strong role in how much echolocators in a group interfere with each other. Even when many bats fly together, they are indeed able to detect at least their nearest frontal neighbors—and this prevents them from colliding into one another. Active sensing animals perceive their surroundings by emitting probes of energy and analyzing how the environment modulates these probes. However, the probes of conspecifics can jam active sensing, which should cause problems for groups of active sensing animals. This problem was termed the cocktail party nightmare for echolocating bats: as bats listen for the faint returning echoes of their loud calls, these echoes will be masked by the loud calls of other close-by bats. Despite this problem, many bats echolocate in groups and roost socially. Here, we present a biologically parametrized framework to quantify echo detection in groups. Incorporating properties of echolocation, psychoacoustics, acoustics, and group flight, we quantify how well bats flying in groups can detect each other despite jamming. A focal bat in the center of a group can detect neighbors in group sizes of up to 100 bats. With increasing group size, fewer and only the closest and frontal neighbors are detected. Neighbor detection is improved by longer call intervals, shorter call durations, denser groups, and more variable flight and sonar beam directions. Our results provide a quantification of the sensory input of echolocating bats in collective group flight, such as mating swarms or emergences. Our results further generate predictions on the sensory strategies bats may use to reduce jamming in the cocktail party nightmare. Lastly, we suggest that the spatially limited sensory field of echolocators leads to limited interactions within a group, so that collective behavior is achieved by following only nearest neighbors.

13.
TITLE: End-to-End Overlapped Speech Detection and Speaker Counting with Raw Waveform
AUTHORS: W. Zhang, M. Sun, L. Wang, Y. Qian
YEAR: 2019
SOURCE: IEEE Automatic Speech Recognition and Understanding Workshop
ABSTRACT: Overlapped speech processing has attracted more and more attention in recent years, and it is a key problem when processing multi-talker mixed speech under the cocktail party scenario. It is commonly observed that the performance of overlapped speech processing can be significantly improved if the number of speakers is given in advance. However, such prior knowledge is often unavailable in real-world conditions, so a robust overlapped speech detection and speaker counting system is demanded. Most existing works focus on combining different handcrafted features to tackle this task, which can be sub-optimal since there are no direct connections between the features and the task. In this work, we try to solve these two problems with an end-to-end manner. First, an end-to-end framework for overlapped speech detection and speaker counting is proposed, which extracts features from the raw waveform directly. Then a curriculum learning strategy is applied to make better use of the training data. The proposed methods are evaluated on multi-talker mixed speech generated from the LibriSpeech corpus. Experimental results show that our proposed methods outperform the model with handcrafted features on both tasks, achieving more than 2% and 4% absolute accuracy improvement on overlapped speech detection and speaker counting respectively.

14.
TITLE: Spatial Mixing between Calling Males of Two Closely Related, Sympatric Crickets Suggests Beneficial Heterospecific Interactions in a NonAdaptive Radiation.
AUTHORS: M. Xu, K.L. Shaw
YEAR: 2019
SOURCE: The Journal of Heredity
ABSTRACT: Sympatry among closely related species occurs in both adaptive and nonadaptive radiations. Among closely related, sympatric species of a nonadaptive radiation, the lack of ecological differentiation brings species into continual contact where individuals are exposed to the risk of reproductive interference. Selection thus should cause divergence in multiple components mediating the reproductive boundary. Besides differentiation of reproductive signals per se, spatial segregation is a commonly proposed mechanism that can mitigate reproductive interference. Studying a pair of broadly sympatric, closely related cricket species from a nonadaptive radiation in Hawaii, we 1) quantified acoustic divergence of male songs and 2) tested alternative hypotheses of spatial distribution of calling males of the 2 species. Acoustic analyses of the recorded songs showed that, while the 2 species differed substantially in pulse rate, no spectral or fine temporal segregation of the pulse structure was evident, indicating the potential for acoustic masking. Moreover, we found that calling males of the 2 species are highly mixed both vertically and horizontally and showed the same preference for calling sites. More surprisingly, calling males were found to form mixed-species calling clusters where heterospecific males are closer to each other than conspecific males. Such an individual spacing pattern suggests low heterospecific aggression and/or high conspecific competition. Because females prefer higher sound intensity, heterospecific males may benefit, rather than interfere, with each other in attracting females. These findings offer a potential mechanism enabling species coexistence in sympatry.

15.
TITLE: Temporal Integration of Conflicting Directional Cues in Sound Localization
AUTHORS: M.S. Reichert, B. Ronacher
YEAR: 2019
SOURCE: Journal of Experimental Biology
ABSTRACT: Sound localization is fundamental to hearing. In nature, sound degradation and noise erode directional cues and can generate conflicting directional perceptions across different subcomponents of sounds. Little is known about how sound localization is achieved in the face of conflicting directional cues in non-human animals, although this is relevant for many species in which sound localization in noisy conditions mediates mate finding or predator avoidance. We studied the effects of conflicting directional cues in male grasshoppers, Chorthippus biguttulus, which orient towards signaling females. We presented playbacks varying in the number and temporal position of song syllables providing directional cues in the form of either time or amplitude differences between two speakers. Males oriented towards the speaker broadcasting a greater number of leading or louder syllables. For a given number of syllables providing directional information, syllables with timing differences at the beginning of the song were weighted most heavily, while syllables with intensity differences were weighted most heavily when they were in the middle of the song. When timing and intensity cues conflicted, the magnitude and temporal position of each cue determined their relative influence on lateralization, and males sometimes quickly corrected their directional responses. We discuss our findings with respect to similar results from humans. Summary: In grasshoppers, the localization of sounds with ambiguous or conflicting directional cues depends on the magnitude and temporal position of each cue. This gives insight into general principles of directional hearing.

16.
TITLE: Active Sensing in Groups: (What) Do Bats Hear in the Sonar Cocktail Party Nightmare?
AUTHORS: T. Beleyur, H. Goerlitz
YEAR: 2019
SOURCE: bioRxiv
ABSTRACT: Active sensing animals perceive their surroundings by emitting probes of energy and analyzing how the environment modulates these probes. However, the probes of conspecifics can jam active sensing, which should cause problems for groups of active sensing animals. This problem was termed the cocktail party nightmare for echolocating bats: as bats listen for the faint returning echoes of their loud calls, these echoes will be masked by the loud calls of other close-by bats. Despite this problem, many bats echolocate in groups and roost socially. Here, we present a biologically parametrized framework to quantify echo detection in groups. Incorporating known properties of echolocation, psychoacoustics, spatial acoustics and group flight, we quantify how well bats flying in groups can detect each other despite jamming. A focal bat in the center of a group can detect neighbors for group sizes of up to 100 bats. With increasing group size, fewer and only the closest and frontal neighbors are detected. Neighbor detection is improved for longer call intervals, shorter call durations, denser groups and more variable flight and sonar beam directions. Our results provide the first quantification of the sensory input of echolocating bats in collective group flight, such as mating swarms or emergences. Our results further generate predictions on the sensory strategies bats may use to reduce jamming in the cocktail party nightmare. Lastly, we suggest that the spatially limited sensory field of echolocators leads to limited interactions within a group, so that collective behavior is achieved by following only nearest neighbors. SIGNIFICANCE STATEMENT Close-by active sensing animals may interfere with each other. We investigated if and what many echolocators fly in a group hear – can they detect each other after all? We modelled acoustic and physical properties in group echolocation to quantify neighbor detection probability as group size increases. Echolocating bats can detect at least one of their closest neighbors per call up to group sizes of even 100 bats. Call parameters such as call rate and call duration play a strong role in how much echolocators in a group interfere with each other. Even when many bats fly together, they are indeed able to detect at least their nearest frontal neighbors – and this prevents them from colliding into one another.

17.
TITLE: Neural Mechanisms of Auditory Species Recognition in Birds
AUTHORS: M.I.M. Louder, S.L. Lawson, K.S. Lynch, C. Balakrishnan, M. Hauber
YEAR: 2019
SOURCE: Biological Reviews of the Cambridge Philosophical Society
ABSTRACT: Auditory communication in humans and other animals frequently takes place in noisy environments with many co-occurring signallers. Receivers are thus challenged to rapidly recognize salient auditory signals and filter out irrelevant sounds. Most bird species produce a variety of complex vocalizations that function to communicate with other members of their own species and behavioural evidence broadly supports preferences for conspecific over heterospecific sounds (auditory species recognition). However, it remains unclear whether such auditory signals are categorically recognized by the sensory and central nervous system. Here, we review 53 published studies that compare avian neural responses between conspecific versus heterospecific vocalizations. Irrespective of the techniques used to characterize neural activity, distinct nuclei of the auditory forebrain are consistently shown to be repeatedly conspecific selective across taxa, even in response to unfamiliar individuals with distinct acoustic properties. Yet, species-specific neural discrimination is not a stereotyped auditory response, but is modulated according to its salience depending, for example, on ontogenetic exposure to conspecific versus heterospecific stimuli. Neuromodulators, in particular norepinephrine, may mediate species recognition by regulating the accuracy of neuronal coding for salient conspecific stimuli. Our review lends strong support for neural structures that categorically recognize conspecific signals despite the highly variable physical properties of the stimulus. The available data are in support of a 'perceptual filter'-based mechanism to determine the saliency of the signal, in that species identity and social experience combine to influence the neural processing of species-specific auditory stimuli. Finally, we present hypotheses and their testable predictions, to propose next steps in species-recognition research into the emerging model of the neural conceptual construct in avian auditory recognition.

18.
TITLE: A GPU Implementation of FastICA in Audio Applications for Small Number of Components
AUTHORS: S. Kanan, M. Gusev, V. Zdraveski
YEAR: 2019
SOURCE: BCI'19: Proceedings of the 9th Balkan Conference on Informatics
ABSTRACT: Extracting independent components from audio data has plenty of uses in biology, music, communication and media and in many other fields. The FastICA algorithm is a relatively fast and simple algorithm, that assumes the original sources to be nongaussian and works by lowering the gaussianity of the mixed sources. Yet as the number of samples increase so does the time required for its execution. While one solution would be to simply use just a subset of the samples, in this paper we look at the possibility of extending the FastICA algorithm to the GPU. While similar efforts have been pursued in the past, we deal with data of relatively few components, as would be more common when dealing with audio data. We implement a fully GPU FastICA as well as a CPU-GPU hybrid algorithm, both based on the CUDA platform and compare them with the CPU version. Our results indicate that for large samples the CPU-GPU hybrid and the GPU algorithms perform better than their CPU counterpart.

19.
TITLE: A Test of the Matched Filter Hypothesis in Two Sympatric Frogs, Chiromantis Doriae and Feihyla Vittata
AUTHORS: T. Yang, B.Zhu, J. Wang, S. Brauth, Y. Tang, J. Cui
YEAR: 2018
SOURCE: The International Journal of Animal Sound and its Recording
ABSTRACT: The matched filter hypothesis proposes that the auditory sensitivity of receivers should match the spectral energy distribution of the senders’ signals. If so, receivers should be able to distinguish between species-specific and hetero-specific signals. We tested the matched filter hypothesis in two sympatric species, Chiromantis doriae and Feihyla vittata, whose calls exhibit similar frequency characters and that overlap in the breeding season and microenvironment. For both species, we recorded male calls and measured the auditory sensitivity of both sexes using the auditory brainstem response (ABR). We compared the auditory sensitivity with the spectral energy distribution of the calls of each species and found that (1) auditory sensitivity matched the signal spectrogram in C. doriae and F. vittata; (2) the concordance conformed better to the conspecific signal versus the hetero-specific signal. In addition, our results show that species differences are larger than sex differences for ABR audiograms.

20.
TITLE: Simultaneously Vocalizing Asian Barbets Adopt Different Frequencies Without Coordinating Temporal Rhythms
AUTHORS: A. Krishnan
YEAR: 2019
SOURCE: bioRxiv
ABSTRACT: Sound stream segregation is an important challenge faced by simultaneously vocalizing animals. In duetting passerine birds, coordinating vocal timing helps minimize overlap. Alternatively, in birds that do not coordinate their vocalizations, sound stream segregation may involve other mechanisms. For example, birds are known to use frequency differences to segregate sound streams, and vocalizing at different frequencies may enable them to remain distinct from each other. Here, I present data showing that conspecific individuals of four species of Asian barbets vocalize at distinctly different peak frequencies from each other. Additionally, they also differ in repetition rate such that each species exhibits two peaks in frequency-repetition rate space. However, conspecific individuals across species do not temporally coordinate with each other during vocal interactions, maintaining independent and highly stereotyped individual rhythms together with different peak frequencies. Frequency differences between individuals may facilitate sound stream segregation when calls overlap in time. I hypothesize that simple, uncoordinated temporal rhythms with different frequencies may have given rise to the more complex coordination seen in duetting birds.

21. 
TITLE: A Physiologically Inspired Model for Solving the Cocktail Party Problem
AUTHORS: K.F. Chou, J. Dong, H. Colburn, K. Sen
YEAR: 2019
SOURCE: Journal of the Association for Research in Otolaryngology
ABSTRACT: At a cocktail party, we can broadly monitor the entire acoustic scene to detect important cues (e.g., our names being called, or the fire alarm going off), or selectively listen to a target sound source (e.g., a conversation partner). It has recently been observed that individual neurons in the avian field L (analog to the mammalian auditory cortex) can display broad spatial tuning to single targets and selective tuning to a target embedded in spatially distributed sound mixtures. Here, we describe a model inspired by these experimental observations and apply it to process mixtures of human speech sentences. This processing is realized in the neural spiking domain. It converts binaural acoustic inputs into cortical spike trains using a multi-stage model composed of a cochlear filter-bank, a midbrain spatial-localization network, and a cortical network. The output spike trains of the cortical network are then converted back into an acoustic waveform, using a stimulus reconstruction technique. The intelligibility of the reconstructed output is quantified using an objective measure of speech intelligibility. We apply the algorithm to single and multi-talker speech to demonstrate that the physiologically inspired algorithm is able to achieve intelligible reconstruction of an “attended” target sentence embedded in two other non-attended masker sentences. The algorithm is also robust to masker level and displays performance trends comparable to humans. The ideas from this work may help improve the performance of hearing assistive devices (e.g., hearing aids and cochlear implants), speech-recognition technology, and computational algorithms for processing natural scenes cluttered with spatially distributed acoustic objects.

22.
TITLE: Spatially-Mediated Call Pattern Recognition and the Cocktail Party Problem in Treefrog Choruses: Can Call Frequency Differences Help During Signal Overlap?
AUTHORS: J.J. Schwartz, M.E.S. Del Monte
YEAR: 2019
SOURCE: Bioacoustics
ABSTRACT: Male gray treefrogs, Hyla versicolor, advertise for mates in dense assemblages characterized by high levels of noise and acoustic clutter. In pairwise interactions, males alternate pulsatile advertisement calls and so reduce call overlap to levels below that expected by chance. However, in choruses consisting of more than two males, acoustic interference increases dramatically. Moreover, males do not seem to exhibit selective attention in a way that reduces call interference among nearest neighbours. Previous research has also demonstrated that although females discriminate strongly against overlapped calls, negative effects of call overlap can be attenuated by a large angular separation between signal sources. However, call stimuli employed were identical in spectrum and so this situation differs from that likely in nature. Based on studies of ‘auditory stream segregation’ with other taxa, we hypothesized that realistic differences in the frequencies of overlapping calls could improve the ability of females to discern critical call features during overlap of separated call sources. We found that, although, under some circumstances, differences in call frequency may help females distinguish among neighbouring males giving temporally proximate calls, naturalistic spectral differences do not seem to help females perceptually separate the overlapping calls of neighbouring conspecific males.

23.
TITLE: Predator Eavesdropping in a Mixed-Species Environment: How Prey Species May Use Grouping, Confusion, and the Cocktail Party Effect to Reduce Predator Detection
AUTHORS: E. Goodale, G. Ruxton, G. Beauchamp
YEAR: 2019
SOURCE: Frontiers in Ecology and Evolution
ABSTRACT: The field of predator eavesdropping concentrates on the detection by a predator or parasite of signals that prey direct at conspecifics, and the subsequent evolution by prey to avoid or lessen such detection. Here, we first point out that signaling prey species are often found in mixed-species moving groups or stationary aggregations, and ask the question of how simultaneous signaling, by members of one species or more, might affect predator eavesdropping behavior and the composition of the groups themselves. The detection risk of prey species will be affected by the other species they associate with, and prey should generally avoid joining a group with more detectable species. Yet prey may select to join other species that are preferred by predators, diluting their own risk of attack, as long as that does not lead to substantially greater detection and thereby increased predation. We next review the evidence that prey grouping and collective responses when attacked can confuse predators, leading to lower capture rates. Evidence for this confusion effect mostly involves visually orienting predators. We then ask if a similar phenomenon could occur when animals in a group simultaneously send acoustic signals, and find relevant evidence for predator confusion under such situations in the literature associated with the “cocktail party effect”. As confusion is heightened by similarities among mixed-species group members, this provides a force at ecological or evolutionary timescales to make species that associate in groups, and their signals, more similar to each other. However, heterogeneous mixed-species groups may be favored if species are differentially preferred as prey. We suggest experiments to examine whether the success rates of acoustically orienting predators depend on the group size of their mixed-species prey. More observations on the relative positions of conspecifics and heterospecifics in space, and the temporal association of their signals, will also increase our understanding of the relationship between mixed-species grouping and predator eavesdropping.

24.
TITLE: The Use of Spatial Cues by Hearing-Impaired Listeners in Complex Listening Environments
AUTHORS: B. Bardsley
YEAR: 2019
SOURCE:
ABSTRACT: People with hearing loss often struggle to understand speech in the presence of  background noise. The features of the interfering noise source, and where the sounds are  located in the environment with reference to the listener’s position, influence the degree  of difficulties faced by the listener.  Those with hearing-impairment demonstrate a substantial loss in the intelligibility  of speech in the presence of interfering noise sources, when compared to their normal-  hearing counterparts. The ability to use dips in the envelope of the interfering source,  fundamental frequency and spatial processing all become less useful for the person with  hearing loss. This deficit cannot simply be explained by a reduction in audibility.  Lip-reading provides an important cue for speech intelligibility and the advice  given to those with hearing loss is to face the speaker of interest directly. The Jelfs et al.  (2011) model predicts substantial benefit of orientating the head away from the target  speech in normal-hearing listeners. This has since been confirmed by Grange and  Culling (2016) in normal-hearing listeners. Hearing-impaired listeners were found to  gain a similar level of benefit by orientating the head away from the target. This has  ramifications for audiology services and the advice provided by clinicians. When  interference comes predominantly from one side, the listener is advised to turn the head  30 degrees away from the target talker and toward the interferers.  A major problem with studying the hearing-impaired population is the variability  in performance on measures that underpin speech intelligibility in noise. This causes an  issue when making comparisons with the normal-hearing population. An approach to  examining the relevance of individual performance measures to overall speech  intelligibility is to make a median split based on each measure, and then compare the  speech-in-noise performance of the resulting groups. Measures of binaural temporal fine  structure processing, processing of level and time differences independently and  contralateral masking were used to explore the factors that lead to poor use of spatial  cues by hearing-impaired listeners. No explanation for the poor use of spatial cues could  be found in these measures.

25. 
TITLE: Dynamic Adaptations in the Echolocation Behavior of Bats in Response to Acoustic Interference
AUTHORS: M. Beetz, M. Kossl, J.C. Hechavarria
YEAR: 2019
SOURCE: bioRxiv
ABSTRACT: Animals must extract relevant sensory information out of a multitude of non-informative and sometimes interfering stimuli. For orientation, bats rely on broadcasted calls and they must assign each echo to the corresponding call. When bats orient in acoustically enriched environments, call-echo assignment becomes challenging due to signal interference. Bats often adapt echolocation parameters which potentially improves signal extraction. However, they also adjust echolocation parameters with respect to target distance. To characterize adaptations that are exclusively elicited to minimize signal interference, we tested the effect of acoustic playback on the echolocation behavior of the fruit-eating bat, Carollia perspicillata. Hereby, distance-dependent changes were considered by swinging bats in a pendulum and directly measuring the object distance. Acoustic playback evoked different call adjustments in parameters such as bandwidth, peak-frequency, duration and call level. These adaptations were highly dynamic and could vary across individuals, days, trials, and even within trials. Our results demonstrate that bats do not only change one echolocation parameter when orienting in acoustically enriched environments. They rather have a tool-kit of different behavioral adaptations to cope with interfering acoustic stimuli. By dynamically switching between different adaptations, bats can maximize the extraction of their biosonar signals from the background. 

26. 
TITLE: Auditory Information Loss in Real-World Listening Environments
AUTHORS: A. Weisser
YEAR: 2019
SOURCE: arXiv
ABSTRACT: Whether animal or speech communication, environmental sounds, or music -- all sounds carry some information. Sound sources are embedded in acoustic environments that contain any number of additional sources that emit sounds that reach the listener's ears concurrently. It is up to the listener to decode the acoustic informational mix, determine which sources are of interest, decide whether extra resources should be allocated to extracting more information from them, or act upon them. While decision making is a high-level process that is accomplished by the listener's cognition, selection and elimination of acoustic information is manifest along the entire auditory system, from periphery to cortex. This review examines latent informational paradigms in hearing research and demonstrates how several hearing mechanisms conspire to gradually eliminate information from the auditory sensory channel. It is motivated through the computational need of the brain to decomplexify unpredictable real-world signals in real time. Decomplexification through information loss is suggested to constitute a unifying principle of the mammalian hearing system, which is specifically demonstrated in human hearing. This perspective can be readily generalised to other sensory modalities.

27.
TITLE: The Social Functions of Complex Vocal Sequences in Wild Geladas
AUTHORS: M.L. Gustison, E.T. Johnson, J.C. Beehner, T.J. Bergman
YEAR: 2019
SOURCE: Behavioral Ecology and Sociobiology
ABSTRACT: Several studies show that highly social taxa produce relatively more complex vocalizations. Yet, very few of these cases have demonstrated the function that vocal complexity plays within a highly social setting. Here, we assess potential functions of vocal complexity in male geladas (Theropithecus gelada) living in the Simien Mountains National Park, Ethiopia. Geladas are known for both their diverse vocalizations (routinely produced in long sequences) and their complex social structure (extremely large groups and long-term male-female bonds). We tested whether sequence complexity (i.e., including elaborate “derived” calls that are unique to geladas and absent in closely related taxa) or size (i.e., number of calls) may function (1) to counteract the challenges of living in a large group (overcoming conspecific noise and crowding, maintaining cohesion), or (2) to maintain social bonds with females. We found that an increase in conspecific noise contributed to the use of longer and more complex sequences. However, behavioral contexts in which the risk of separation was highest (i.e., traveling) were associated with only longer (but not more complex) sequences. We also found that sequence complexity (but not size) was associated with male-female bonding as complex call sequences were produced primarily when males were in close proximity to and approached females, and they led to males being groomed by females. Together, these findings suggest that, while a noisy backdrop of conspecific vocalizations might contribute to vocal complexity, the potential driver of gelada vocal complexity is the need to maintain cross-sex bonds.Significance statementWhy do some animals make many diverse sounds while others make only a few simple sounds? Broad comparisons suggest that sociality may be important as more social species (e.g., those with large group size and social bonding) tend to make more types of sounds. Yet, it remains unclear why gregarious species need an expanded call repertoire. Here, we take advantage of previous work on a highly social primate (geladas) that identified several complex vocalizations that contribute to gelada’s expanded vocal repertoire. To better understand why geladas evolved an expanded set of calls, we focus on the context where complex calls are produced and the responses those calls elicit. We found that the potential driver of the use of more call types is the need to maintain cross-sex bonds, suggesting an important role for male-female bonds in the evolution of vocal complexity.

28.
TITLE: Behavioral and Neural Auditory Thresholds in Frogs
AUTHORS: R.C. Taylor, K. Akre, W. Wilczynski, M. Ryan
YEAR: 2019
SOURCE: Current Zoology
ABSTRACT: Vocalizations play a critical role in mate recognition and mate choice in a number of taxa, especially, but not limited to, orthopterans, frogs, and birds. But receivers can only recognize and prefer sounds that they can hear. Thus a fundamental question linking neurobiology and sexual selection asks—what is the threshold for detecting acoustic sexual displays? In this study, we use 3 methods to assess such thresholds in túngara frogs: behavioral responses, auditory brainstem responses, and multiunit electrophysiological recordings from the midbrain. We show that thresholds are lowest for multiunit recordings (ca. 45 dB SPL), and then for behavioral responses (ca. 61 dB SPL), with auditory brainstem responses exhibiting the highest thresholds (ca. 71 dB SPL). We discuss why these estimates differ and why, as with other studies, it is unlikely that they should be the same. Although all of these studies estimate thresholds they are not measuring the same thresholds; behavioral thresholds are based on signal salience whereas the 2 neural assays estimate physiological thresholds. All 3 estimates, however, make it clear that to have an appreciation for detection and salience of acoustic signals we must listen to those signals through the ears of the receivers. 

29. 
TITLE: Explaining Human Auditory Scene Analysis Through Bayesian Clustering
AUTHORS: N. Larigaldie, U.R. Beierholm
YEAR: 2019
SOURCE: 2019 Conference on Cognitive Computational Neuroscience
ABSTRACT: The way auditory stimuli are being processed to form perceptual unitary or segregated groups of sounds is still an ongoing discussion in the Auditory Scene Analysis literature. Mechanistic approaches to model this phenomenon have been somewhat successful but are often overly complicated and constrained to specific paradigms. Our approach is that of simplicity. We have previously proposed a higher-level source inference model in the Bayesian statistical framework that only implements a few simple but sensible rules applied to the stimuli’s statistics. Yet, it still captures results from behavioral data (Yates, Larigaldie, & Beierholm, 2017). We have expanded on this model to show its ability to adapt to a wider range of well-known perceptual auditory phenomena. Several original experiments have also been conducted to explore a broader range of stimuli statistics. Our model’s responses give insight into possible underlying processes in the brain that could provide a guide towards more behavioral experiments or medical exploration.

30.
TITLE: Stimulus-Based and Task-Based Attention Modulate Auditory Stream Segregation Context Effects
AUTHORS: B.D. Yerkes, D.M. Wientraub, J.S. Snyder
YEAR: 2019
SOURCE: Journal of Experimental Psychology: Human Perception and Performance
ABSTRACT: Previous studies have shown that perceptual segregation increases after listening to longer tone sequences, an effect known as buildup. More recently, an effect of prior frequency separation (deltaf) has been discovered: presenting tone sequences with a small deltaf biases following sequences with an intermediate &Dgr;ƒ to be segregated into two separate streams, whereas presenting context sequences with a large deltaf biases following sequences to be integrated into one stream. Here we investigated how attention and task demands influenced these effects of prior stimuli by having participants perform one of three tasks during the context: making streaming judgments on the tone sequences, detecting amplitude modulation in the tones, and performing a visual task while ignoring the tones. Results from two experiments showed that although the effect of prior deltaf was present across all conditions, the effect was reduced whenever streaming judgments were not made during the context. Experiment 2 showed that streaming was reduced during the beginning of a test sequence only when participants performed the visual task during the context. These experiments suggest that task-based and stimulus-based attention differentially affect distinct influences of prior stimuli, and are consistent with the contribution of distinct levels of processing that affect auditory segregation.

31.
TITLE: Application of Independent Component Analysis for Infant Cries Separation
AUTHORS: C.Y. Chang, C.J. Chen, C.J Chen
YEAR: 2018
SOURCE: International Conference on Network-Based Information Systems
ABSTRACT: The research on analysing infant crying has received many attentions in recent years. In our prior work, a baby crying translation method called infant crying translator was proposed and showed high recognition accuracy. However, in a real environment, there may be more than one baby crying. These mixed cries will seriously affect the accuracy of recognition. In order to isolate these mixed cries, the independent component analysis was adopted herein. Experimental results show that the proposed method can separate out the mixed cries and greatly improves the recognition rate of infant crying translator. The recognition rate increased from 34% to 68%.

32.
TITLE: Robustness of Cortical and Subcortical Processing in the Presence of Natural Masking Sounds
AUTHORS:M. Beetz, G. Garcia-rosales, M. Kossl, J.C. Hechavarria
YEAR: 2018
SOURCE: Scientific Reports
ABSTRACT: Processing of ethologically relevant stimuli could be interfered by non-relevant stimuli. Animals have behavioral adaptations to reduce signal interference. It is largely unexplored whether the behavioral adaptations facilitate neuronal processing of relevant stimuli. Here, we characterize behavioral adaptations in the presence of biotic noise in the echolocating bat Carollia perspicillata and we show that the behavioral adaptations could facilitate neuronal processing of biosonar information. According to the echolocation behavior, bats need to extract their own signals in the presence of vocalizations from conspecifics. With playback experiments, we demonstrate that C. perspicillata increases the sensory acquisition rate by emitting groups of echolocation calls when flying in noisy environments. Our neurophysiological results from the auditory midbrain and cortex show that the high sensory acquisition rate does not vastly increase neuronal suppression and that the response to an echolocation sequence is partially preserved in the presence of biosonar signals from conspecifics.

33.
TITLE: Inharmonic Speech Reveals the Role of Harmonicity in the Cocktail Party Problem
AUTHORS: S. Popham, D. Boebinger, D. Ellis, H. Kawahara, J. McDermott
YEAR: 2018
SOURCE: Nature Communications
ABSTRACT: The “cocktail party problem” requires us to discern individual sound sources from mixtures of sources. The brain must use knowledge of natural sound regularities for this purpose. One much-discussed regularity is the tendency for frequencies to be harmonically related (integer multiples of a fundamental frequency). To test the role of harmonicity in real-world sound segregation, we developed speech analysis/synthesis tools to perturb the carrier frequencies of speech, disrupting harmonic frequency relations while maintaining the spectrotemporal envelope that determines phonemic content. We find that violations of harmonicity cause individual frequencies of speech to segregate from each other, impair the intelligibility of concurrent utterances despite leaving intelligibility of single utterances intact, and cause listeners to lose track of target talkers. However, additional segregation deficits result from replacing harmonic frequencies with noise (simulating whispering), suggesting additional grouping cues enabled by voiced speech excitation. Our results demonstrate acoustic grouping cues in real-world sound segregation.Harmonicity is associated with a single sound source and may be a useful cue with which to segregate the speech of multiple talkers. Here the authors introduce a method for perturbing the constituent frequencies of speech and show that violating harmonicity degrades intelligibility of speech mixtures.

34.
TITLE: Processing of Interaural Phase Differences in Components of Harmonic and Mistuned Complexes in the Inferior Colliculus of the Mongolian Gerbil
AUTHORS: L. Eipert, A. Klinge-Strahl, G.M. Klump
YEAR:2019
SOURCE: European Journal of Neuroscience
ABSTRACT: Harmonicity and spatial location provide eminent cues for the perceptual grouping of sounds. In general, harmonicity is a strong grouping cue. In contrast, spatial cues such as interaural phase or time difference provide for strong grouping of stimulus sequences but weak grouping for simultaneously presented sounds. By studying the neuronal basis underlying the interaction of these cues in processing simultaneous sounds using van Rossum spike train distance measures, we aim at explaining the interaction observed in psychophysical experiments. Responses to interaural phase differences imposed on single components of harmonic and mistuned complex tones as well as noise delay functions were recorded as multiunit responses from the inferior colliculus of Mongolian gerbils. Results revealed a better representation of interaural phase differences if imposed on a harmonic rather than a mistuned frequency component of a complex tone. The representation of interaural phase differences was better for long integration-time windows approximately reflecting firing rates rather than short integration-time windows reflecting the temporal pattern of the stimulus-driven response. We found only a weak impact of interaural phase differences if combined with mistuning of a component in a harmonic tone complex.

35.
TITLE: Schema Learning for the Cocktail Party Problem
AUTHORS: 2018
YEAR: K.J.P. Woods, J. McDermott
SOURCE: Proceedings of the National Academy of Sciences
ABSTRACT: The “cocktail party problem” is encountered when sounds from different sources in the world mix in the air before arriving at the ear, requiring the brain to estimate individual sources from the received mixture. Sounds produced by a given source often exhibit consistencies in structure that might be useful for separating sources if they could be learned. Here we show that listeners rapidly learn the abstract structure shared by sounds from novel sources and use the learned structure to extract these sounds when they appear in mixtures. The involvement of learning and memory in our ability to hear one sound among many opens an avenue to understanding the role of statistical regularities in auditory scene analysis. The cocktail party problem requires listeners to infer individual sound sources from mixtures of sound. The problem can be solved only by leveraging regularities in natural sound sources, but little is known about how such regularities are internalized. We explored whether listeners learn source “schemas”—the abstract structure shared by different occurrences of the same type of sound source—and use them to infer sources from mixtures. We measured the ability of listeners to segregate mixtures of time-varying sources. In each experiment a subset of trials contained schema-based sources generated from a common template by transformations (transposition and time dilation) that introduced acoustic variation but preserved abstract structure. Across several tasks and classes of sound sources, schema-based sources consistently aided source separation, in some cases producing rapid improvements in performance over the first few exposures to a schema. Learning persisted across blocks that did not contain the learned schema, and listeners were able to learn and use multiple schemas simultaneously. No learning was evident when schema were presented in the task-irrelevant (i.e., distractor) source. However, learning from task-relevant stimuli showed signs of being implicit, in that listeners were no more likely to report that sources recurred in experiments containing schema-based sources than in control experiments containing no schema-based sources. The results implicate a mechanism for rapidly internalizing abstract sound structure, facilitating accurate perceptual organization of sound sources that recur in the environment.

36.
TITLE: New Roles for Attention and Memory in the Cocktail Party Problem
AUTHORS: K.J.P. Woods
YEAR: 2018
SOURCE: Digital Access to Scholarship at Harvard
ABSTRACT: Listening to one sound source among many (‘the cocktail party problem’) is particularly difficult when competing sources are similar and change over time. Such sources may not have consistent features (e.g., high vs. low pitch) that easily distinguish them and promote separation, but attention and memory may help. This thesis demonstrates novel behavioral phenomena reflecting mechanisms of dynamic attention and rapid memory formation that can aid scene analysis under these challenging conditions. A first set of experiments demonstrates that human listeners can use a moving locus of attention to follow sound sources that change over time, and do not need consistent distinguishing features (e.g., ‘the higher-pitched voice’) to select sources with attention. An experimental paradigm is introduced to study auditory attentive tracking, and is used to reveal some of its characteristics. A second set of experiments demonstrates that human listeners can rapidly learn recurring structures and use them for scene analysis. This kind of learning does not require immediate repetition, and can occur on relatively abstract source structure (e.g., the ‘shape’ of a source’s trajectory through a feature space). The rapidity of learning made it desirable to run short experiments with large numbers of participants in order to observe learning as it occurred. Because this was impractical in the lab, we developed methods to obtain reliable data via web-based auditory psychophysics. A core component of this was a screening task to ensure that online participants are wearing headphones rather than listening in free field. These new methods and tools were validated with behavioral experiments and have been made publicly available.

37. 
TITLE: Communication Masking by Man-Made Noise
AUTHORS: R.J. Dooling, M.R. Leek
YEAR: 2018
SOURCE: Springer Handbook of Auditory Research
ABSTRACT: Conservationists and regulators are often challenged with determining the masking effects of man-made sound introduced into the environment. A considerable amount is known from laboratory studies of auditory masking of communication signals in birds, so that it is now feasible to develop a functional model for estimating the masking effects of noise on acoustic communication in natural environments not only for birds but for other animals as well. Broadband noise can affect the detection, discrimination, and recognition of sounds and whether acoustic communication is judged comfortable or challenged. Estimates of these effects can be obtained from a simple measure called the critical ratio. Critical ratio data are available in both humans and a wide variety of other animals. Because humans have smaller critical ratios (i.e., hear better in noise) than other animals, human listeners can be used as a crude proxy for estimating the limits of effects on animals. That is, if a human listener can barely hear a signal in noise in the environment, it is unlikely that an animal can hear it. The key to estimating the amount of masking from noise that can occur in animals in their natural habitats is in measuring or estimating the signal and noise levels precisely at the animal’s ears in complex environments. Once that is done, a surprising amount of comparative laboratory critical ratio data exists, especially for birds, from which it is possible to predict the effect of noise on acoustic communication. Although best developed for birds, these general principles should hold for all animals.

38.
TITLE: Effects of Noise on Marine Mammals
AUTHORS: C. Erbe, R.A. Dunlop, S.J. Dolman
YEAR: 2018
SOURCE: Springer Handbook of Auditory Research
ABSTRACT: Marine mammals (whales, dolphins, seals, sea lions, sea cows) use sound both actively and passively to communicate and sense their environment, covering frequencies from a few hertz to greater than 100 kHz, differing with species. Although a few documents on marine mammal sound production and reception date back 200 years, concern about the effects of man-made noise on marine mammals has only been documented since the 1970s. Underwater noise can interfere with key life functions of marine mammals (e.g., foraging, mating, nursing, resting, migrating) by impairing hearing sensitivity, masking acoustic signals, eliciting behavioral responses, or causing physiological stress. Many countries are developing and updating guidelines and regulations for underwater noise management in relation to marine mammal conservation. In the United States, the Marine Mammal Protection Act, enacted in 1972, is increasingly being applied to underwater noise emission. Common mitigation methods include (1) time/area closures, (2) the establishment of safety zones that are monitored by visual observers or passive acoustics and that lead to shut-down or low-power operations if animals enter these zones, (3) noise reduction gear like bubble curtains around pile driving, and (4) noise source modifications or operational parameters like soft starts. Mitigation management mostly deals with single operations (like a one-month seismic survey). Key questions that remain are how noise impacts accumulate over time and multiple exposures, how multiple acoustic and nonacoustic stressors interact, and how effects on individuals affect a population as a whole.

39. 
TITLE: Effects of Anthropogenic Noise on Animals
AUTHORS: R. Dooling, H. Slabbekoorn, A. Popper, R. Fay
YEAR: 2018
SOURCE: Springer Handbook of Auditory Research
ABSTRACT: The world is full of sounds of abiotic and biotic origin, and animals may use those sounds to gain information about their surrounding environment. However, it is becoming increasingly clear that the presence of man-made sounds has the potential to undermine the ability of animals to exploit useful environmental sounds. This volume provides an overview of how sounds may affect animals so that those interested in the effects of man-made sounds on animals can better understand the nature and breadth of potential impacts. This chapter provides an introduction to the issues associated with hearing and man-made sound and serves as a guide to the succeeding chapters. Chapters 2, 3, 4 and 5 cover the basic principles of sound and hearing, including an introduction to the acoustic ecology of the modern world in which man-made sounds have become very prominent. They also address how noisy conditions may hinder auditory perception, how hearing adaptations allow coping under acoustically challenging conditions, and how man-made sounds may damage the inner ear. The role of sound propagation in affecting signals and noise levels is treated for both terrestrial and aquatic habitats. This chapter also provides an overview of hearing and the effects of sound on particular taxa, which are the focus of Chaps. 6, 7, 8, 9, and 10. Those chapters address the concepts and insights in five different vertebrate taxa: fishes, amphibians and reptiles, birds, terrestrial mammals, and marine mammals. The overall aim of this volume is to stimulate and guide future investigations to fill in taxonomic and conceptual gaps in the knowledge about how man-made sounds affect animals.

40.
TITLE: Perceived Synchrony of Frog Multimodal Signal Components Is Influenced by Content and Order.
AUTHORS: R.C. Taylor, R. Page, B.A. Klein, M. Ryan, K. Hunter
YEAR: 2017
SOURCE: Integrative and Comparative Biology
ABSTRACT: Multimodal signaling is common in communication systems. Depending on the species, individual signal components may be produced synchronously as a result of physiological constraint (fixed) or each component may be produced independently (fluid) in time. For animals that rely on fixed signals, a basic prediction is that asynchrony between the components should degrade the perception of signal salience, reducing receiver response. Male túngara frogs, Physalaemus pustulosus, produce a fixed multisensory courtship signal by vocalizing with two call components (whines and chucks) and inflating a vocal sac (visual component). Using a robotic frog, we tested female responses to variation in the temporal arrangement between acoustic and visual components. When the visual component lagged a complex call (whine + chuck), females largely rejected this asynchronous multisensory signal in favor of the complex call absent the visual cue. When the chuck component was removed from one call, but the robofrog inflation lagged the complex call, females responded strongly to the asynchronous multimodal signal. When the chuck component was removed from both calls, females reversed preference and responded positively to the asynchronous multisensory signal. When the visual component preceded the call, females responded as often to the multimodal signal as to the call alone. These data show that asynchrony of a normally fixed signal does reduce receiver responsiveness. The magnitude and overall response, however, depend on specific temporal interactions between the acoustic and visual components. The sensitivity of túngara frogs to lagging visual cues, but not leading ones, and the influence of acoustic signal content on the perception of visual asynchrony is similar to those reported in human psychophysics literature. Virtually all acoustically communicating animals must conduct auditory scene analyses and identify the source of signals. Our data suggest that some basic audiovisual neural integration processes may be at work in the vertebrate brain.

41. 
TITLE: Acoustic Reflector Localisation for Blind Source Separation and Spatial Audio.
AUTHORS: L. Remaggi
YEAR: 2017
SOURCE: University of Surrey
ABSTRACT: From a physical point of view, sound is classically defined by wave functions. Like every other physical model based on waves, during its propagation, it interacts with the obstacles it encounters. These interactions result in reflections of the main signal that can be defined as either being supportive or interfering. In the signal processing research field, it is, therefore, important to identify these reflections, in order to either exploit or avoid them, respectively.   The main contribution of this thesis focuses on the acoustic reflector localisation. Four novel methods are proposed: a method localising the image source before finding the reflector position; two variants of this method, which utilise information from multiple loudspeakers; a method directly localising the reflector without any pre-processing. Finally, utilising both simulated and measured data, a comparative evaluation is conducted among different acoustic reflector localisation methods. The results show the last proposed method outperforming the state-of-the-art. The second contribution of this thesis is given by applying the acoustic reflector localisation solution into spatial  audio, with the main objective of enabling the listeners with the sensation of being in the recorded environment. A novel way of encoding and decoding the room acoustic information is proposed, by parametrising sounds, and defining them as reverberant spatial audio objects (RSAOs). A set of subjective assessments are performed. The results prove both the high quality of the sound produced by the proposed parametrisation, and the reliability on manually modifying the acoustic of recorded environments. The third contribution is proposed in the field of speech source separation. A modified version of a state-of-the-art method is presented, where the direct sound and first reflection information is utilised to model binaural cues. Experiments were performed to separate speech sources in different environments. The results show the new method to outperform the state-of-the-art, where one interferer is present in the recordings.   The simulation and experimental results presented in this thesis represent a significant addition to the literature and will influence the future choices of acoustic reflector localisation systems, 3D rendering, and source separation techniques. Future work may focus on the fusion of acoustic and visual cues to enhance the acoustic scene analysis

42. 
TITLE: The First Call Note Plays a Crucial Role in Frog Vocal Communication
AUTHORS: X. Yue, Y. Fan, F. Xue, S.E. Brauth, Y. Tang, G. Fang
YEAR: 2017
SOURCE: Scientific reports
ABSTRACT: Vocal Communication plays a crucial role in survival and reproductive success in most amphibian species. Although amphibian communication sounds are often complex consisting of many temporal features, we know little about the biological significance of each temporal component. The present study examined the biological significance of notes of the male advertisement calls of the Emei music frog (Babina daunchina) using the optimized electroencephalogram (EEG) paradigm of mismatch negativity (MMN). Music frog calls generally contain four to six notes separated approximately by 150 millisecond intervals. A standard stimulus (white noise) and five deviant stimuli (five notes from one advertisement call) were played back to each subject while simultaneously recording multi-channel EEG signals. The results showed that the MMN amplitude for the first call note was significantly larger than for that of the others. Moreover, the MMN amplitudes evoked from the left forebrain and midbrain were typically larger than those from the right counterpart. These results are consistent with the ideas that the first call note conveys more information than the others for auditory recognition and that there is left-hemisphere dominance for processing information derived from conspecific calls in frogs.

43.
TITLE: Individual Recognition of Opposite Sex Vocalizations in the Zebra Finch
AUTHORS: P.B. D'Amelio, M. Klumb, M.N. Adreani, M. Gahr, A. Ter Maat 
YEAR: 2017
SOURCE: Scientific Reports
ABSTRACT: Individual vocal recognition plays an important role in the social lives of many vocally active species. In group-living songbirds the most common vocalizations during communal interactions are low-intensity, soft, unlearned calls. Being able to tell individuals apart solely from a short call would allow a sender to choose a specific group member to address, resulting in the possibility to form complex communication networks. However, little research has yet been carried out to discover whether soft calls contain individual identity. In this study, males and females of zebra finch pairs were tested with six vocalization types - four different soft calls, the distance call and the male song - to investigate whether they are able to distinguish individuals of the opposite sex. For both sexes, we provide the first evidence of individual vocal recognition for a zebra finch soft unlearned call. Moreover, while controlling for habituation and testing for repeatability of the findings, we quantify the effects of hitherto little studied variables such as partners’ vocal exchange previous to the experiment, spectral content of playback calls and quality of the answers. We suggest that zebra finches can recognize individuals via soft vocalizations, therefore allowing complex directed communication within vocalizing flocks.

44.
TITLE: Blind Nonnegative Source Separation Using Biological Neural Networks
AUTHORS: C. Pehlevan, S. Mohan, D.B. Chklovskii
YEAR: 2017
SOURCE: arXiv
ABSTRACT: Blind source separation—the extraction of independent sources from a mixture—is an important problem for both artificial and natural signal processing. Here, we address a special case of this problem when sources (but not the mixing matrix) are known to be nonnegative—for example, due to the physical nature of the sources. We search for the solution to this problem that can be implemented using biologically plausible neural networks. Specifically, we consider the online setting where the data set is streamed to a neural network. The novelty of our approach is that we formulate blind nonnegative source separation as a similarity matching problem and derive neural networks from the similarity matching objective. Importantly, synaptic weights in our networks are updated according to biologically plausible local learning rules.

45.
TITLE: Feature-Selective Attention Adaptively Shifts Noise Correlations in Primary Auditory Cortex
AUTHORS: J.D. Downer, B.C. Rapone, J.R. Verhein, K.N. O'Connor, M.L. Sutter
YEAR: 2017
SOURCE: The Journal of Neuroscience
ABSTRACT: Sensory environments often contain an overwhelming amount of information, with both relevant and irrelevant information competing for neural resources. Feature attention mediates this competition by selecting the sensory features needed to form a coherent percept. How attention affects the activity of populations of neurons to support this process is poorly understood because population coding is typically studied through simulations in which one sensory feature is encoded without competition. Therefore, to study the effects of feature attention on population-based neural coding, investigations must be extended to include stimuli with both relevant and irrelevant features. We measured noise correlations (rnoise) within small neural populations in primary auditory cortex while rhesus macaques performed a novel feature-selective attention task. We found that the effect of feature-selective attention on rnoise depended not only on the population tuning to the attended feature, but also on the tuning to the distractor feature. To attempt to explain how these observed effects might support enhanced perceptual performance, we propose an extension of a simple and influential model in which shifts in rnoise can simultaneously enhance the representation of the attended feature while suppressing the distractor. These findings present a novel mechanism by which attention modulates neural populations to support sensory processing in cluttered environments. SIGNIFICANCE STATEMENT Although feature-selective attention constitutes one of the building blocks of listening in natural environments, its neural bases remain obscure. To address this, we developed a novel auditory feature-selective attention task and measured noise correlations (rnoise) in rhesus macaque A1 during task performance. Unlike previous studies showing that the effect of attention on rnoise depends on population tuning to the attended feature, we show that the effect of attention depends on the tuning to the distractor feature as well. We suggest that these effects represent an efficient process by which sensory cortex simultaneously enhances relevant information and suppresses irrelevant information.

46.
TITLE: Who Shall I Say Is Calling? Validation of a Caller Recognition Procedure in Bornean Flanged Male Orangutan (Pongo Pygmaeus Wurmbii) Long Calls
AUTHORS: B. Spillman, C.V. van Schaik, T.M. Setia, S.O. Sadjadi
YEAR: 2017
SOURCE: Bioacoustics
ABSTRACT: Acoustic individual discrimination has been demonstrated for a wide range of animal taxa. However, there has been far less scientific effort to demonstrate the effectiveness of automatic individual identification, which could greatly facilitate research, especially when data are collected via an acoustic localization system (ALS). In this study, we examine the accuracy of acoustic caller recognition in long calls (LCs) emitted by Bornean male orangutans (Pongo pygmaeus wurmbii) derived from two data-sets: the first consists of high-quality recordings taken during individual focal follows (N = 224 LCs by 14 males) and the second consists of LC recordings with variable microphone-caller distances stemming from ALS (N = 123 LCs by 10 males). The LC is a long-distance vocalization. We therefore expect that even the low-quality test-set should yield caller recognition results significantly better than by chance. Automatic individual identification was accomplished using software originally developed for human speaker recognition (i.e. the MSR identity toolbox). We obtained a 93.3% correct identification rate with high-quality recordings, and 72.23% with recordings stemming from the ALS with variable microphone-caller distances (20–420 m). These results show that automatic individual identification is possible even though the accuracy declines compared with the results of high-quality recordings due to severe signal degradations (e.g. sound attenuation, environmental noise contamination, and echo interference) with increasing distance. We therefore suggest that acoustic individual identification with speaker recognition software can be a valuable tool to apply to data obtained through an ALS, thereby facilitating field research on vocal communication.

47.
TITLE: The Signal in Noise: Acoustic Information for Soundscape Orientation in Two North American Tree Frogs
AUTHORS: A. Velez, N.M. Gordon, M. Bee
YEAR: 2017
SOURCE: Behavioral Ecology
ABSTRACT: Information conveyed by the timing and temporal structure of calls embedded in sounds of breeding choruses is necessary for eliciting orientation in treefrogs. In contrast, information based on emergent acoustic properties arising from the collective mixture of calls constituting the chorus is not enough. Orientation based on the timing and temporal properties of the actual calls composing choruses could limit the distances over which female frogs could use sound to orient toward and localize breeding aggregations.

48.
TITLE: Recent Advances in Exploring the Neural Underpinnings of Auditory Scene Perception
AUTHORS: J.S. Snyder
YEAR: 2017
SOURCE: Annals of the New York Academy of Sciences
ABSTRACT: Studies of auditory scene analysis have traditionally relied on paradigms using artificial sounds-and conventional behavioral techniques-to elucidate how we perceptually segregate auditory objects or streams from each other. In the past few decades, however, there has been growing interest in uncovering the neural underpinnings of auditory segregation using human and animal neuroscience techniques, as well as computational modeling. This largely reflects the growth in the fields of cognitive neuroscience and computational neuroscience and has led to new theories of how the auditory system segregates sounds in complex arrays. The current review focuses on neural and computational studies of auditory scene perception published in the last few years. Following the progress that has been made in these studies, we describe (1) theoretical advances in our understanding of the most well-studied aspects of auditory scene perception, namely segregation of sequential patterns of sounds and concurrently presented sounds; (2) the diversification of topics and paradigms that have been investigated; and (3) how new neuroscience techniques (including invasive neurophysiology in awake humans, genotyping, and brain stimulation) have been used in this field.

49.
TITLE: Frogs Exploit Statistical Regularities in Noisy Acoustic Scenes to Solve Cocktail-Party-like Problems
AUTHORS: N. Lee, J.L. Ward, A. Velez, C. Micheyl, M. Bee
YEAR: 2017
SOURCE: Current Biology
ABSTRACT: Noise is a ubiquitous source of errors in all forms of communication [1]. Noise-induced errors in speech communication, for example, make it difficult for humans to converse in noisy social settings, a challenge aptly named the "cocktail party problem" [2]. Many nonhuman animals also communicate acoustically in noisy social groups and thus face biologically analogous problems [3]. However, we know little about how the perceptual systems of receivers are evolutionarily adapted to avoid the costs of noise-induced errors in communication. In this study of Cope's gray treefrog (Hyla chrysoscelis; Hylidae), we investigated whether receivers exploit a potential statistical regularity present in noisy acoustic scenes to reduce errors in signal recognition and discrimination. We developed an anatomical/physiological model of the peripheral auditory system to show that temporal correlation in amplitude fluctuations across the frequency spectrum ("comodulation") [4-6] is a feature of the noise generated by large breeding choruses of sexually advertising males. In four psychophysical experiments, we investigated whether females exploit comodulation in background noise to mitigate noise-induced errors in evolutionarily critical mate-choice decisions. Subjects experienced fewer errors in recognizing conspecific calls and in selecting the calls of high-quality mates in the presence of simulated chorus noise that was comodulated. These data show unequivocally, and for the first time, that exploiting statistical regularities present in noisy acoustic scenes is an important biological strategy for solving cocktail-party-like problems in nonhuman animal communication.

50.
TITLE: Animal Models for Auditory Streaming
AUTHORS: N. Itatani, G.M. Klump
YEAR: 2017
SOURCE: Philosophical Transactions of the Royal Society B: Biological Sciences
ABSTRACT: Sounds in the natural environment need to be assigned to acoustic sources to evaluate complex auditory scenes. Separating sources will affect the analysis of auditory features of sounds. As the benefits of assigning sounds to specific sources accrue to all species communicating acoustically, the ability for auditory scene analysis is widespread among different animals. Animal studies allow for a deeper insight into the neuronal mechanisms underlying auditory scene analysis. Here, we will review the paradigms applied in the study of auditory scene analysis and streaming of sequential sounds in animal models. We will compare the psychophysical results from the animal studies to the evidence obtained in human psychophysics of auditory streaming, i.e. in a task commonly used for measuring the capability for auditory scene analysis. Furthermore, the neuronal correlates of auditory streaming will be reviewed in different animal models and the observations of the neurons’ response measures will be related to perception. The across-species comparison will reveal whether similar demands in the analysis of acoustic scenes have resulted in similar perceptual and neuronal processing mechanisms in the wide range of species being capable of auditory scene analysis.

51.
TITLE: Schema vs. Primitive Perceptual Grouping: The Relative Weighting of Sequential vs. Spatial Cues during an Auditory grouping Task in Frogs
AUTHORS: H. Farris, M.Ryan
YEAR: 2017
SOURCE: Journal of Comparative Physiology A
ABSTRACT: Perceptually, grouping sounds based on their sources is critical for communication. This is especially true in túngara frog breeding aggregations, where multiple males produce overlapping calls that consist of an FM ‘whine’ followed by harmonic bursts called ‘chucks’. Phonotactic females use at least two cues to group whines and chucks: whine-chuck spatial separation and sequence. Spatial separation is a primitive cue, whereas sequence is schema-based, as chuck production is morphologically constrained to follow whines, meaning that males cannot produce the components simultaneously. When one cue is available, females perceptually group whines and chucks using relative comparisons: components with the smallest spatial separation or those closest to the natural sequence are more likely grouped. By simultaneously varying the temporal sequence and spatial separation of a single whine and two chucks, this study measured between-cue perceptual weighting during a specific grouping task. Results show that whine-chuck spatial separation is a stronger grouping cue than temporal sequence, as grouping is more likely for stimuli with smaller spatial separation and non-natural sequence than those with larger spatial separation and natural sequence. Compared to the schema-based whine-chuck sequence, we propose that spatial cues have less variance, potentially explaining their preferred use when grouping during directional behavioral responses.

52.
TITLE: Energetic Masking and Masking Release
AUTHORS: J.F. Culling, M.A. Stone
YEAR: 2017
SOURCE: Springer Handbook of Auditory Research
ABSTRACT: Masking is of central interest in the cocktail party problem, because interfering voices may be sufficiently intense or numerous to mask the voice to which the listener is attending, rendering its discourse unintelligible. The definition of energetic masking is problematic, but it may be considered to consist of effects by which an interfering sound disrupts the processing of the speech signal in the lower levels of the auditory system. Maskers can affect speech intelligibility by overwhelming its representation on the auditory nerve and by obscuring its amplitude modulations. A release from energetic masking is obtained by using mechanisms at these lower levels that can recover a useful representation of the speech. These mechanisms can exploit differences between the target and masking speech such as in harmonic structure or in interaural time delay. They can also exploit short-term dips in masker strength or improvements in speech-to-masker ratio at one or other ear.

53.
TITLE: Sound Source Localization and Segregation with Internally Coupled Ears: The Treefrog Model
AUTHORS: M. Bee, J. Christensen-Dalsgaard
YEAR: 2016
SOURCE: Biological Cybernetics
ABSTRACT: Acoustic signaling plays key roles in mediating many of the reproductive and social behaviors of anurans (frogs and toads). Moreover, acoustic signaling often occurs at night, in structurally complex habitats, such as densely vegetated ponds, and in dense breeding choruses characterized by high levels of background noise and acoustic clutter. Fundamental to anuran behavior is the ability of the auditory system to determine accurately the location from where sounds originate in space (sound source localization) and to assign specific sounds in the complex acoustic milieu of a chorus to their correct sources (sound source segregation). Here, we review anatomical, biophysical, neurophysiological, and behavioral studies aimed at identifying how the internally coupled ears of frogs contribute to sound source localization and segregation. Our review focuses on treefrogs in the genus Hyla, as they are the most thoroughly studied frogs in terms of sound source localization and segregation. They also represent promising model systems for future work aimed at understanding better how internally coupled ears contribute to sound source localization and segregation. We conclude our review by enumerating directions for future research on these animals that will require the collaborative efforts of biologists, physicists, and roboticists.

54.
TITLE: Multimodal Signaling Improves Mating Success in the Green Tree Frog (Hyla Cinerea), But May Not Help Small Males
AUTHORS: K.L.Laird, P. Clements, K.L. Hunter, R.C. Taylor
YEAR: 2016
SOURCE: Behavioral Ecology and Sociobiology
ABSTRACT: Many anuran amphibians are challenged with the detection of courtship signals in noisy chorus environments. Anurans and other animals partially solve this discrimination challenge by employing auditory mechanisms such as grouping sounds by frequency, time, or spatial location. Animals are also known to employ visual cues as a mechanism of improving auditory signal detection. In this study, we examined the effect of acoustic and visual stimuli on female mate choice preferences in the green tree frog, Hyla cinerea. We used a series of two choice playback tests and added a robotic frog, with an inflatable vocal sac, to test interactions among visual and acoustic signal components. Females preferred vocalizations with faster call rates (i.e., high energy cost) and lower call frequencies (i.e., larger males). When call properties were held equal, females discriminated against an acoustic only stimulus in favor of the combined acoustic/visual multimodal signal. A visual component did not, however, increase the attractiveness of an otherwise unattractive (high-frequency) acoustic signal. Thus, female green tree frogs integrate the visual display into the acoustic communication system and males that are visually accessible can increase their probability of mating success. Visual accessibility, however, is unlikely to improve mating success for small males (high-frequency callers).Significance statementAnimal communication signals are often complex and communicated in multiple sensory channels (e.g., auditory + visual). Female choice is known to be an important mechanism driving signal evolution. Thus, for complex mating signals, a first step in understanding their evolution is to test how females respond to various combinations of components. Here, we tested female mate choice in the green tree frog, H. cinerea, using a combination of audio playbacks and a robotic frog as the visual component. When the audio signal was standardized, females preferred a signal enhanced by a robotic frog. The robotic frog did not increase female responses to an unattractive call (indicative of a small male), however. These results suggest that visual accessibility can improve a male’s chance of mating, but this advantage is context dependent and does not extend to smaller males.

55.
TITLE: Effects of Physiological Internal Noise on Model Predictions of Concurrent Vowel Identification for Normal-Hearing Listeners
AUTHORS: M. Hendrick, I.J. Moon, J. Woo, J.Won
YEAR: 2016
SOURCE: PLoS ONE
ABSTRACT: Previous studies have shown that concurrent vowel identification improves with increasing temporal onset asynchrony of the vowels, even if the vowels have the same fundamental frequency. The current study investigated the possible underlying neural processing involved in concurrent vowel perception. The individual vowel stimuli from a previously published study were used as inputs for a phenomenological auditory-nerve (AN) model. Spectrotemporal representations of simulated neural excitation patterns were constructed (i.e., neurograms) and then matched quantitatively with the neurograms of the single vowels using the Neurogram Similarity Index Measure (NSIM). A novel computational decision model was used to predict concurrent vowel identification. To facilitate optimum matches between the model predictions and the behavioral human data, internal noise was added at either neurogram generation or neurogram matching using the NSIM procedure. The best fit to the behavioral data was achieved with a signal-to-noise ratio (SNR) of 8 dB for internal noise added at the neurogram but with a much smaller amount of internal noise (SNR of 60 dB) for internal noise added at the level of the NSIM computations. The results suggest that accurate modeling of concurrent vowel data from listeners with normal hearing may partly depend on internal noise and where internal noise is hypothesized to occur during the concurrent vowel identification process.

56.
TITLE: Acoustic Sequences in Non-Human Animals: A Tutorial Review and Prospectus
AUTHORS: A. Kershenbaum, et al.
YEAR: 2016
SOURCE: Biological Reviews of the Cambridge Philosophical Society
ABSTRACT: Animal acoustic communication often takes the form of complex sequences, made up of multiple distinct acoustic units. Apart from the well-known example of birdsong, other animals such as insects, amphibians, and mammals (including bats, rodents, primates, and cetaceans) also generate complex acoustic sequences. Occasionally, such as with birdsong, the adaptive role of these sequences seems clear (e.g. mate attraction and territorial defence). More often however, researchers have only begun to characterise - let alone understand - the significance and meaning of acoustic sequences. Hypotheses abound, but there is little agreement as to how sequences should be defined and analysed. Our review aims to outline suitable methods for testing these hypotheses, and to describe the major limitations to our current and near-future knowledge on questions of acoustic sequences. This review and prospectus is the result of a collaborative effort between 43 scientists from the fields of animal behaviour, ecology and evolution, signal processing, machine learning, quantitative linguistics, and information theory, who gathered for a 2013 workshop entitled, 'Analysing vocal sequences in animals'. Our goal is to present not just a review of the state of the art, but to propose a methodological framework that summarises what we suggest are the best practices for research in this field, across taxa and across disciplines. We also provide a tutorial-style introduction to some of the most promising algorithmic approaches for analysing sequences. We divide our review into three sections: identifying the distinct units of an acoustic sequence, describing the different ways that information can be contained within a sequence, and analysing the structure of that sequence. Each of these sections is further subdivided to address the key questions and approaches in that area. We propose a uniform, systematic, and comprehensive approach to studying sequences, with the goal of clarifying research terms used in different fields, and facilitating collaboration and comparative studies. Allowing greater interdisciplinary collaboration will facilitate the investigation of many important questions in the evolution of communication and sociality.

57.
TITLE: Technology and Sensory, Perceptual, and Cognitive Processes
AUTHORS: V.K. Kool
YEAR: 2016
SOURCE: Psychology of Technology
ABSTRACT: While one human characteristic, namely, human anatomy, places limits on the use of technology, another is the cornerstone of its very creation and development. This is the cortex of the highly developed human brain, bringing in its wake, tools and technology far surpassing those created by other primates. Evolutionary forces and environmental pressures have helped shape this brain, leading to the development of structures that have given it remarkable mental capabilities. Chapter 3 focuses on how this brain enables us to make the right choices so as to enhance the ability to collectively improve ourselves and to create technology that has helped in adaptation to the environment. It takes the reader through the intricacies of our highly advanced sensory systems which receive environmental cues, organizing them into perceptual wholes (or Gestalt) on the basis of which ways certain mental operations such as attention, memory, problem solving and reasoning, etc. (under the rubric of cognition) are performed. A thorough understanding of the sensory and perceptual systems along with their limits, especially those related to the visual and the auditory system provides useful insights to the technology developer, helping to resolve many of the problems associated with, say, the design of visual and auditory displays, sonification, and audiovisual synchronization. The chapter then moves on to describe some of the features of the human cognitive architecture. Using simple language with the minimum of technical jargon, the reader is enabled to understand the complex ways in which human cognitive processes operate leading to massive improvements in technology, helping it to grow exponentially, leapfrogging at times and destroying old technology to be replaced by new at other times. The isolation of regions of the brain associated with various cognitive processes has also gone a long way in enabling the design of brain interfaces through which cognitive processes may be further augmented. It has been pointed out that this can be possible only through interdisciplinary efforts requiring the participation of disciplines ranging from psychology to nanotechnology. The chapter ends with an introduction to convergent technologies, a perfect example of the symbiotic relationship between psychology and technology.

58.
TITLE: Perceptual and Neural Mechanisms of Auditory Scene Analysis in the European Starling
AUTHORS: G.M. Klump
YEAR: 2016
SOURCE: Animal Signals and Communication
ABSTRACT: Humans and many other animals, such as songbirds, communicate acoustically in large, dense social groups. In such environments, the signals produced by different signalers commonly overlap in time and frequency, and background noise can be intense. How can receivers make sense of the acoustic scene when there is so much noise and acoustic clutter? The answer is that vocal communication in such environments engages a suite of perceptual and cognitive mechanisms responsible for parsing the acoustic scene into perceptually discrete auditory “objects” or “streams” of behavioral relevance. In this chapter, I review psychophysical and neurophysiological studies of European starlings ( Sturnus vulgaris , Sturnidae) that have aimed to identify mechanisms underlying the perceptual organization of complex acoustic scenes. The focus of this review is on recent efforts to discover neural mechanisms for auditory scene analysis (ASA) that promote signal detection (e.g., comodulation masking release and the comodulation detection difference), signal recognition (e.g., perceptual restoration), and signal segregation (e.g., auditory streaming ) under adverse listening conditions. The chapter emphasizes that key insights into the neural codes for ASA are to be gained by integrating neurophysiological approaches with objective measures of psychophysical performance in animal models for which receiving communication signals in a crowd is a key feature of their biology.

59.
TITLE: Mate Searching Animals as Model Systems for Understanding Perceptual Grouping
AUTHORS: H. Farris, R.C. Taylor
YEAR: 2016
SOURCE: Animal Signals and Communication
ABSTRACT: A critical component of communication in humans and nonhuman animals is the ability to group signals so that they can be assigned to their correct sources. This is especially true for mate choice behavior, as incorrect stimulus grouping could lead to inaccurate evaluation of signalers by receivers, ultimately resulting in costly mate choice decisions . Sexual signals are often complex, consisting of components that vary in several physical parameters and across sensory modalities. Thus, the mate choice behavior of receivers is well suited for psychophysical tests of the limits and mechanisms of perceptual grouping both within and across sensory modalities. This chapter examines perceptual grouping in comparative models of mate choice behavior. We focus primarily on mate attraction in frogs, reviewing first the effects of spectral, temporal, and spatial parameters on sequential and simultaneous auditory grouping. We then review research on cross-modal perceptual grouping of frog visual and acoustic signals, a perceptual ability analogous to that of grouping human speech with its coincident mouth movements. In addition, we suggest that data from comparative models are not only useful for understanding signal processing in animal communication but also for potentially understanding the fundamental mechanisms receivers use to sort complex signals across all taxa and how such mechanisms may evolve.

60.
TITLE: Choosing a Mate in a Cocktail Party-Like Situation: The Effect of Call Complexity and Call Timing between Two Rival Males on Female Mating Preferences in the Túngara Frog Physalaemus Pustulosus
AUTHORS: Z. Tarano
YEAR: 2015
SOURCE: Ethology
ABSTRACT: Signal detection, recognition, and localization are hampered when multiple signalers coincide in time and space, a problem known as ‘cocktail party effect’. In many taxa, senders utter complex calls consisting of two or more elements which often vary in the ease with which they can be assessed in different signaling environments. Receivers’ selective attention to different cues may increase the probability of correctly assigning a signal to its source (localization) in face of conspecific interference. Tungara frogs, Physalaemus pustulosus, produce complex calls consisting of an initial whine, followed by zero up to seven broad-banded, amplitude-modulated chucks. Under ideal conditions (without interference or noise), females prefer whines followed by chucks over whines alone, but the preference is not linear; females do not discriminate between whines with one or two chucks. When whines lack chucks, call overlap elicits random responses in females, with no preference for leading calls. In this study, I explored the combined effect of call timing and call complexity on female preferences in a two-choice paradigm—a simplification of the cocktail party scenario. I tested the hypothesis that the effect of call overlap can be reduced when the calls of one of the two rivals have chucks, specifically more chucks than those of the rival. I gave females a choice between whines alone and with chucks (one or two) presented at three time relations (alternated, abutted, and partially overlapped) and two emission orders (whine with less chucks leading and whine with more chucks leading). I found that the preference for one chuck over no chuck was preserved in all the experimental treatments, but when a w + 2chk preceded a w + chk, either overlapped or abutted, a preference existed for the whine with more chucks. Therefore, an interaction between call order and the number of chucks was obtained. The results only partially supported the hypothesis, and call order emerges as an opportunistic component of signaling in P. pustulosus.

61.
TITLE: Neural Mechanisms for Acoustic Signal Detection under Strong Masking in an Insect
AUTHORS: K. Kostarakos, H. Romer
YEAR: 2015
SOURCE: The Journal of Neuroscience
ABSTRACT: Communication is fundamental for our understanding of behavior. In the acoustic modality, natural scenes for communication in humans and animals are often very noisy, decreasing the chances for signal detection and discrimination. We investigated the mechanisms enabling selective hearing under natural noisy conditions for auditory receptors and interneurons of an insect. In the studied katydid Mecopoda elongata species-specific calling songs (chirps) are strongly masked by signals of another species, both communicating in sympatry. The spectral properties of the two signals are similar and differ only in a small frequency band at 2 kHz present in the chirping species. Receptors sharply tuned to 2 kHz are completely unaffected by the masking signal of the other species, whereas receptors tuned to higher audio and ultrasonic frequencies show complete masking. Intracellular recordings of identified interneurons revealed two mechanisms providing response selectivity to the chirp. (1) Response selectivity is when several identified interneurons exhibit remarkably selective responses to the chirps, even at signal-to-noise ratios of −21 dB, since they are sharply tuned to 2 kHz. Their dendritic arborizations indicate selective connectivity with low-frequency receptors tuned to 2 kHz. (2) Novelty detection is when a second group of interneurons is broadly tuned but, because of strong stimulus-specific adaptation to the masker spectrum and “novelty detection” to the 2 kHz band present only in the conspecific signal, these interneurons start to respond selectively to the chirp shortly after the onset of the continuous masker. Both mechanisms provide the sensory basis for hearing at unfavorable signal-to-noise ratios. SIGNIFICANCE STATEMENT Animal and human acoustic communication may suffer from the same “cocktail party problem,” when communication happens in noisy social groups. We address solutions for this problem in a model system of two katydids, where one species produces an extremely noisy sound, yet the second species still detects its own song. Using intracellular recording techniques we identified two neural mechanisms underlying the surprising behavioral signal detection at the level of single identified interneurons. These neural mechanisms for signal detection are likely to be important for other sensory modalities as well, where noise in the communication channel creates similar problems. Also, they may be used for the development of algorithms for the filtering of specific signals in technical microphones or hearing aids.

62.
TITLE: Diverse Cortical Codes for Scene Segmentation in Primate Auditory Cortex
AUTHORS: B.J.Malone, B.H. Scott, M.N. Semple
YEAR: 2015
SOURCE: Journal of Neurophysiology
ABSTRACT: The temporal coherence of amplitude fluctuations is a critical cue for segmentation of complex auditory scenes. The auditory system must accurately demarcate the onsets and offsets of acoustic signals. We explored how and how well the timing of onsets and offsets of gated tones are encoded by auditory cortical neurons in awake rhesus macaques. Temporal features of this representation were isolated by presenting otherwise identical pure tones of differing durations. Cortical response patterns were diverse, including selective encoding of onset and offset transients, tonic firing, and sustained suppression. Spike train classification methods revealed that many neurons robustly encoded tone duration despite substantial diversity in the encoding process. Excellent discrimination performance was achieved by neurons whose responses were primarily phasic at tone offset and by those that responded robustly while the tone persisted. Although diverse cortical response patterns converged on effective duration discrimination, this diversity significantly constrained the utility of decoding models referenced to a spiking pattern averaged across all responses or averaged within the same response category. Using maximum likelihood-based decoding models, we demonstrated that the spike train recorded in a single trial could support direct estimation of stimulus onset and offset. Comparisons between different decoding models established the substantial contribution of bursts of activity at sound onset and offset to demarcating the temporal boundaries of gated tones. Our results indicate that relatively few neurons suffice to provide temporally precise estimates of such auditory "edges," particularly for models that assume and exploit the heterogeneity of neural responses in awake cortex.

63. 
TITLE: Treefrogs as Animal Models for Research on Auditory Scene Analysis and the Cocktail Party Problem
AUTHORS: M. Bee
YEAR: 2015
SOURCE: International Journal of Psychophysiology
ABSTRACT: The perceptual analysis of acoustic scenes involves binding together sounds from the same source and separating them from other sounds in the environment. In large social groups, listeners experience increased difficulty performing these tasks due to high noise levels and interference from the concurrent signals of multiple individuals. While a substantial body of literature on these issues pertains to human hearing and speech communication, few studies have investigated how nonhuman animals may be evolutionarily adapted to solve biologically analogous communication problems. Here, I review recent and ongoing work aimed at testing hypotheses about perceptual mechanisms that enable treefrogs in the genus Hyla to communicate vocally in noisy, multi-source social environments. After briefly introducing the genus and the methods used to study hearing in frogs, I outline several functional constraints on communication posed by the acoustic environment of breeding "choruses". Then, I review studies of sound source perception aimed at uncovering how treefrog listeners may be adapted to cope with these constraints. Specifically, this review covers research on the acoustic cues used in sequential and simultaneous auditory grouping, spatial release from masking, and dip listening. Throughout the paper, I attempt to illustrate how broad-scale, comparative studies of carefully considered animal models may ultimately reveal an evolutionary diversity of underlying mechanisms for solving cocktail-party-like problems in communication.

64.
TITLE: Temporal Coherence for Pure Tones in Budgerigars (Melopsittacus Undulatus) and Humans (Homo Sapiens)
AUTHORS: E.G. Neilans, M.L. Dent
YEAR: 2015
SOURCE: Journal of Comparative Psychology
ABSTRACT: Auditory scene analysis has been suggested as a universal process that exists across all animals. Relative to humans, however, little work has been devoted to how animals perceptually isolate different sound sources. Frequency separation of sounds is arguably the most common parameter studied in auditory streaming, but it is not the only factor contributing to how the auditory scene is perceived. Researchers have found that in humans, even at large frequency separations, synchronous tones are heard as a single auditory stream, whereas asynchronous tones with the same frequency separations are perceived as 2 distinct sounds. These findings demonstrate how both the timing and frequency separation of sounds are important for auditory scene analysis. It is unclear how animals, such as budgerigars (Melopsittacus undulatus), perceive synchronous and asynchronous sounds. In this study, budgerigars and humans (Homo sapiens) were tested on their perception of synchronous, asynchronous, and partially overlapping pure tones using the same psychophysical procedures. Species differences were found between budgerigars and humans in how partially overlapping sounds were perceived, with budgerigars more likely to segregate overlapping sounds and humans more apt to fuse the 2 sounds together. The results also illustrated that temporal cues are particularly important for stream segregation of overlapping sounds. Lastly, budgerigars were found to segregate partially overlapping sounds in a manner predicted by computational models of streaming, whereas humans were not.

65.
TITLE: Neuromorphic Model for Sound Source Segregation
AUTHORS: L. Krishnan
YEAR: 2015
SOURCE: University of Maryland
ABSTRACT: While humans can easily segregate and track a speaker’s voice in a loud noisy environment, most modern speech recognition systems still perform poorly in loud background noise. The computational principles behind auditory source segregation in humans is not yet fully understood. In this dissertation, we develop a computational model for source segregation inspired by auditory processing in the brain. To support the key principles behind the computational model, we conduct a series of electro-encephalography experiments using both simple tone-based stimuli and more natural speech stimulus. Most source segregation algorithms utilize some form of prior information about the target speaker or use more than one simultaneous recording of the noisy speech mixtures. Other methods develop models on the noise characteristics. Source segregation of simultaneous speech mixtures with a single microphone recording and no knowledge of the target speaker is still a challenge. Using the principle of temporal coherence, we develop a novel computational model that exploits the difference in the temporal evolution of features that belong to different sources to perform unsupervised monaural source segregation. While using no prior information about the target speaker, this method can gracefully incorporate knowledge about the target speaker to further enhance the segregation.Through a series of EEG experiments we collect neurological evidence to support the principle behind the model. Aside from its unusual structure and computational innovations, the proposed model provides testable hypotheses of the physiological mechanisms of the remarkable perceptual ability of humans to segregate acoustic sources, and of its psychophysical manifestations in navigating complex sensory environments. Results from EEG experiments provide further insights into the assumptions behind the model and provide motivation for future single unit studies that can provide more direct evidence for the principle of temporal coherence. 

66. 
TITLE: Segregating Complex Sound Sources through Temporal Coherence
AUTHORS: L. Krishnan, M. Elhilali, S. Shamma
YEAR: 2014
SOURCE: PLoS Computational Biology
ABSTRACT: A new approach for the segregation of monaural sound mixtures is presented based on the principle of temporal coherence and using auditory cortical representations. Temporal coherence is the notion that perceived sources emit coherently modulated features that evoke highly-coincident neural response patterns. By clustering the feature channels with coincident responses and reconstructing their input, one may segregate the underlying source from the simultaneously interfering signals that are uncorrelated with it. The proposed algorithm requires no prior information or training on the sources. It can, however, gracefully incorporate cognitive functions and influences such as memories of a target source or attention to a specific set of its attributes so as to segregate it from its background. Aside from its unusual structure and computational innovations, the proposed model provides testable hypotheses of the physiological mechanisms of this ubiquitous and remarkable perceptual ability, and of its psychophysical manifestations in navigating complex sensory environments.

67.
TITLE: Auditory Cortical Processing in Real-World Listening: The Auditory System Going Real
AUTHORS: I. Nelken, J. Bizley, S. Shama, X. Wang
YEAR: 2014
SOURCE: The Journal of Neuroscience
ABSTRACT: The auditory sense of humans transforms intrinsically senseless pressure waveforms into spectacularly rich perceptual phenomena: the music of Bach or the Beatles, the poetry of Li Bai or Omar Khayyam, or more prosaically the sense of the world filled with objects emitting sounds that is so important for those of us lucky enough to have hearing. Whereas the early representations of sounds in the auditory system are based on their physical structure, higher auditory centers are thought to represent sounds in terms of their perceptual attributes. In this symposium, we will illustrate the current research into this process, using four case studies. We will illustrate how the spectral and temporal properties of sounds are used to bind together, segregate, categorize, and interpret sound patterns on their way to acquire meaning, with important lessons to other sensory systems as well.

68.
TITLE: The Anuran Vocal Sac: A Tool for Multimodal Signalling
AUTHORS: I. Starnberger, D. Peiniger, W. Hoedl
YEAR: 2014
SOURCE: Animal Behaviour
ABSTRACT: Although in anurans the predominant mode of intra- and intersexual communication is vocalization, modalities used in addition to or instead of acoustic signals range from seismic and visual to chemical. In some cases, signals of more than one modality are produced through or by the anuran vocal sac. However, its role beyond acoustics has been neglected for some time and nonacoustic cues such as vocal sac movement have traditionally been seen as an epiphenomenon of sound production. The diversity in vocal sac coloration and shape found in different species is striking and recently its visual properties have been given a more important role in signalling. Chemosignals seem to be the dominant communication mode in newts, salamanders and caecilians and certainly play a role in the aquatic life phase of anurans, but airborne chemical signalling has received less attention. There is, however, increasing evidence that at least some terrestrial anuran species integrate acoustic, visual and chemical cues in species recognition and mate choice and a few secondarily mute anuran species seem to fully rely on volatile chemical cues produced in glands on the vocal sac. Within vertebrates, frogs in particular are suitable organisms for investigating multimodal communication by means of experiments, since they are tolerant of disturbance by observers and can be easily manipulated under natural conditions. Thus, the anuran vocal sac might be of great interest not only to herpetologists, but also to behavioural biologists studying communication systems.

69.
TITLE: Directional Hearing: From Biophysical Binaural Cues to Directional Hearing Outdoors
AUTHORS: H. Romer
YEAR: 2014
SOURCE: Journal of Comparative Physiology. A, Neuroethology, Sensory, Neural, and Behavioral Physiology
ABSTRACT: When insects communicate by sound, or use acoustic cues to escape predators or detect prey or hosts they have to localize the sound in most cases, to perform adaptive behavioral responses. In the case of particle velocity receivers such as the antennae of mosquitoes, directionality is no problem because such receivers are inherently directional. Insects equipped with bilateral pairs of tympanate ears could principally make use of binaural cues for sound localization, like all other animals with two ears. However, their small size is a major problem to create sufficiently large binaural cues, with respect to both interaural time differences (ITDs, because interaural distances are so small), but also with respect to interaural intensity differences (IIDs), since the ratio of body size to the wavelength of sound is rather unfavorable for diffractive effects. In my review, I will only shortly cover these biophysical aspects of directional hearing. Instead, I will focus on aspects of directional hearing which received relatively little attention previously, the evolution of a pressure difference receiver, 3D-hearing, directional hearing outdoors, and directional hearing for auditory scene analysis.

70.
TITLE: Crossmodal Comparisons of Signal Components Allow for Relative-Distance Assessment
AUTHORS: W. Halfwerk, R. Page, R.C. Taylor, P. Wilson, M. Ryan
YEAR: 2014
SOURCE: Current Biology
ABSTRACT: Animals have multiple senses through which they detect their surroundings and often integrate sensory information across different modalities to generate perceptions. Animal communication, likewise, often consists of signals containing stimuli processed by different senses. Stimuli with different physical forms (i.e., from different sensory modalities) travel at different speeds. As a consequence, multimodal stimuli simultaneously emitted at a source can arrive at a receiver at different times. Such differences in arrival time can provide unique information about the distance to the source. Male túngara frogs (Physalaemus pustulosus) call from ponds to attract females and to repel males. Production of the sound incidentally creates ripples on the water surface, providing a multimodal cue. We tested whether male frogs attend to distance-dependent cues created by a calling rival and whether their response depends on crossmodal comparisons. In a first experiment, we showed distance-dependent changes in vocal behavior: males responded more strongly with decreasing distance to a mimicked rival. In a second experiment, we showed that males can discriminate between relatively near and far rivals by using a combination of unimodal cues, specifically amplitude changes of sound and water waves, as well as crossmodal differences in arrival time. Our data reveal that animals can compare the arrival time of simultaneously emitted multimodal cues to obtain information on relative distance to a source. We speculate that communicative benefits from crossmodal comparison may have been an important driver of the evolution of elaborate multimodal displays.

71.
TITLE: Spring Peepers Pseudacris Crucifer Modify Their Call Structure in Response to Noise
AUTHORS: D.E.L. Hanna, D.R. Wilson, D. Blouin-Demers, D.J. Mennill
YEAR: 2014
SOURCE: Current Zoology
ABSTRACT: Acoustic interference can impede effective communication that is important for survival and reproduction of animals. In response to acoustic interference, some animals can improve signalling efficacy by altering the structure of their signals. In this study, we played artificial noise to 46 male spring peepers Pseudacris crucifer, on their breeding grounds, and tested whether the noise affected the duration, call rate, and peak frequency of their advertisement calls. We used two experimental noise treatments that masked either the high- or low-frequency components of an average advertisement call; this allowed us to evaluate whether frogs adaptively shift the peak frequency of their calls away from both types of interference. Our playback treatments caused spring peepers to produce shorter calls, and the high-frequency noise treatment caused them to lower the frequency of their calls immediately after the noise ceased. Call rate did not change in response to playback. Consistent with previous studies, ambient temperature was inversely related to call duration and positively related to call rate. We conclude that noise affects the structure of spring peeper advertisement calls, and that spring peepers therefore have a mechanism for altering signal structure in response to noise. Future studies should test if other types of noise, such as biotic or anthropogenic noise, have similar effects on call structure, and if the observed changes to call structure enhance or impair communication in noisy environments.

72.
TITLE: Neural Correlates of Auditory Streaming in An Objective Behavioral Task
AUTHORS: N. Itatani, G.M. Klump
YEAR: 2014
SOURCE: Proceedings of the National Academy of Sciences
ABSTRACT: When we listen in the real world, we often segregate sounds into different streams based on their acoustical features. This “auditory streaming” allows us to individuate and separately process sounds from different sources, like different voices at a cocktail party. In the present study we investigated the neuronal mechanism of streaming in a songbird while it performed a sound discrimination task that gave us an objective measure of the streamed percept. The study revealed that the songbirds' primary auditory forebrain area represents stimulus features with a high sensitivity that reflects the birds' ability for streaming. However, the bird's perceptual decisions about streamed sounds are not reflected in the neuronal responses in this auditory area, a result consistent with evidence obtained in earlier non-invasive studies on auditory streaming in human listeners. Segregating streams of sounds from sources in complex acoustic scenes is crucial for perception in real world situations. We analyzed an objective psychophysical measure of stream segregation obtained while simultaneously recording forebrain neurons in the European starlings to investigate neural correlates of segregating a stream of A tones from a stream of B tones presented at one-half the rate. The objective measure, sensitivity for time shift detection of the B tone, was higher when the A and B tones were of the same frequency (one stream) compared with when there was a 6- or 12-semitone difference between them (two streams). The sensitivity for representing time shifts in spiking patterns was correlated with the behavioral sensitivity. The spiking patterns reflected the stimulus characteristics but not the behavioral response, indicating that the birds’ primary cortical field represents the segregated streams, but not the decision process.

73.
TITLE: Detection of Modulated Tones in Modulated Noise by Non-Human Primates
AUTHORS: P. Bohlen, M.E. Dylla, C. Timms, R. Ramachandran
YEAR: 2014
SOURCE: Journal of the Association for Research in Otolaryngology
ABSTRACT: In natural environments, many sounds are amplitude-modulated. Amplitude modulation is thought to be a signal that aids auditory object formation. A previous study of the detection of signals in noise found that when tones or noise were amplitude-modulated, the noise was a less effective masker, and detection thresholds for tones in noise were lowered. These results suggest that the detection of modulated signals in modulated noise would be enhanced. This paper describes the results of experiments investigating how detection is modified when both signal and noise were amplitude-modulated. Two monkeys (Macaca mulatta) were trained to detect amplitude-modulated tones in continuous, amplitude-modulated broadband noise. When the phase difference of otherwise similarly amplitude-modulated tones and noise were varied, detection thresholds were highest when the modulations were in phase and lowest when the modulations were anti-phase. When the depth of the modulation of tones or noise was varied, detection thresholds decreased if the modulations were anti-phase. When the modulations were in phase, increasing the depth of tone modulation caused an increase in tone detection thresholds, but increasing depth of noise modulations did not affect tone detection thresholds. Changing the modulation frequency of tone or noise caused changes in threshold that saturated at modulation frequencies higher than 20 Hz; thresholds decreased when the tone and noise modulations were in phase and decreased when they were anti-phase. The relationship between reaction times and tone level were not modified by manipulations to the nature of temporal variations in the signal or noise. The changes in behavioral threshold were consistent with a model where the brain subtracted noise from signal. These results suggest that the parameters of the modulation of signals and maskers heavily influence detection in very predictable ways. These results are consistent with some results in humans and avians and form the baseline for neurophysiological studies of mechanisms of detection in noise.

74.
TITLE: Modelling the Effects of Chorus Species Composition and Caller Density on Acoustic Masking Interference in Multispecies Choruses of Crickets and Katydids
AUTHORS: 
YEAR: 2014
SOURCE: Ecological Informatics
ABSTRACT: Natural multispecies acoustic choruses such as the dusk chorus of a tropical rain forest consist of simultaneously signalling individuals of different species whose calls travel through a common shared medium before reaching their ‘intended’ receivers. This causes masking interference between signals and impedes signal detection, recognition and localization. The levels of acoustic overlap depend on a number of factors, including call structure, intensity, habitat-dependent signal attenuation and receiver tuning. In addition, acoustic overlaps should also depend on caller density and the species composition of choruses, including relative and absolute abundance of the different calling species. In this study, we used simulations to examine the effects of chorus species relative abundance and caller density on the levels of effective heterospecific acoustic overlap in multispecies choruses composed of the calls of five species of crickets and katydids that share the understorey of a rain forest in southern India. We found that on average species-even choruses resulted in higher levels of effective heterospecific acoustic overlap than choruses with strong dominance structures. This effect was found consistently across dominance levels ranging from 0.4 to 0.8 for larger choruses of forty individuals. For smaller choruses of twenty individuals, the effect was seen consistently for dominance levels of 0.6 and 0.8 but not 0.4. Effective acoustic overlap (EAO) increased with caller density but the manner and extent of increase depended both on the species' call structure and the acoustic context provided by the composition scenario. The Phaloria sp. experienced very low levels of EAO and was highly buffered to changes in acoustic context whereas other species experienced high EAO across contexts or were poorly buffered. These differences were not simply predictable from call structures. These simulation-based findings may have important implications for acoustic biodiversity monitoring and for the study of acoustic masking interference in natural environments.

75. 
TITLE: Mechanisms of Noise Robust Representation of Speech in Primary Auditory Cortex
AUTHORS: N. Mesgarani, S. David, J. Fritz., S. Shamma
YEAR: 2014
SOURCE: Proceedings of the National Academy of Sciences
ABSTRACT: We show that the auditory system maintains a robust representation of speech in noisy and reverberant conditions by preserving the same statistical distribution of responses in all conditions. Reconstructed stimulus from population of cortical neurons resembles more the original clean than the distorted signal. We show that a linear spectrotemporal receptive field model of neurons with a static nonlinearity fails to account for the neural noise reduction. Although replacing static nonlinearity with a dynamic model of synaptic depression can account for the reduction of additive noise, only the combined model with feedback gain normalization is able to predict the effects across both additive and reverberant conditions. Humans and animals can reliably perceive behaviorally relevant sounds in noisy and reverberant environments, yet the neural mechanisms behind this phenomenon are largely unknown. To understand how neural circuits represent degraded auditory stimuli with additive and reverberant distortions, we compared single-neuron responses in ferret primary auditory cortex to speech and vocalizations in four conditions: clean, additive white and pink (1/f) noise, and reverberation. Despite substantial distortion, responses of neurons to the vocalization signal remained stable, maintaining the same statistical distribution in all conditions. Stimulus spectrograms reconstructed from population responses to the distorted stimuli resembled more the original clean than the distorted signals. To explore mechanisms contributing to this robustness, we simulated neural responses using several spectrotemporal receptive field models that incorporated either a static nonlinearity or subtractive synaptic depression and multiplicative gain normalization. The static model failed to suppress the distortions. A dynamic model incorporating feed-forward synaptic depression could account for the reduction of additive noise, but only the combined model with feedback gain normalization was able to predict the effects across both additive and reverberant conditions. Thus, both mechanisms can contribute to the abilities of humans and animals to extract relevant sounds in diverse noisy environments.

76. 
TITLE: Attention Effects on Auditory Scene Analysis: Insights from Event-Related Brain Potentials
AUTHORS: M. Spielmann, E. Schroffer, S. Kotz, A. Bendixen
YEAR: 2014
SOURCE: Psychological Research
ABSTRACT: Sounds emitted by different sources arrive at our ears as a mixture that must be disentangled before meaningful information can be retrieved. It is still a matter of debate whether this decomposition happens automatically or requires the listener’s attention. These opposite positions partly stem from different methodological approaches to the problem. We propose an integrative approach that combines the logic of previous measurements targeting either auditory stream segregation (interpreting a mixture as coming from two separate sources) or integration (interpreting a mixture as originating from only one source). By means of combined behavioral and event-related potential (ERP) measures, our paradigm has the potential to measure stream segregation and integration at the same time, providing the opportunity to obtain positive evidence of either one. This reduces the reliance on zero findings (i.e., the occurrence of stream integration in a given condition can be demonstrated directly, rather than indirectly based on the absence of empirical evidence for stream segregation, and vice versa). With this two-way approach, we systematically manipulate attention devoted to the auditory stimuli (by varying their task relevance) and to their underlying structure (by delivering perceptual tasks that require segregated or integrated percepts). ERP results based on the mismatch negativity (MMN) show no evidence for a modulation of stream integration by attention, while stream segregation results were less clear due to overlapping attention-related components in the MMN latency range. We suggest future studies combining the proposed two-way approach with some improvements in the ERP measurement of sequential stream segregation.

77.
TITLE: Specialized Prefrontal “Auditory Fields”: Organization of Primate Prefrontal-Temporal Pathways
AUTHORS: M. Medalla, H. Barbas
YEAR: 2014
SOURCE: Frontiers in Neuroscience
ABSTRACT: No other modality is more frequently represented in the prefrontal cortex than the auditory, but the role of auditory information in prefrontal functions is not well understood. Pathways from auditory association cortices reach distinct sites in the lateral, orbital, and medial surfaces of the prefrontal cortex in rhesus monkeys. Among prefrontal areas, frontopolar area 10 has the densest interconnections with auditory association areas, spanning a large antero-posterior extent of the superior temporal gyrus from the temporal pole to auditory parabelt and belt regions. Moreover, auditory pathways make up the largest component of the extrinsic connections of area 10, suggesting a special relationship with the auditory modality. Here we review anatomic evidence showing that frontopolar area 10 is indeed the main frontal “auditory field” as the major recipient of auditory input in the frontal lobe and chief source of output to auditory cortices. Area 10 is thought to be the functional node for the most complex cognitive tasks of multitasking and keeping track of information for future decisions. These patterns suggest that the auditory association links of area 10 are critical for complex cognition. The first part of this review focuses on the organization of prefrontal-auditory pathways at the level of the system and the synapse, with a particular emphasis on area 10. Then we explore ideas on how the elusive role of area 10 in complex cognition may be related to the specialized relationship with auditory association cortices.

78.
TITLE: Discrimination of Ultrasonic Vocalizations by CBA/CaJ Mice (Mus musculus) Is Related to Spectrotemporal Dissimilarity of Vocalizations
AUTHORS: E.G. Neilans, D.P. Holfoth, K.E. Radziwon, C.V.Portfors, M.L. Dent
YEAR: 2014
SOURCE: PLoS ONE
ABSTRACT: The function of ultrasonic vocalizations (USVs) produced by mice (Mus musculus) is a topic of broad interest to many researchers. These USVs differ widely in spectrotemporal characteristics, suggesting different categories of vocalizations, although this has never been behaviorally demonstrated. Although electrophysiological studies indicate that neurons can discriminate among vocalizations at the level of the auditory midbrain, perceptual acuity for vocalizations has yet to be determined. Here, we trained CBA/CaJ mice using operant conditioning to discriminate between different vocalizations and between a spectrotemporally modified vocalization and its original version. Mice were able to discriminate between vocalization types and between manipulated vocalizations, with performance negatively correlating with spectrotemporal similarity. That is, discrimination performance was higher for dissimilar vocalizations and much lower for similar vocalizations. The behavioral data match previous neurophysiological results in the inferior colliculus (IC), using the same stimuli. These findings suggest that the different vocalizations could carry different meanings for the mice. Furthermore, the finding that behavioral discrimination matched neural discrimination in the IC suggests that the IC plays an important role in the perceptual discrimination of vocalizations.

79. 
TITLE: Scene Analysis in the Natural Environment
AUTHORS: M.S. Lewicki, B. Olshausen, A. Surlykke, C. Moss
YEAR: 2014
SOURCE: Frontiers in Psychology
ABSTRACT: The problem of scene analysis has been studied in a number of different fields over the past decades. These studies have led to important insights into problems of scene analysis, but not all of these insights are widely appreciated, and there remain critical shortcomings in current approaches that hinder further progress. Here we take the view that scene analysis is a universal problem solved by all animals, and that we can gain new insight by studying the problems that animals face in complex natural environments. In particular, the jumping spider, songbird, echolocating bat, and electric fish, all exhibit behaviors that require robust solutions to scene analysis problems encountered in the natural environment. By examining the behaviors of these seemingly disparate animals, we emerge with a framework for studying scene analysis comprising four essential properties: (1) the ability to solve ill-posed problems, (2) the ability to integrate and store information across time and modality, (3) efficient recovery and representation of 3D scene structure, and (4) the use of optimal motor actions for acquiring information to progress toward behavioral goals.

80.
TITLE: Analysis of Natural Scenes by Echolocation in Bats and Dolphins
AUTHORS: C. Moss, C. Chiu, P. Moore
YEAR: 2014
SOURCE: Springer Handbook of Auditory Research
ABSTRACT: Echolocation research has carefully detailed the acoustic cues used by bats and dolphins to localize and discriminate sonar targets; however, there remains an incomplete understanding of the larger problem of auditory scene analysis, namely how echo features from the natural environment are perceptually organized in the animal’s sonar receiver. This chapter reviews research that contributes to our understanding of auditory scene analysis by echolocating bats and dolphins. A review of psychophysical studies of sonar perception brings to light the limitations of understanding auditory scene analysis by echolocation when animals are constrained to a limited repertoire of emitted signals, and listening to a mix of simulated and real echoes that can compromise the perceptual salience of the experimental setting. Adaptive sonar behaviors are an integral component of echolocation systems that would be expected to feed into auditory scene analysis processes, and studies of the echolocating animal’s control over the signals it uses to probe the environment can shed light on sonar scene perception. However, adaptive sonar studies do not provide a direct measure of the animal’s perception of a complex, natural environment. Instead, auditory perception can be inferred only from the animal’s adaptive motor behaviors. Future research on auditory scene analysis by echolocation must embrace the challenge of marrying the advantages of psychophysical and adaptive motor studies, taking creative new approaches to tap into an animal’s perception of its complex, 3D auditory world, while allowing it to engage in its natural behaviors.

81.
TITLE: Vibrational Communication Networks: Eavesdropping and Biotic Noise
AUTHORS: M. Virant-Doberlet, et al.
YEAR: 2014
SOURCE: Animal Signals and Communication
ABSTRACT: In nature, communication predominantly occurs in a group of several conspecific and/or heterospecific individuals within signaling and receiving range of each other, i.e., in a network environment. Vibrational communication in the context of sexual behavior has been, in the past, usually considered as a private communication channel, free of potential competitors and eavesdropping predators or parasitoids and consequently only rarely studied outside an emitter–receiver dyad. We provide an overview of work related to vibrational communication in the presence of (a) environmental (abiotic) noise, (b) other conspecific and/or heterospecific signalers (biotic noise), (c) rivals and (d) exploiters (predators and parasitoids). The evidence gathered in the last few years shows that arthropods relying on substrate-borne vibrations communicate within a rich and complex vibrational world and reveals diverse interactions and mechanisms. Considering vibrational communication from a network perspective may allow us in the future to identify sources of selection pressures that cannot be recognized in a communication dyad.

82.
TITLE: Maintaining Acoustic Communication at a Cocktail Party: Heterospecific Masking Noise Improves Signal Detection Through Frequency Separation
AUTHORS: M.E. Siegert, H. Romer, M. Hartbauer
YEAR: 2013
SOURCE: Journal of Experimental Biology
ABSTRACT: We examined acoustic masking in a chirping katydid species of the Mecopoda elongata complex due to interference with a sympatric Mecopoda species where males produce continuous trills at high amplitudes. Frequency spectra of both calling songs range from 1 to 80 kHz; the chirper species has more energy in a narrow frequency band at 2 kHz and above 40 kHz. Behaviourally, chirper males successfully phase-locked their chirps to playbacks of conspecific chirps under masking conditions at signal-to-noise ratios (SNRs) of −8 dB. After the 2 kHz band in the chirp had been equalised to the level in the masking trill, the breakdown of phase-locked synchrony occurred at a SNR of +7 dB. The remarkable receiver performance is partially mirrored in the selective response of a first-order auditory interneuron (TN1) to conspecific chirps under these masking conditions. However, the selective response is only maintained for a stimulus including the 2 kHz component, although this frequency band has no influence on the unmasked TN1 response. Remarkably, the addition of masking noise at 65 dB sound pressure level (SPL) to threshold response levels of TN1 for pure tones of 2 kHz enhanced the sensitivity of the response by 10 dB. Thus, the spectral dissimilarity between masker and signal at a rather low frequency appears to be of crucial importance for the ability of the chirping species to communicate under strong masking by the trilling species. We discuss the possible properties underlying the cellular/synaptic mechanisms of the ‘novelty detector’.

83.
TITLE: Spatial Release from Masking Improves Sound Pattern Discrimination Along a Biologically Relevant Pulse-Rate Continuum in Gray Treefrogs
AUTHORS: J.L. Ward, N.P. Buerkle, M. Bee
YEAR: 2013
SOURCE: Hearing Research
ABSTRACT: Many frogs form large choruses during their mating season in which males produce loud advertisement calls to attract females and repel rival males. High background noise levels in these social aggregations can impair vocal perception. In humans, spatial release from masking contributes to our ability to understand speech in noisy social groups. Here, we tested the hypothesis that spatial separation between target signals and 'chorus-shaped noise' improves the ability of female gray treefrogs (Hyla chrysoscelis) to perform a behavioral discrimination task based on perceiving differences in the pulsatile structure of advertisement calls. We used two-stimulus choice tests to measure phonotaxis (approach toward sound) in response to calls differing in pulse rate along a biologically relevant continuum between conspecific (50 pulses s(-1)) and heterospecific (20 pulses s(-1)) calls. Signals were presented in quiet, in colocated noise, and in spatially separated noise. In quiet conditions, females exhibited robust preferences for calls with relatively faster pulse rates more typical of conspecific calls. Behavioral discrimination between calls differing in pulse rate was impaired in the presence of colocated noise but similar between quiet and spatially separated noise conditions. Our results indicate that spatial release from energetic masking facilitates a biologically important temporal discrimination task in frogs. We discuss these results in light of previous work on spatial release from masking in frogs and other animals.

84.
TITLE: The Lombard Effect and Other Noise-Induced Vocal Modifications: Insight from Mammalian Communication Systems
AUTHORS: C.F. Hotchkin, S. Parks
YEAR: 2013
SOURCE: Biological Reviews of the Cambridge Philosophical Society
ABSTRACT: Humans and non-human mammals exhibit fundamentally similar vocal responses to increased noise, including increases in vocalization amplitude (the Lombard effect) and changes to spectral and temporal properties of vocalizations. Different research focuses have resulted in significant discrepancies in study methodologies and hypotheses among fields, leading to particular knowledge gaps and techniques specific to each field. This review compares and contrasts noise-induced vocal modifications observed from human and non-human mammals with reference to experimental design and the history of each field. Topics include the effects of communication motivation and subject-specific characteristics on the acoustic parameters of vocalizations, examination of evidence for a proposed biomechanical linkage between the Lombard effect and other spectral and temporal modifications, and effects of noise on self-communication signals (echolocation). Standardized terminology, cross-taxa tests of hypotheses, and open areas for future research in each field are recommended. Findings indicate that more research is needed to evaluate linkages among vocal modifications, context dependencies, and the finer details of the Lombard effect during natural communication. Studies of non-human mammals could benefit from applying the tightly controlled experimental designs developed in human research, while studies of human speech in noise should be expanded to include natural communicative contexts. The effects of experimental design and behavioural context on vocalizations should not be neglected as they may impact the magnitude and type of noise-induced vocal modifications.

85.
TITLE: Take Time to Smell the Frogs: Vocal Sac Glands of Reed Frogs (Anura: Hyperoliidae) Contain Species-Specific Chemical Cocktails
AUTHORS: I. Starnberger, et al.
YEAR: 2013
SOURCE: Biological Journal of the Linnean Society
ABSTRACT: Males of all reed frog species (Anura: Hyperoliidae) have a prominent, often colourful, gular patch on their vocal sac, which is particularly conspicuous once the vocal sac is inflated. Although the presence, shape, and form of the gular patch are well-known diagnostic characters for these frogs, its function remains unknown. By integrating biochemical and histological methods, we found strong evidence that the gular patch is a gland producing volatile compounds, which might be emitted while calling. Volatile compounds were confirmed by gas chromatography–mass spectrometry in the gular glands in 11 species of the hyperoliid genera Afrixalus, Heterixalus, Hyperolius, and Phlyctimantis. Comparing the gular gland contents of 17 specimens of four sympatric Hyperolius species yielded a large variety of 65 compounds in species-specific combinations. We suggest that reed frogs might use a complex combination of at least acoustic and chemical signals in species recognition and mate choice.

86.
TITLE: The Importance of Ambient Sound Level to Characterise Anuran Habitat
AUTHORS: S. Goutte, A. Dubois, F. Legendre
YEAR: 2013
SOURCE: PLoS ONE
ABSTRACT: Habitat characterisation is a pivotal step of any animal ecology study. The choice of variables used to describe habitats is crucial and need to be relevant to the ecology and behaviour of the species, in order to reflect biologically meaningful distribution patterns. In many species, acoustic communication is critical to individuals’ interactions, and it is expected that ambient acoustic conditions impact their local distribution. Yet, classic animal ecology rarely integrates an acoustic dimension in habitat descriptions. Here we show that ambient sound pressure level (SPL) is a strong predictor of calling site selection in acoustically active frog species. In comparison to six other habitat-related variables (i.e. air and water temperature, depth, width and slope of the stream, substrate), SPL had the most important explanatory power in microhabitat selection for the 34 sampled species. Ambient noise was particularly useful in differentiating two stream-associated guilds: torrents and calmer streams dwelling species. Guild definitions were strongly supported by SPL, whereas slope, which is commonly used in stream-associated habitat, had a weak explanatory power. Moreover, slope measures are non-standardized across studies and are difficult to assess at small scale. We argue that including an acoustic descriptor will improve habitat-species analyses for many acoustically active taxa. SPL integrates habitat topology and temporal information (such as weather and hour of the day, for example) and is a simple and precise measure. We suggest that habitat description in animal ecology should include an acoustic measure such as noise level because it may explain previously misunderstood distribution patterns.

87.
TITLE: Pulse-Number Discrimination by Cope's Gray Treefrog (Hyla Chrysoscelis) in Modulated and Unmodulated Noise
AUTHORS: A. Velez, B.J. Linehan-Skillings, Y. Gu, Y. Sun, M. Bee
YEAR: 2013
SOURCE: The Journal of the Acoustical Society of America
ABSTRACT: In Cope's gray treefrog (Hyla chrysoscelis), thresholds for recognizing conspecific calls are lower in temporally modulated noise backgrounds compared with unmodulated noise. The effect of modulated noise on discrimination among different conspecific calls is unknown. In quiet, females prefer calls with relatively more pulses. This study tested the hypotheses that noise impairs selectivity for longer calls and that processes akin to dip listening in modulated noise can ameliorate this impairment. In two-stimulus choice tests, female subjects were allowed to choose between an average-length call and a shorter or longer alternative. Tests were replicated at two signal levels in quiet and in the presence of chorus-shaped noise that was unmodulated, modulated by a sinusoid, or modulated by envelopes resembling natural choruses. When subjects showed a preference, it was always for the relatively longer call. Noise reduced preferences for longer calls, but the magnitude of this reduction was unrelated to whether the noise envelope was modulated or unmodulated. Together, the results are inconsistent with the hypothesis that dip listening improves a female gray treefrog's ability to select longer calls in modulated compared with unmodulated noise.

88.
TITLE: Using a Staircase Procedure for the Objective Measurement of Auditory Stream Integration and Segregation Thresholds
AUTHORS: M. Spielman, E. Schroger, S. Kotz, T. Pechmann, A. Bendixen
YEAR: 2013
SOURCE: Frontiers in Psychology
ABSTRACT: Auditory scene analysis describes the ability to segregate relevant sounds out from the environment and to integrate them into a single sound stream using the characteristics of the sounds to determine whether or not they are related. This study aims to contrast task performances in objective threshold measurements of segregation and integration using identical stimuli, manipulating two variables known to influence streaming, inter-stimulus-interval (ISI) and frequency difference (deltaf). For each measurement, one parameter (either ISI or deltaf) was held constant while the other was altered in a staircase procedure. By using this paradigm, it is possible to test within-subject across multiple conditions, covering a wide deltaf and ISI range in one testing session. The objective tasks were based on across-stream temporal judgments (facilitated by integration) and within-stream deviance detection (facilitated by segregation). Results show the objective integration task is well suited for combination with the staircase procedure, as it yields consistent threshold measurements for separate variations of ISI and deltaf, as well as being significantly related to the subjective thresholds. The objective segregation task appears less suited to the staircase procedure. With the integration-based staircase paradigm, a comprehensive assessment of streaming thresholds can be obtained in a relatively short space of time. This permits efficient threshold measurements particularly in groups for which there is little prior knowledge on the relevant parameter space for streaming perception.

89.
TITLE: SPREAD: Sound Propagation and Perception for Autonomous Agents in Dynamic Environments
AUTHORS: P. Huang, M. Kapadia, N. Badler
YEAR: 2013
SOURCE: SCA '13: Proceedings of the 12th ACM SIGGRAPH/Eurographics Symposium on Computer Animation
ABSTRACT: The perception of sensory information and its impact on behavior is a fundamental component of being human. While visual perception is considered for navigation, collision, and behavior selection, the acoustic domain is relatively unexplored. Recent work in acoustics focuses on synthesizing sound in 3D environments; however, the perception of acoustic signals by a virtual agent is a useful and realistic adjunct to any behavior selection mechanism. In this paper, we present SPREAD, a novel agent-based sound perception model using a discretized sound packet representation with acoustic features including amplitude, frequency range, and duration. SPREAD simulates how sound packets are propagated, attenuated, and degraded as they traverse the virtual environment. Agents perceive and classify the sounds based on the locally-received packet set using a hierarchical clustering scheme, and have individualized hearing and understanding of their surroundings. Using this model, we demonstrate several simulations that greatly enrich controls and outcomes.

90.
TITLE: Interactions of Multisensory Components Perceptually Rescue Túngara Frog Mating Signals
AUTHORS: R.C. Taylor, M.J. Ryan
YEAR: 2013
SOURCE: Science
ABSTRACT: In túngara frogs, auditory and visual components of mate calling do not naturally occur together. Taylor and Ryan (p. 273, published online 6 June) now show that two signals that are unattractive to female frogs when presented alone become highly attractive when presented together. In a kind of “perceptual rescue,” the unique combination of two signals increased the receiver's interest in the previously uninteresting signals. Mating signals that females find unattractive when presented singly become attractive when combined. Sexual signals are often complex and perceived by multiple senses. How animals integrate signal components across sensory modalities can influence signal evolution. Here we show that two relatively unattractive signals that are perceived acoustically and visually can be combined in a pattern to form a signal that is attractive to female túngara frogs. Such unanticipated perceptual effects suggest that the evolution of complex signals can occur by alteration of the relationships among already-existing traits.

91.
TITLE: Perceptual Bistability in Streaming: How Much Do Stimulus Features Matter?
AUTHORS: S.L. Denham, K. Gyimesi, G. Stefanics, I. Winkler
YEAR: 2013
SOURCE: Learning and Perception
ABSTRACT: The auditory two-tone streaming paradigm has been used extensively to study the mechanisms that underlie the decomposition of the auditory input into coherent sound sequences. Using longer tone sequences than usual in the literature, we show that listeners hold their first percept of the sound sequence for a relatively long period, after which perception switches between two or more alternative sound organizations, each held on average for a much shorter duration. The first percept also differs from subsequent ones in that stimulus parameters influence its quality and duration to a far greater degree than the subsequent ones. We propose an account of auditory streaming in terms of rivalry between competing temporal associations based on two sets of processes. The formation of associations (discovery of alternative interpretations) mainly affects the first percept by determining which sound group is discovered first and how long it takes for alternative groups to be established. In contrast, subsequent percepts arise from stochastic switching between the alternatives, the dynamics of which are determined by competitive interactions between the set of coexisting interpretations.

92. 
TITLE: Call Perception in Mice
AUTHORS: E.G. Neilans, D.P. Holfoth, K.E. Radziwon, C.V. Portfors, M.L. Dent
YEAR: 2013
SOURCE: Journal of the Acoustical Society of America
ABSTRACT: Acoustic communication in laboratory mice is a relatively recent subject of experimental study, often yielding disparate findings. For example, researchers often manually place mouse ultrasonic vocalizations (USVs) into categories based on spectrotemporal characteristics, but the numbers and types of categories differ widely between laboratories. Here, we attempt to determine what cues CBA/CaJ mice use to discriminate between vocalizations by testing them in an operant conditioning paradigm. The mice were trained to discriminate a repeating background containing one USV from several target USVs. The targets were different call types used by Holmstrom et al. (2010) and manipulations of the background calls, such as removing the frequency modulation, shifting the entire call up or down in frequency, shortening or lengthening the call, or reversing the entire call. Results show that large frequency shifts were easy for the mice to discriminate, while reversing the calls and removing the frequency modulation were much more difficult. For most calls, similarity in spectrotemporal characteristics yielded poor discrimination performance. These results are the first to show that mice can discriminate between some vocalizations but not others, and that they may place different meaning to different call types, though not necessarily the call types designated by humans.

93.
TITLE: Auditory Object Formation in Cope's Great Treefrogs (Hyla Chrysoscelis)
AUTHORS: M. Bee, K.M. Schrode
YEAR: 2013
SOURCE: Journal of the Acoustical Society of America
ABSTRACT: Hearing and acoustic communication in 'real world,' multi-source environments require animals to group sound elements produced by the same source into perceptually coherent 'auditory objects.' However, research on nonhuman animal communication rarely investigates perceptual processes involved in forming auditory objects of communication sounds. We tested the hypotheses that spectral and spatial proximity promote the sequential integration of temporally separated sounds produced by the same source into coherent auditory objects of acoustic signals. Male gray treefrogs produce a pulsatile advertisement call; females prefer longer calls (= more pulses) to shorter calls and discriminate against calls missing pulses. We gave females a choice between a short but spectrally and spatially coherent call (25 pulses) and a longer call (35 pulses) in which alternating groups of 5 pulses had different frequencies (deltaf, 0-12 semitones) and came from different locations (deltatheta, 0 degrees or 90 degrees). Females generally preferred the longer call at smaller values of deltaf and deltatheta, indicating a role for spectral and spatial proximity in sequential integration. Under some conditions, however, subjects showed a surprising willingness to integrate pulses despite large deltafs. Together, these data shed light on the perceptual cues that receivers exploit to form coherent auditory objects of communication sounds.

94.
TITLE: Signal Recognition by Green Treefrogs (Hyla Cinerea) and Cope's Gray Treefrogs (Hyla Chrysoscelis) in Naturally Fluctuating Noise
AUTHORS: A. Velez, M. Bee
YEAR: 2013
SOURCE: Journal of Comparative Psychology
ABSTRACT: This study tested three hypotheses about the ability of female frogs to exploit temporal fluctuations in the level of background noise to overcome the problem of recognizing male advertisement calls in noisy breeding choruses. Phonotaxis tests with green treefrogs (Hyla cinerea) and Cope's gray treefrogs (Hyla chrysoscelis) were used to measure thresholds for recognizing calls in the presence of noise maskers with (a) no level fluctuations, (b) random fluctuations, or level fluctuations characteristic of (c) conspecific choruses and (d) heterospecific choruses. The dip-listening hypothesis predicted lower signal recognition thresholds in the presence of fluctuating maskers compared with nonfluctuating maskers. Support for the dip-listening hypothesis was weak; only Cope's gray treefrogs experienced dip listening and only in the presence of randomly fluctuating maskers. The natural soundscapes advantage hypothesis predicted lower recognition thresholds when level fluctuations resembled those of natural soundscapes compared with artificial fluctuations. This hypothesis was rejected. In noise backgrounds with natural fluctuations, the species-specific advantage hypothesis predicted lower recognition thresholds when fluctuations resembled species-specific patterns of conspecific soundscapes. No evidence was found to support this hypothesis. These results corroborate previous findings showing that Cope's gray treefrogs, but not green treefrogs, experience dip listening under some noise conditions. Together, the results suggest level fluctuations in the soundscape of natural breeding choruses may present few dip-listening opportunities. The findings of this study provide little support for the hypothesis that receivers are adapted to exploit level fluctuations of natural soundscapes in recognizing communication signals.

95.
TITLE: Behavioral Sensitivity to Broadband Binaural Localization Cues in the Ferret
AUTHORS: P. Keating, F.R. Nodal, K. Gananandan, A.L. Schulz, A.J. King
YEAR: 2013
SOURCE: Journal of the Association for Research in Otolaryngology
ABSTRACT: Although the ferret has become an important model species for studying both fundamental and clinical aspects of spatial hearing, previous behavioral work has focused on studies of sound localization and spatial release from masking in the free field. This makes it difficult to tease apart the role played by different spatial cues. In humans and other species, interaural time differences (ITDs) and interaural level differences (ILDs) play a critical role in sound localization in the azimuthal plane and also facilitate sound source separation in noisy environments. In this study, we used a range of broadband noise stimuli presented via customized earphones to measure ITD and ILD sensitivity in the ferret. Our behavioral data show that ferrets are extremely sensitive to changes in either binaural cue, with levels of performance approximating that found in humans. The measured thresholds were relatively stable despite extensive and prolonged (>16 weeks) testing on ITD and ILD tasks with broadband stimuli. For both cues, sensitivity was reduced at shorter durations. In addition, subtle effects of changing the stimulus envelope were observed on ITD, but not ILD, thresholds. Sensitivity to these cues also differed in other ways. Whereas ILD sensitivity was unaffected by changes in average binaural level or interaural correlation, the same manipulations produced much larger effects on ITD sensitivity, with thresholds declining when either of these parameters was reduced. The binaural sensitivity measured in this study can largely account for the ability of ferrets to localize broadband stimuli in the azimuthal plane. Our results are also broadly consistent with data from humans and confirm the ferret as an excellent experimental model for studying spatial hearing.

96.
TITLE: Auditory Stream Segregation for Alternating and Synchronous Tones
AUTHORS: C. Micheyl, C. Hanson, L. Demany, S. Shamma, A. Oxenham
YEAR: 2013
SOURCE: Journal of Experimental Psychology. Human Perception and Performance
ABSTRACT: Sound sequences, such as music, are usually organized perceptually into concurrent "streams." The mechanisms underlying this "auditory streaming" phenomenon are not completely known. The present study sought to test the hypothesis that synchrony limits listeners' ability to separate sound streams. To test this hypothesis, both perceptual-organization judgments and performance measures were used. In Experiment 1, listeners indicated whether they perceived sequences of alternating or synchronous tones as a single stream or as two streams. In Experiments 2 and 3, listeners detected rare changes in the intensity of "target" tones at one frequency in the presence of synchronous or asynchronous random-intensity "distractor" tones at another frequency. The results of these experiments showed that, for large frequency separations between the tones, the probability of perceiving two streams was lower on average for synchronous than for alternating tones, and that sensitivity to intensity changes in the target sequence was greater for asynchronous than for synchronous distractors. Overall, these results are consistent with the hypothesis that synchrony limits listeners' ability to form separate streams and/or to attend selectively to certain sounds in the presence of other sounds, even when the target and distractor sounds are well separated from each other in frequency.

97.
TITLE: Spectral Niche Segregation and Community Organization in a Tropical Cricket Assemblage
AUTHORS: A.K.D. Schmidt, H. Romer, K. Riede
YEAR: 2013
SOURCE: Behavioral Ecology
ABSTRACT: In species-rich biomes such as tropical rainforests, the efficiency of intraspecific acoustic communication will strongly depend on the degree of signal overlap. Signal interference deteriorates detection, recognition, and localization of conspecific signals. Thus, the communication space should be partitioned sufficiently to reduce masking interference and to promote intraspecific communication. Here, we studied the community organization of a tropical cricket assemblage with respect to its multidimensional niche axes, such as song frequencies, space (horizontal and vertical), and time, affecting acoustic communication. We used the null model approach to test whether observed community patterns differed from those expected by chance. The assemblage clearly showed partitioning in the spectral domain of calling frequencies of their songs. Furthermore, the range of song frequencies occupied by species is positively correlated with the distance to the average calling frequency of its adjacent neighbors. Thus, species tended to use a greater range of frequency channels for intraspecific communication if the frequency space is available. Our results support the idea that competition for the acoustic communication channel may have resulted in niche segregation along the frequency axes. Concerning the spatiotemporal organization at the community level the spatial (horizontal) distribution appeared to be randomly structured, whereas we found a significant vertical stratification between species. At a temporal scale, the assemblage aggregated their calling activity with an observed niche overlap significantly greater than expected by chance. However, combining the spatial and temporal distribution resulted in low co-occurrence of pairwise species association, consequently reducing chances for masking events.

98.
TITLE: Noise-invariant Neurons in the Avian Auditory Cortex: Hearing the Song in Noise
AUTHORS: R.C. Moore, T. Lee, F. Theunissen
YEAR: 2013
SOURCE: PLoS Computational Biology
ABSTRACT: Given the extraordinary ability of humans and animals to recognize communication signals over a background of noise, describing noise invariant neural responses is critical not only to pinpoint the brain regions that are mediating our robust perceptions but also to understand the neural computations that are performing these tasks and the underlying circuitry. Although invariant neural responses, such as rotation-invariant face cells, are well described in the visual system, high-level auditory neurons that can represent the same behaviorally relevant signal in a range of listening conditions have yet to be discovered. Here we found neurons in a secondary area of the avian auditory cortex that exhibit noise-invariant responses in the sense that they responded with similar spike patterns to song stimuli presented in silence and over a background of naturalistic noise. By characterizing the neurons' tuning in terms of their responses to modulations in the temporal and spectral envelope of the sound, we then show that noise invariance is partly achieved by selectively responding to long sounds with sharp spectral structure. Finally, to demonstrate that such computations could explain noise invariance, we designed a biologically inspired noise-filtering algorithm that can be used to separate song or speech from noise. This novel noise-filtering method performs as well as other state-of-the-art de-noising algorithms and could be used in clinical or consumer oriented applications. Our biologically inspired model also shows how high-level noise-invariant responses could be created from neural responses typically found in primary auditory cortex.

99. 
TITLE: Multimodal Signaling in the Small Torrent Frog (Micrixalus Saxicola) in a Complex Acoustic Environment
AUTHORS: D. Preininger, M. Boeckle, A. Freudmann, I. Starnberger, M. Sztatecsny, W. Hoedl
YEAR: 2013
SOURCE: Behavioral Ecology and Sociobiology
ABSTRACT: Many animals use multimodal (both visual and acoustic) components in courtship signals. The acoustic communication of anuran amphibians can be masked by the presence of environmental background noise, and multimodal displays may enhance receiver detection in complex acoustic environments. In the present study, we measured sound pressure levels of concurrently calling males of the Small Torrent Frog (Micrixalus saxicola) and used acoustic playbacks and an inflatable balloon mimicking a vocal sac to investigate male responses to controlled unimodal (acoustic) and multimodal (acoustic and visual) dynamic stimuli in the frogs’ natural habitat. Our results suggest that abiotic noise of the stream does not constrain signal detection, but males are faced with acoustic interference and masking from conspecific chorus noise. Multimodal stimuli elicited greater response from males and triggered significantly more visual signal responses than unimodal stimuli. We suggest that the vocal sac acts as a visual cue and improves detection and discrimination of acoustic signals by making them more salient to receivers amidst complex biotic background noise.

100.
TITLE: Stimulus Change Detection in Phasic Auditory Units in the Frog Midbrain: Frequency and Ear Specific Adaptation
AUTHORS: A. Ponnath, K. Hoke, H. Farris
YEAR: 2013
SOURCE: Journal of Comparative Physiology A
ABSTRACT: Neural adaptation, a reduction in the response to a maintained stimulus, is an important mechanism for detecting stimulus change. Contributing to change detection is the fact that adaptation is often stimulus specific: adaptation to a particular stimulus reduces excitability to a specific subset of stimuli, while the ability to respond to other stimuli is unaffected. Phasic cells (e.g., cells responding to stimulus onset) are good candidates for detecting the most rapid changes in natural auditory scenes, as they exhibit fast and complete adaptation to an initial stimulus presentation. We made recordings of single phasic auditory units in the frog midbrain to determine if adaptation was specific to stimulus frequency and ear of input. In response to an instantaneous frequency step in a tone, 28 % of phasic cells exhibited frequency specific adaptation based on a relative frequency change (delta-f = ±16 %). Frequency specific adaptation was not limited to frequency steps, however, as adaptation was also overcome during continuous frequency modulated stimuli and in response to spectral transients interrupting tones. The results suggest that adaptation is separated for peripheral (e.g., frequency) channels. This was tested directly using dichotic stimuli. In 45 % of binaural phasic units, adaptation was ear specific: adaptation to stimulation of one ear did not affect responses to stimulation of the other ear. Thus, adaptation exhibited specificity for stimulus frequency and lateralization at the level of the midbrain. This mechanism could be employed to detect rapid stimulus change within and between sound sources in complex acoustic environments.

101.
TITLE: Anuran Acoustic Signal Production in Noisy Environments
AUTHORS: J.J. Schwartz, M. Bee
YEAR: 2013
SOURCE: Animal Signals and Communication
ABSTRACT: Choruses of acoustically signaling frogs and toads are among the most impressive acoustic spectacles known from the natural world. They are loud, raucous social environments that form for one purpose and one purpose only: sex. The loud sexual advertisement signals that males produce are often necessary and sufficient to elicit responses from reproductive females, and they also function in communicating with other males during interactions over calling sites and territories. Frogs listening in a chorus must detect, recognize, localize, and discriminate among competing signals amid high levels of biotic, and often abiotic, background noise. In essence, frogs must solve a biological analog of the human cocktail party problem. In this chapter, we describe the frog’s cocktail party problem in functional terms relevant to frog reproduction and communication. We then describe results from experimental studies, mostly of behavior, that elucidate how the frog auditory system goes about solving problems related to auditory masking and auditory scene analysis.

102.
TITLE: Rule-Encoding Neurons in Prefrontal and Auditory Cortex of Rats Performing a Task Similar to the Cocktail Party Problem
AUTHORS: C. Rodgers
YEAR: 2013
SOURCE: 
ABSTRACT: The human auditory system easily solves the "cocktail party problem" - that is, even when multiple people are speaking at once, we can easily select and pay attention to a single voice while ignoring the others. Though this seems easy to do, the problem is known to be quite computationally complex. It requires identifying the important sound, selecting it for special processing, and using information from it to make behavioral decisions; meanwhile, the other voices must not be allowed to distract us.How does the brain do this? In chapter 1, I review previous approaches to this question and motivate the choices I made in designing my experiments. In chapter 2, I present the data and conclusions I obtained in collaboration with my advisor, Dr Michael DeWeese. (We are submitting this chapter for publication separately.) In chapter 3, I present a detailed protocol for repeating our behavioral results.The final chapter, Chapter 4, is broader in scope. I discuss how our models and results relate to existing models of prefrontal control over other brain regions. Finally, I consider what my results have taught me about the scientific process of investigating neural function and ruminate on where this field may be headed next.

103.
TITLE: Calling Dynamics and Call Synchronization in a Local Group of Unison Bout Callers
AUTHORS: D. Jones, R. Jones, R. Ratnam
YEAR: 2013
SOURCE: Journal of Comparative Physiology A
ABSTRACT: In many species of chorusing frogs, callers can rapidly adjust their call timing with reference to neighboring callers so as to maintain call rate while minimizing acoustic interference. The rules governing the interactions, in particular, who is listening to whom are largely unknown, presumably influenced by distance between callers, caller density, and intensities of interfering calls. We report vocal interactions in a unison bout caller, the green tree frog (Hyla cinerea). Using a microphone array, we monitored bouts from a local group of six callers embedded in a larger chorus. Data were analyzed in a 21-min segment at the peak of the chorus. Callers within this group were localized and their voices were separated for analysis of spatio-temporal interactions. We show that callers in this group: (1) synchronize with one another, (2) prefer to time their calls antiphonally, almost exactly at one-third and two-thirds of the call intervals of their neighbors, (3) tolerate call collision when antiphonal calling is not possible, and (4) perform discrete phase-hopping between three preferred phases when tracking other callers. Further, call collision increases and phase-locking decreases, with increasing inter-caller spacing. We conclude that the precise phase-positioning, phase-tracking, and phase-hopping minimizes acoustic jamming while maintaining chorus synchrony.

104.
TITLE: Multi-Pitch Streaming : Based on Clustering Algorithms
AUTHORS: Y. Gao
YEAR: 2013
SOURCE:
ABSTRACT: Multi-pitch analysis is an important subjects in audio signal processing, while at the same time it is also a very challenging issue. In this paper, we are mainly focusing on the multi-pitch streaming part, while in order to perform the streaming process, we need first estimate the pitch values in individual time frame. Thus a multi-pitch estimation process is also provided here. Recently both multi-pitch estimation and multi-pitch streaming are receiving many research interests and several advances in these two areas were made. In this paper we will describe two categories of multipitch estimation or MPE methods. And then we are going to introduce the basic concept of clustering as well as several clustering algorithms based on this concept. For each clustering algorithm presented, a recently proposed multipitch streaming method using this clustering algorithm will be detailed discussed. Experimental results and some defects of these methods will also be provided.

105.
TITLE: Masking by Noise in Acoustic Insects: Problems and Solutions
AUTHORS: H. Romer
YEAR: 2013
SOURCE:  Animal Signals and Communication
ABSTRACT: In most environments, acoustic signals of insects are a source of high background noise levels for many birds and mammals, but at the same time, their own communication channel is noisy due to conspecific and heterospecific signalers as well. In this chapter, I first demonstrate how this situation influences communication and the evolution of related traits at the population level. Solutions for communicating under noise differ between insect taxa, because their hearing system evolved independently many times, and the signals vary strongly in the time and frequency domain. After describing some solutions from the senders’ point of view the focus of the chapter is on properties of the sensory and central nervous system, and how these properties enable receivers to detect relevant acoustic events from irrelevant noise, and to discriminate between signal variants.

106.
TITLE: A Rain Forest Dusk. Chorus: Cacophony or Sounds of Silence
AUTHORS: M. Jain, S. Diwakar, J. Bahuleyan, R. Deb, R. Balakrishnan
YEAR: 2013
SOURCE: Evolutionary Ecology
ABSTRACT: A rain forest dusk chorus consists of a large number of individuals of acoustically communicating species signaling at the same time. How different species achieve effective intra-specific communication in this complex and noisy acoustic environment is not well understood. In this study we examined acoustic masking interference in an assemblage of rain forest crickets and katydids. We used signal structures and spacing of signalers to estimate temporal, spectral and active space overlap between species. We then examined these overlaps for evidence of strategies of masking avoidance in the assemblage: we asked whether species whose signals have high temporal or spectral overlap avoid calling together. Whereas we found evidence that species with high temporal overlap may avoid calling together, there was no relation between spectral overlap and calling activity. There was also no correlation between the spectral and temporal overlaps of the signals of different species. In addition, we found little evidence that species calling in the understorey actively use spacing to minimize acoustic overlap. Increasing call intensity and tuning receivers however emerged as powerful strategies to minimize acoustic overlap. Effective acoustic overlaps were on average close to zero for most individuals in natural, multispecies choruses, even in the absence of behavioral avoidance mechanisms such as inhibition of calling or active spacing. Thus, call temporal structure, intensity and frequency together provide sufficient parameter space for several species to call together yet communicate effectively with little interference in the apparent cacophony of a rain forest dusk chorus.

107.
TITLE: Sound Source Segregation in the Acoustic Parasitiod Fly Ormia Ochracea
AUTHORS: N. Lee
YEAR: 2012
SOURCE: University of Toronto
ABSTRACT: Sound source localization depends on the auditory system to identify, recognize, and segregate elements of salient sources over distracting noise. My research investigates sensory mechanisms involved in these auditory processing tasks of an insect hearing specialist, to isolate individual sound sources of interest over noise. I first developed quantitative methods to determine signal features that the acoustic parasitoid fly Ormia ochracea (Diptera: Tachinidae) evaluate for host cricket song recognition. With flies subjected to a no-choice paradigm and forced to track a switch in the broadcast location of test songs, I describe several response features (distance, steering velocity, and angular orientation) that vary with song pulse rate preferences. I incorporate these response measures in a phonotaxis performance index that is sensitive to capturing response variation that may underlie song recognition. I demonstrate that Floridian O. ochracea exhibit phonotaxis to a combination of pulse durations and interpulse intervals that combine to a range of accepted pulse periods. Under complex acoustic conditions of multiple coherent cricket songs that overlap in time and space, O. ochracea may experience a phantom source illusion and localize a direction between actual source locations. By varying the temporal overlap between competing sources, I demonstrate that O. ochracea are able to resolve this illusion via the precedence effect: exploitation of small time differences between competing sources to selectively localize the leading over lagging sources. An increase in spatial separation between cricket song and masking noise does not reduce song detection thresholds nor improve song localization accuracy. Instead, walking responses are diverted away from both song and noise. My findings support the idea that the ears of O. ochracea function as bilateral symmetry .

108.
TITLE: The Build-up of Auditory Stream Segregation: A Different Perspective
AUTHORS: S. Deike, P. Heil, M.Bockmann-Barthel, A. 
YEAR: 2012
SOURCE: Frontiers in Psychology
ABSTRACT: The build-up of auditory stream segregation refers to the notion that sequences of alternating A and B sounds initially tend to be heard as a single stream, but with time appear to split into separate streams. The central assumption in the analysis of this phenomenon is that streaming sequences are perceived as one stream at the beginning by default. In the present study, we test the validity of this assumption and document its impact on the apparent build-up phenomenon. Human listeners were presented with ABAB sequences, where A and B were harmonic tone complexes of seven different fundamental frequency separations (deltaf) ranging from 2 to 14 semitones. Subjects had to indicate, as promptly as possible, their initial percept of the sequences, as either “one stream” or “two streams,” and any changes thereof during the sequences. We found that subjects did not generally indicate a one-stream percept at the beginning of streaming sequences. Instead, the first perceptual decision depended on deltaf, with the probability of a one-stream percept decreasing, and that of a two-stream percept increasing, with increasing deltaf. Furthermore, subjects required some time to make and report a decision on their perceptual organization. Taking this time into account, the resulting time courses of two-stream probabilities differ markedly from those suggested by the conventional analysis. A build-up-like increase in two-stream probability was found only for the deltaf of six semitones. At the other deltaf conditions no or only minor increases in two-stream probability occurred. These results shed new light on the build-up of stream segregation and its possible neural correlates.

109.
TITLE: Dip Listening or Modulation Masking? Call Recognition by Green Treefrogs (Hyla Cinerea) in Temporally Fluctuating Noise
AUTHORS: A. Velez, G. Hobel, N.M. Gordon, M. Bee
YEAR: 2012
SOURCE: Journal of Comparative Physiology A
ABSTRACT: Despite the importance of perceptually separating signals from background noise, we still know little about how nonhuman animals solve this problem. Dip listening, an ability to catch meaningful ‘acoustic glimpses’ of a target signal when fluctuating background noise levels momentarily drop, constitutes one possible solution. Amplitude-modulated noises, however, can sometimes impair signal recognition through a process known as modulation masking. We asked whether fluctuating noise simulating a breeding chorus affects the ability of female green treefrogs (Hyla cinerea) to recognize male advertisement calls. Our analysis of recordings of the sounds of green treefrog choruses reveal that their levels fluctuate primarily at rates below 10 Hz. In laboratory phonotaxis tests, we found no evidence for dip listening or modulation masking. Mean signal recognition thresholds in the presence of fluctuating chorus-like noises were never statistically different from those in the presence of a non-fluctuating control. An analysis of statistical effects sizes indicates that masker fluctuation rates, and the presence versus absence of fluctuations, had negligible effects on subject behavior. Together, our results suggest that females listening in natural settings should receive no benefits, nor experience any additional constraints, as a result of level fluctuations in the soundscape of green treefrog choruses.

110.
TITLE: Blind Extraction and Localization of Sound Sources Using Point Sources Based Approaches
AUTHORS: S.F. Wu, N. Zhu
YEAR: 2012
SOURCE: The Journal of the Acoustical Society of America
ABSTRACT: This paper presents theoretical models for blind sound source localization and separation of the signals emitted by arbitrary point sources in free space. Source localizations are achieved by a model based approach that accounts for the spherical spreading of an acoustic wave and utilizes an iterative triangulation, based on the signals measured by a three-dimensional microphone array. Once source locations are determined, the source signals are separated by using the point source separation (PSS) method, which is valid for all types of signals, including harmonic, continuous, transient, random, narrowband and broadband. General solutions for signals separation are presented. Theoretically, PSS can reconstruct the individual source signals exactly. This is because it employs the free-space Green's function, which defines the exact correlation among individual sources and measurement microphones. To validate PSS, numerical simulations are carried out and results are compared with those obtained by FastICA (Independent Component Analysis) code. The impacts of various parameters such as the microphone configuration, type of source signals, signal to noise ratio, number of microphones and source localization errors on the quality of signals separation by using PSS and FastICA are examined. The advantages and disadvantages of PSS and FastICA are compared and discussed

111.
TITLE: Selective Cortical Representation of Attended Speaker in Multi-Talker Speech Perception
AUTHORS: N. Mesgarani, E. Chang
YEAR: 2012
SOURCE: Nature
ABSTRACT: Humans possess a remarkable ability to attend to a single speaker’s voice in a multi-talker background. How the auditory system manages to extract intelligible speech under such acoustically complex and adverse listening conditions is not known, and, indeed, it is not clear how attended speech is internally represented. Here, using multi-electrode surface recordings from the cortex of subjects engaged in a listening task with two simultaneous speakers, we demonstrate that population responses in non-primary human auditory cortex encode critical features of attended speech: speech spectrograms reconstructed based on cortical responses to the mixture of speakers reveal the salient spectral and temporal features of the attended speaker, as if subjects were listening to that speaker alone. A simple classifier trained solely on examples of single speakers can decode both attended words and speaker identity. We find that task performance is well predicted by a rapid increase in attention-modulated neural selectivity across both single-electrode and population-level cortical responses. These findings demonstrate that the cortical representation of speech does not merely reflect the external acoustic environment, but instead gives rise to the perceptual aspects relevant for the listener’s intended goal.

112.
TITLE: Neural Mechanisms of Rhythmic Masking Release in Monkey Primary Auditory Cortex: Implications for Models of Auditory Scene Analysis.
AUTHORS: Y. Fishman, C. Michel, M. Steinschneider
YEAR: 2012
SOURCE: Journal of Neurophysiology
ABSTRACT: The ability to detect and track relevant acoustic signals embedded in a background of other sounds is crucial for hearing in complex acoustic environments. This ability is exemplified by a perceptual phenomenon known as "rhythmic masking release" (RMR). To demonstrate RMR, a sequence of tones forming a target rhythm is intermingled with physically identical "Distracter" sounds that perceptually mask the rhythm. The rhythm can be "released from masking" by adding "Flanker" tones in adjacent frequency channels that are synchronous with the Distracters. RMR represents a special case of auditory stream segregation, whereby the target rhythm is perceptually segregated from the background of Distracters when they are accompanied by the synchronous Flankers. The neural basis of RMR is unknown. Previous studies suggest the involvement of primary auditory cortex (A1) in the perceptual organization of sound patterns. Here, we recorded neural responses to RMR sequences in A1 of awake monkeys in order to identify neural correlates and potential mechanisms of RMR. We also tested whether two current models of stream segregation, when applied to these responses, could account for the perceptual organization of RMR sequences. Results suggest a key role for suppression of Distracter-evoked responses by the simultaneous Flankers in the perceptual restoration of the target rhythm in RMR. Furthermore, predictions of stream segregation models paralleled the psychoacoustics of RMR in humans. These findings reinforce the view that preattentive or "primitive" aspects of auditory scene analysis may be explained by relatively basic neural mechanisms at the cortical level.

113.
TITLE: Sound Source Perception in Anuran Amphibians
AUTHORS: M. Bee
YEAR: 2012
SOURCE: Current Opinion in Neurobiology
ABSTRACT: Sound source perception refers to the auditory system's ability to parse incoming sensory information into coherent representations of distinct sound sources in the environment. Such abilities are no doubt key to successful communication in many taxa, but we know little about their function in animal communication systems. For anuran amphibians (frogs and toads), social and reproductive behaviors depend on a listener's ability to hear and identify sound signals amid high levels of background noise in acoustically cluttered environments. Recent neuroethological studies are revealing how frogs parse these complex acoustic scenes to identify individual calls in noisy breeding choruses. Current evidence highlights some interesting similarities and differences in how the auditory systems of frogs and other vertebrates (most notably birds and mammals) perform auditory scene analysis.

114.
TITLE: Spatial Release from Masking in a Free-Field Source Identification Task by Gray Treefrogs
AUTHORS: V. Nityananda, M. Bee
YEAR: 2012
SOURCE: Hearing Research
ABSTRACT: Humans and other animals often communicate acoustically in noisy social groups, in which the background noise generated by other individuals can mask signals of interest. When listening to speech in the presence of speech-like noise, humans experience a release from auditory masking when target and masker are spatially separated. We investigated spatial release from masking (SRM) in a free-field call recognition task in Cope's gray treefrog (Hyla chrysoscelis). In this species, reproduction requires that females successfully detect, recognize, and localize a conspecific male in the noisy social environment of a breeding chorus. Using no-choice phonotaxis assays, we measured females' signal recognition thresholds in response to a target signal (an advertisement call) in the presence and absence of chorus-shaped noise. Females experienced about 3 dB of masking release, compared with a co-localized condition, when the masker was displaced 90° in azimuth from the target. The magnitude of masking release was independent of the spectral composition of the target (carriers of 1.3 kHz, 2.6 kHz, or both). Our results indicate that frogs experience a modest degree of spatial unmasking when performing a call recognition task in the free-field, and suggest that variation in signal spectral content has small effects on both source identification and spatial unmasking. We discuss these results in the context of spatial unmasking in vertebrates and call recognition in frogs.

115.
TITLE: Effects of Noise Bandwidth and Amplitude Modulation on Masking in Frog Auditory Midbrain Neurons
AUTHORS: J.B.M. Goense, A.S. Feng
YEAR: 2012
SOURCE: PLoS ONE
ABSTRACT: Natural auditory scenes such as frog choruses consist of multiple sound sources (i.e., individual vocalizing males) producing sounds that overlap extensively in time and spectrum, often in the presence of other biotic and abiotic background noise. Detection of a signal in such environments is challenging, but it is facilitated when the noise shares common amplitude modulations across a wide frequency range, due to a phenomenon called comodulation masking release (CMR). Here, we examined how properties of the background noise, such as its bandwidth and amplitude modulation, influence the detection threshold of a target sound (pulsed amplitude modulated tones) by single neurons in the frog auditory midbrain. We found that for both modulated and unmodulated masking noise, masking was generally stronger with increasing bandwidth, but it was weakened for the widest bandwidths. Masking was less for modulated noise than for unmodulated noise for all bandwidths. However, responses were heterogeneous, and only for a subpopulation of neurons the detection of the probe was facilitated when the bandwidth of the modulated masker was increased beyond a certain bandwidth – such neurons might contribute to CMR. We observed evidence that suggests that the dips in the noise amplitude are exploited by TS neurons, and observed strong responses to target signals occurring during such dips. However, the interactions between the probe and masker responses were nonlinear, and other mechanisms, e.g., selective suppression of the response to the noise, may also be involved in the masking release.

116.
TITLE: Frequency-Specificity and Pattern-Specificity of the Buildup of Auditory Stream Segregation
AUTHORS: D.M. Weintraub
YEAR: 2012
SOURCE: University of Nevada, Las Vegas
ABSTRACT: During repeating sequences of low (A) and high (B) tones in an “...ABAB...” pattern, the likelihood of hearing two separate streams (“streaming”) increases with more repetitions of the patterns, a phenomenon referred to as “buildup”. Previous studies have shown that buildup is frequency specific (Anstis & Saida, 1985) and that its biasing effects decays over several seconds (Beauvois & Meddis, 1997). No study has examined whether the frequency specificity of buildup persists for such a long duration. To address these issues, Experiment 1 tested the decay of frequency-specific and non-frequency specific buildup. The results revealed that (1) frequency-specific buildup effects were strongest during short decay intervals and decayed with longer intervals, (2) non- frequency-specific buildup showed weaker buildup effects and less decay, and (3) both types of buildup had significant effects compared to a silence baseline comparison even after long decay intervals. It is assumed non-frequency-specific buildup involved mechanisms in a high-level auditory area not finely tuned to frequency and sensitive to complex features. Therefore, Experiment 2 tested whether mechanisms subserving buildup occur in areas of the auditory pathway sensitive to rhythmic pattern. The main results revealed that (1) frequency-specific and non-frequency specific buildup effects were both disrupted by rhythmic pattern irregularity given their effects were large without such irregularity, and (2) replicated all other aspects of Experiment 1. The results of both experiments confirmed the presence of a frequency-specific mechanism subserving buildup that may be longer-lasting than previously recognized and further supported the presence of non-frequency specific mechanisms that are also long-lasting. Additionally, buildup appeared to involve mechanisms in high-level auditory areas sensitive to rhythmic pattern. Taken together, this study demonstrated buildup is a complex process that involves multiple levels of analysis along the auditory pathway.

117.
TITLE: Auditory Object Analysis
AUTHORS: T.D. Griffiths, S. Kumar, K. Kriegstein, T. Overath, K. Stephan, K.J. Friston
YEAR: 2012
SOURCE: Springer Handbook of Auditory Research
ABSTRACT: The concept of what constitutes an auditory object is controversial (Kubovy & Van Valkenburg, 2001; Griffi ths & Warren, 2004; Nelken, 2004). It is more difficult to examine the sound pressure waveform that enters the cochlea and “see” different objects in the same way that we “see” objects in the visual input to the retina. However, in both the auditory system and the visual system, objects can be understood in terms of the “images” they produce during the processing of sense data. The idea that objects are mental events that result from the creation of images from sense data goes back to Kant (1929). Visual images, representations in the visual brain corresponding to objects, can be understood as having two spatial dimensions.

118.
TITLE: The Role of Top-Down Attention in the Cocktail Party: Revisiting Cherry's Experiment after Sixty Years
AUTHORS: L. Marchegiani, S.G. Karadogan, T. Andersen, J.Larsen, L. Hansen
YEAR: 2011
SOURCE: 2011 10th International Conference on Machine Learning and Applications and Workshops
ABSTRACT: We investigate the role of top-down task drive attention in the cocktail party problem. In a recently proposed computational model of top-down attention it is possible to simulate the cocktail party problem and make predictions about sensitivity to confounders under different levels of attention. Based on such simulations we expect that under strong top-down attention pattern recognition is improved as the model can compensate for noise and confounders. We next investigate the role of temporal and spectral overlaps and speech intelligibility in humans, and how the presence of a task influences their relation. For this purpose, we perform behavioral experiments inspired by Cherry's classic experiments carried out almost sixty years ago. We make participants listen to a mono signal consisting of two different narratives pronounced by a speech synthesizer under two different conditions. In the first case, participants listen with no specific task, while in the second one they are asked to follow one of the stories. Participants report the words they heard by choosing from a list which also includes terms not present in any of the narratives. We define temporal and spectral overlaps using the ideal binary mask (IBMs) as a gauge. We analyze the correlation between overlaps and the amount of reported words. We observe a significant negative correlation when there is no task, while no correlation is detected when a task is involved. Hence, results that are well aligned with the simulation results in our computational top-down attention model.

119.
TITLE: Solutions to the Cocktail Party Problem in Insects: Selective Filters, Spatial Release from Masking and Gain Control in Tropical Crickets
AUTHORS: A.K.D. Schmidt, H. Romer
YEAR: 2011
SOURCE: PLoS ONE
ABSTRACT: Insects often communicate by sound in mixed species choruses; like humans and many vertebrates in crowded social environments they thus have to solve cocktail-party-like problems in order to ensure successful communication with conspecifics. This is even more a problem in species-rich environments like tropical rainforests, where background noise levels of up to 60 dB SPL have been measured. Principal Findings Using neurophysiological methods we investigated the effect of natural background noise (masker) on signal detection thresholds in two tropical cricket species Paroecanthus podagrosus and Diatrypa sp., both in the laboratory and outdoors. We identified three ‘bottom-up’ mechanisms which contribute to an excellent neuronal representation of conspecific signals despite the masking background. First, the sharply tuned frequency selectivity of the receiver reduces the amount of masking energy around the species-specific calling song frequency. Laboratory experiments yielded an average signal-to-noise ratio (SNR) of −8 dB, when masker and signal were broadcast from the same side. Secondly, displacing the masker by 180° from the signal improved SNRs by further 6 to 9 dB, a phenomenon known as spatial release from masking. Surprisingly, experiments carried out directly in the nocturnal rainforest yielded SNRs of about −23 dB compared with those in the laboratory with the same masker, where SNRs reached only −14.5 and −16 dB in both species. Finally, a neuronal gain control mechanism enhances the contrast between the responses to signals and the masker, by inhibition of neuronal activity in interstimulus intervals. Conclusions Thus, conventional speaker playbacks in the lab apparently do not properly reconstruct the masking noise situation in a spatially realistic manner, since under real world conditions multiple sound sources are spatially distributed in space. Our results also indicate that without knowledge of the receiver properties and the spatial release mechanisms the detrimental effect of noise may be strongly overestimated.

120.
TITLE: Dip Listening and the Cocktail Party Problem in Grey Treefrogs: Signal Recognition in Temporally Fluctuating Noise
AUTHORS: A. Velez, M. Bee
YEAR: 2011
SOURCE: Animal Behaviour
ABSTRACT: Dip listening refers to our ability to catch brief "acoustic glimpses" of speech and other sounds when fluctuating background noise levels momentarily decrease. Exploiting dips in natural fluctuations of noise contributes to our ability to overcome the "cocktail party problem" of understanding speech in multi-talker social environments. We presently know little about how nonhuman animals solve analogous communication problems. Here, we asked whether female grey treefrogs (Hyla chrysoscelis) might benefit from dip listening in selecting a mate in the noisy social setting of a breeding chorus. Consistent with a dip listening hypothesis, subjects recognized conspecific calls at lower thresholds when the dips in a chorus-like noise masker were long enough to allow glimpses of nine or more consecutive pulses. No benefits of dip listening were observed when dips were shorter and included five or fewer pulses. Recognition thresholds were higher when the noise fluctuated at a rate similar to the pulse rate of the call. In a second experiment, advertisement calls comprising six to nine pulses were necessary to elicit responses under quiet conditions. Together, these results suggest that in frogs, the benefits of dip listening are constrained by neural mechanisms underlying temporal pattern recognition. These constraints have important implications for the evolution of male signalling strategies in noisy social environments.

121.
TITLE: Sound Texture Perception via Statistics of the Auditory Periphery: Evidence from Sound Synthesis
AUTHORS: J. McDermott, E.P. Simoncelli
YEAR: 2011
SOURCE: Neuron
ABSTRACT: Rainstorms, insect swarms, and galloping horses produce "sound textures"--the collective result of many similar acoustic events. Sound textures are distinguished by temporal homogeneity, suggesting they could be recognized with time-averaged statistics. To test this hypothesis, we processed real-world textures with an auditory model containing filters tuned for sound frequencies and their modulations, and measured statistics of the resulting decomposition. We then assessed the realism and recognizability of novel sounds synthesized to have matching statistics. Statistics of individual frequency channels, capturing spectral power and sparsity, generally failed to produce compelling synthetic textures; however, combining them with correlations between channels produced identifiable and natural-sounding textures. Synthesis quality declined if statistics were computed from biologically implausible auditory models. The results suggest that sound texture perception is mediated by relatively simple statistics of early auditory representations, presumably computed by downstream neural populations. The synthesis methodology offers a powerful tool for their further investigation.

122.
TITLE: Costly Help of Audiovisual Bimodality for Female Mate Choice in a Nocturnal Anuran (Hyla Arborea)
AUTHORS: D. Gomez, M. Thery, A.L. Gauthier, T. Lengagne
YEAR: 2011
SOURCE: Behavioral Ecology
ABSTRACT: In nocturnal anurans, females often have to choose a mate in a sensory-challenging situation--noisy background and high density of potential mates. Multimodality can help female choice by improving mate choice accuracy or reducing time to choose. Here, we conducted 2 choice experiments in the European tree frog (Hyla arborea) to test this question in 2 sensory conditions of large or reduced difference between potential targets, the latter being a discrimination challenge for females. We examined female choice (target, time to choose) in 3 conditions: unimodal conditions with varying acoustic signals, multimodal conditions with nondiscriminating acoustic signals and varying visual signals, and multimodal conditions with varying both acoustic and visual signals. We find that females are less accurate and take more time to choose when targets are more similar. The use of 2 varying sensory modalities reduces latency to choose when targets largely differ. It improves mate choice accuracy when targets are more similar. This improvement is associated to a longer time to choose, suggesting a speed--accuracy trade-off, which is shown for the first time in a mate choice context. We also find that the presence of nondiscriminating acoustic cues is not helpful for mate choice. Finally, we show that females choose the high-quality stimulus more quickly and suggest that this temporal gain may be reinforced when targets are more similar. We discuss our results in relation to female sampling tactics and to the benefits and costs of multimodality.

123.
TITLE: Finding Your Mate at a Cocktail Party: Frequency Separation Promotes Auditory Stream Segregation of Concurrent Voices in Multi-Species Frog Choruses
AUTHORS: V. Nityananda, M. Bee
YEAR: 2011
SOURCE: PLoS ONE
ABSTRACT: Vocal communication in crowded social environments is a difficult problem for both humans and nonhuman animals. Yet many important social behaviors require listeners to detect, recognize, and discriminate among signals in a complex acoustic milieu comprising the overlapping signals of multiple individuals, often of multiple species. Humans exploit a relatively small number of acoustic cues to segregate overlapping voices (as well as other mixtures of concurrent sounds, like polyphonic music). By comparison, we know little about how nonhuman animals are adapted to solve similar communication problems. One important cue enabling source segregation in human speech communication is that of frequency separation between concurrent voices: differences in frequency promote perceptual segregation of overlapping voices into separate “auditory streams” that can be followed through time. In this study, we show that frequency separation (deltaf) also enables frogs to segregate concurrent vocalizations, such as those routinely encountered in mixed-species breeding choruses. We presented female gray treefrogs (Hyla chrysoscelis) with a pulsed target signal (simulating an attractive conspecific call) in the presence of a continuous stream of distractor pulses (simulating an overlapping, unattractive heterospecific call). When the ΔF between target and distractor was small (e.g., ≤3 semitones), females exhibited low levels of responsiveness, indicating a failure to recognize the target as an attractive signal when the distractor had a similar frequency. Subjects became increasingly more responsive to the target, as indicated by shorter latencies for phonotaxis, as the deltaf between target and distractor increased (e.g., deltaf = 6–12 semitones). These results support the conclusion that gray treefrogs, like humans, can exploit frequency separation as a perceptual cue to segregate concurrent voices in noisy social environments. The ability of these frogs to segregate concurrent voices based on frequency separation may involve ancient hearing mechanisms for source segregation shared with humans and other vertebrates.

124.
TITLE: Multimodal Signal Variation in Space and Time: How Important Is Matching a Signal with Its Signaler?
AUTHORS: R.C. Taylor, B.A. Klein, J. Stein, M. Ryan
YEAR: 2011
SOURCE: Journal of Experimental Biology
ABSTRACT: Multimodal signals (acoustic+visual) are known to be used by many anuran amphibians during courtship displays. The relative degree to which each signal component influences female mate choice, however, remains poorly understood. In this study we used a robotic frog with an inflating vocal sac and acoustic playbacks to document responses of female túngara frogs to unimodal signal components (acoustic and visual). We then tested female responses to a synchronous multimodal signal. Finally, we tested the influence of spatial and temporal variation between signal components for female attraction. Females failed to approach the isolated visual cue of the robotic frog and they showed a significant preference for the call over the spatially separate robotic frog. When presented with a call that was temporally synchronous with the vocal sac inflation of the robotic frog, females did not show a significant preference for this over the call alone; when presented with a call that was temporally asynchronous with vocal sac inflation of the robotic frog, females discriminated strongly against the asynchronous multimodal signal in favor of the call alone. Our data suggest that although the visual cue is neither necessary nor sufficient for attraction, it can strongly modulate mate choice if females perceive a temporal disjunction relative to the primary acoustic signal.

125.
TITLE: Neural Correlates of Auditory Perceptual Organization Measured with Direct Cortical Recordings in Humans
AUTHORS: A. Dykstra
YEAR: 2011
SOURCE: Massachusetts Institute of Technology
ABSTRACT: One of the primary functions of the human auditory system is to separate the complex mixture of sound arriving at the ears into neural representations of individual sound sources. This function is thought to be crucial for survival and communication in noisy settings, and allows listeners to selectively and dynamically attend to a sound source of interest while suppressing irrelevant information. How the brain works to perceptually organize the acoustic environment remains unclear despite the multitude of recent studies utilizing microelectrode recordings in experimental animals or non-invasive human neuroimaging. In particular, the role that brain areas outside the auditory cortex might play is, comparatively, vastly understudied. The experiments described in this thesis combined classic behavioral paradigms with electrical recordings made directly from the cortical surface of neurosurgical patients undergoing clinically-indicated invasive monitoring for localization of epileptogenic foci. By sampling from widespread brain areas with high temporal resolution while participants simultaneously engaged in streaming and jittered multi-tone masking paradigms, the present experiments sought to overcome limitations inherent in previous work, namely sampling extent, resolution in time and space, and direct knowledge of the perceptual experience of the listener. In experiment 1, participants listened to sequences of tones alternating in frequency (i.e., ABA-) and indicated whether they perceived the tones as grouped ("1 stream") or segregated ("2 streams"). As has been reported in neurologically-normal listeners since the 1950s, patients heard the sequences as grouped when the frequency separation between the A and B tones was small and segregated when it was large. Evoked potentials from widespread brain areas showed amplitude correlations with frequency separation but surprisingly did not differ based solely on perceptual organization in the absence of changes in the stimuli. In experiment 2, participants listened to sequences of jittered multi-tone masking stimuli on which a regularly-repeating target stream of tones was sometimes superimposed and indicated when they heard the target stream. Target detectability, as indexed behaviorally, increased throughout the course of each sequence. Evoked potentials and high-gamma activity differed strongly based on the listener's subjective perception of the target tones. These results extend and constrain theories of how the brain subserves auditory perceptual organization and suggests several new avenues of research for understanding the neural mechanisms underlying this critical function.

126.
TITLE: Recovering Sound Sources from Embedded Repetition
AUTHORS: J. McDermott, D.W. Wrobleski, A.J. Oxenham
YEAR: 2011
SOURCE: Proceedings of the National Academy of Sciences
ABSTRACT: Cocktail parties and other natural auditory environments present organisms with mixtures of sounds. Segregating individual sound sources is thought to require prior knowledge of source properties, yet these presumably cannot be learned unless the sources are segregated first. Here we show that the auditory system can bootstrap its way around this problem by identifying sound sources as repeating patterns embedded in the acoustic input. Due to the presence of competing sounds, source repetition is not explicit in the input to the ear, but it produces temporal regularities that listeners detect and use for segregation. We used a simple generative model to synthesize novel sounds with naturalistic properties. We found that such sounds could be segregated and identified if they occurred more than once across different mixtures, even when the same sounds were impossible to segregate in single mixtures. Sensitivity to the repetition of sound sources can permit their recovery in the absence of other segregation cues or prior knowledge of sounds, and could help solve the cocktail party problem.

127.
TITLE: Relative Comparisons of Call Parameters Enable Auditory Grouping in Frogs
AUTHORS: H. Farris, M. Ryan
YEAR: 2011
SOURCE: Nature Communications
ABSTRACT: Whereas many studies on mate choice have measured the relative attractiveness of acoustic sexual signals, there is little understanding of another critical process: grouping and assigning the signals to their sources. For female túngara frogs, assigning the distinct components of male calls to the correct source is a challenge because males sing in aggregations, producing overlapping calls that lead to perceptual errors analogous to those of the 'cocktail party problem'. Here we show that for presentation of >2 call components, however, subjects are more likely to group the two components with the smallest relative differences in call parameters, including relative spatial separation (a primitive acoustic cue) and relative similarity to the species-specific call sequence (a schema-based cue). Thus, like humans, the cognitive rules for the perception of auditory groups amidst multiple sound sources include the use of relative comparisons, a flexible strategy for dynamic acoustic environments.

128.
TITLE: Temporal Coherence and Attention in Auditory Scene Analysis
AUTHORS: S. Shamma, M. Elhilali, C. Micheyl
YEAR: 2011
SOURCE: Trends in Neuroscience
ABSTRACT: Humans and other animals can attend to one of multiple sounds and follow it selectively over time. The neural underpinnings of this perceptual feat remain mysterious. Some studies have concluded that sounds are heard as separate streams when they activate well-separated populations of central auditory neurons, and that this process is largely pre-attentive. Here, we argue instead that stream formation depends primarily on temporal coherence between responses that encode various features of a sound source. Furthermore, we postulate that only when attention is directed towards a particular feature (e.g. pitch) do all other temporally coherent features of that source (e.g. timbre and location) become bound together as a stream that is segregated from the incoherent features of other sources.

129.
TITLE: The Focus of Attention at the Virtual Cocktail Party—Electrophysiological Evidence
AUTHORS: J. Lambrecht, D.K. Spring, T. Munte
YEAR: 2011
SOURCE: Neuroscience Letters
ABSTRACT: The width of the attentional focus during the selection of one of two concurrent normal human participants was investigated using event-related potentials. Two stories were presented from virtual locations located 15° to the left and right azimuth by convolving the speech message by the appropriate head-related transfer function determined for each individual participant. Task irrelevant probe stimuli (phoneme/da/uttered by the same speaker as the story) were presented in rapid sequence from the same virtual locations. Occasionally, probes were presented at locations 15 or 30° lateral of the standard probes. Probes coinciding with the attended message gave rise to a fronto-central negativity relative to the phoneme probes coinciding with the unattended speech message. This was similar to the typical ERP attention effect. On the attended side probes deviating from the standard location by 30° elicited a different type of negative response, tentatively identified as a reorienting negativity, whereas probes deviating by 15° did not. These results are taken to suggest that spatial information is used for message selection in a cocktail-party situation but that the focus of spatial attention is relatively wide.

130.
TITLE: Signal Recognition by Frogs in the Presence of Temporally Fluctuating Chorus-Shaped Noise
AUTHORS: A. Velez, M. Bee
YEAR: 2010
SOURCE: Behavioral Ecology and Sociobiology
ABSTRACT: The background noise generated in large social aggregations of calling individuals is a potent source of auditory masking for animals that communicate acoustically. Despite similarities with the so-called cocktail party problem in humans, few studies have explicitly investigated how nonhuman animals solve the perceptual task of separating biologically relevant acoustic signals from ambient background noise. Under certain conditions, humans experience a release from auditory masking when speech is presented in speech-like masking noise that fluctuates in amplitude. We tested the hypothesis that females of Cope’s gray treefrog (Hyla chrysoscelis) experience masking release in artificial chorus noise that fluctuates in level at modulations rates characteristic of those present in ambient chorus noise. We estimated thresholds for recognizing conspecific advertisement calls (pulse rate = 40–50 pulses/s) in the presence of unmodulated and sinusoidally amplitude-modulated (SAM) chorus-shaped masking noise. We tested two rates of modulation (5 and 45 Hz) because the sounds of frog choruses are modulated at low rates (e.g., less than 5–10 Hz) and because those of species with pulsatile signals are additionally modulated at higher rates typical of the pulse rate of calls (e.g., between 15 and 50 Hz). Recognition thresholds were similar in the unmodulated and 5-Hz SAM conditions and 12 dB higher in the 45-Hz SAM condition. These results did not support the hypothesis that female gray treefrogs experience masking release in temporally fluctuating chorus-shaped noise. We discuss our results in terms of modulation masking and hypothesize that natural amplitude fluctuations in ambient chorus noise may impair mating call perception.

131.
TITLE: Auditory Stream Segregation and the Perception of Across-Frequency Synchrony
AUTHORS: C. Micheyl, C.L. Hunter, A.J. Oxenham
YEAR: 2010
SOURCE: Journal of Experimental Psychology. Human Perception and Performance
ABSTRACT: This study explored the extent to which sequential auditory grouping affects the perception of temporal synchrony. In Experiment 1, listeners discriminated between 2 pairs of asynchronous "target" tones at different frequencies, A and B, in which the B tone either led or lagged. Thresholds were markedly higher when the target tones were temporally surrounded by "captor tones" at the A frequency than when the captor tones were absent or at a remote frequency. Experiment 2 extended these findings to asynchrony detection, revealing that the perception of synchrony, one of the most potent cues for simultaneous auditory grouping, is not immune to competing effects of sequential grouping. Experiment 3 examined the influence of ear separation on the interactions between sequential and simultaneous grouping cues. The results showed that, although ear separation could facilitate perceptual segregation and impair asynchrony detection, it did not prevent the perceptual integration of simultaneous sounds.

132.
TITLE: Behavioral Measures of Auditory Streaming in Ferrets (Mustela Putorius)
AUTHORS: L. Ma, C. Micheyl, P. Yin, A.J. Oxenham, S.A. Shamma
YEAR: 2010
SOURCE: Journal of Comparative Psychology
ABSTRACT: An important aspect of the analysis of auditory "scenes" relates to the perceptual organization of sound sequences into auditory "streams." In this study, we adapted two auditory perception tasks, used in recent human psychophysical studies, to obtain behavioral measures of auditory streaming in ferrets (Mustela putorius). One task involved the detection of shifts in the frequency of tones within an alternating tone sequence. The other task involved the detection of a stream of regularly repeating target tones embedded within a randomly varying multitone background. In both tasks, performance was measured as a function of various stimulus parameters, which previous psychophysical studies in humans have shown to influence auditory streaming. Ferret performance in the two tasks was found to vary as a function of these parameters in a way that is qualitatively consistent with the human data. These results suggest that auditory streaming occurs in ferrets, and that the two tasks described here may provide a valuable tool in future behavioral and neurophysiological studies of the phenomenon.

133.
TITLE: Behind the Scenes of Auditory Perception
AUTHORS: S.A. Shamma, C. Micheyl
YEAR: 2010
SOURCE: Current Opinion in Neurobiology
ABSTRACT: 'Auditory scenes' often contain contributions from multiple acoustic sources. These are usually heard as separate auditory 'streams', which can be selectively followed over time. How and where these auditory streams are formed in the auditory system is one of the most fascinating questions facing auditory scientists today. Findings published within the past two years indicate that both cortical and subcortical processes contribute to the formation of auditory streams, and they raise important questions concerning the roles of primary and secondary areas of auditory cortex in this phenomenon. In addition, these findings underline the importance of taking into account the relative timing of neural responses, and the influence of selective attention, in the search for neural correlates of the perception of auditory streams.

134.
TITLE: Neural Adaptation to Tone Sequences in the Songbird Forebrain: Patterns, Determinants, and Relation to the Build-Up of Auditory Streaming
AUTHORS: M. Bee, C. Micheyl, A.J. Oxenham, G. Klump
YEAR: 2010
SOURCE: Journal of Comparative Physiology A
ABSTRACT: Neural responses to tones in the mammalian primary auditory cortex (A1) exhibit adaptation over the course of several seconds. Important questions remain about the taxonomic distribution of multi-second adaptation and its possible roles in hearing. It has been hypothesized that neural adaptation could explain the gradual “build-up” of auditory stream segregation. We investigated the influence of several stimulus-related factors on neural adaptation in the avian homologue of mammalian A1 (field L2) in starlings (Sturnus vulgaris). We presented awake birds with sequences of repeated triplets of two interleaved tones (ABA–ABA–…) in which we varied the frequency separation between the A and B tones (deltaf), the stimulus onset asynchrony (time from tone onset to onset within a triplet), and tone duration. We found that stimulus onset asynchrony generally had larger effects on adaptation compared with deltaf and tone duration over the parameter range tested. Using a simple model, we show how time-dependent changes in neural responses can be transformed into neurometric functions that make testable predictions about the dependence of the build-up of stream segregation on various spectral and temporal stimulus properties.

135.
TITLE: Objective and Subjective Psychophysical Measures of Auditory Stream Integration and Segregation
AUTHORS: C. Micheyl, A.J. Oxenham
YEAR: 2010
SOURCE: Journal of the Association for Research in Otolaryngology
ABSTRACT: The perceptual organization of sound sequences into auditory streams involves the integration of sounds into one stream and the segregation of sounds into separate streams. “Objective” psychophysical measures of auditory streaming can be obtained using behavioral tasks where performance is facilitated by segregation and hampered by integration, or vice versa. Traditionally, these two types of tasks have been tested in separate studies involving different listeners, procedures, and stimuli. Here, we tested subjects in two complementary temporal-gap discrimination tasks involving similar stimuli and procedures. One task was designed so that performance in it would be facilitated by perceptual integration; the other, so that performance would be facilitated by perceptual segregation. Thresholds were measured in both tasks under a wide range of conditions produced by varying three stimulus parameters known to influence stream formation: frequency separation, tone-presentation rate, and sequence length. In addition to these performance-based measures, subjective judgments of perceived segregation were collected in the same listeners under corresponding stimulus conditions. The patterns of results obtained in the two temporal-discrimination tasks, and the relationships between thresholds and perceived-segregation judgments, were mostly consistent with the hypothesis that stream segregation helped performance in one task and impaired performance in the other task. The tasks and stimuli described here may prove useful in future behavioral or neurophysiological experiments, which seek to manipulate and measure neural correlates of auditory streaming while minimizing differences between the physical stimuli.

136.
TITLE: Spectral Preferences and the Role of Spatial Coherence in Simultaneous Integration in Gray Treefrogs (Hyla Chrysoscelis).
AUTHORS: M. Bee
YEAR: 2010
SOURCE: Journal of Comparative Psychology
ABSTRACT: The perceptual analysis of acoustic scenes may often require the integration of simultaneous sounds arising from a single source. Few studies have investigated the cues that promote simultaneous integration in the context of acoustic communication in nonhuman animals. This study of Cope's gray treefrog (Hyla chrysoscelis) examined female preferences based on spectral features of conspecific male advertisement calls to test the hypothesis that cues related to common spatial origin promote the perceptual integration of simultaneous signal elements (harmonics). The typical advertisement call comprises two harmonically related spectral peaks near 1.1 kHz and 2.2 kHz. Females generally exhibited preferences for calls with two spatially coherent harmonics over alternatives with just one harmonic. When given a choice between a spatially coherent call (both harmonics originating from the same speaker) and a spatially incoherent call (each harmonic from different spatially separated speakers), females preferentially chose the former in the same relative proportions in which it was chosen over single-harmonic alternatives. Preferences for spatially coherent calls over spatially incoherent alternatives did not appear to result from greater difficulty localizing the spatially incoherent sources. These results are consistent with the hypothesis that spatial coherence promotes perceptual integration of simultaneous signal elements in frogs.

137.
TITLE:Acoustics of Anthropogenic Habitats: The Impact of Noise Pollution on Eastern Bluebirds
AUTHORS: C.R. Kight
YEAR: 2009
SOURCE: William and Mary
ABSTRACT: An increasing number of habitats are affected by anthropogenic noise pollution, which is often louder, has a different frequency emphasis, and may occur over a different temporal scale, than natural noise. An increasing number of studies indicate that acoustically-communicating animals in such areas can modify their vocalizations in order to make themselves heard over the noise, but many questions still remain, including: How taxonomically widespread is vocal flexibility in response to anthropogenic noise, and do all vocally flexible species employ the same mechanisms to escape acoustic masking? Are there fitness repercussions for living, communicating, and breeding in noisy habitats? And, can particular habitat features be used to predict environmental noise levels and sound propagation characteristics? Here, I present data collected from the breeding territories of eastern bluebirds (Sialia sialis) to address each of these questions. My results add another species to the list of those who are able to avoid acoustic masking by modifying temporal and spectral traits of vocalizations. I also show that anthropogenic noise is associated with changes in several eastern bluebird breeding parameters. Finally, I demonstrate that both anthropogenic noise levels and sound propagation traits can be predicted by particular habitat characteristics.

138.
TITLE: Blind Location and Separation of Callers in a Natural Chorus Using a Microphone Array.
AUTHORS: D.L. Jones, R. Ratnam
YEAR: 2009
SOURCE: The Journal of the Acoustical Society of America
ABSTRACT: Male frogs and toads call in dense choruses to attract females. Determining the vocal interactions and spatial distribution of the callers is important for understanding acoustic communication in such assemblies. It has so far proved difficult to simultaneously locate and recover the vocalizations of individual callers. Here a microphone-array technique is developed for blindly locating callers using arrival-time delays at the microphones, estimating their steering-vectors, and recovering the calls with a frequency-domain adaptive beamformer. The technique exploits the time-frequency sparseness of the signal space to recover sources even when there are more sources than sensors. The method is tested with data collected from a natural chorus of Gulf Coast toads (Bufo valliceps) and Northern cricket frogs (Acris crepitans). A spatial map of locations accurate to within a few centimeters is constructed, and the individual call waveforms are recovered for nine individual animals within a 9 x 9 m(2). These methods work well in low reverberation when there are no reflectors other than the ground. They will require modifications to incorporate multi-path propagation, particularly for the estimation of time-delays.

139.
TITLE: Multiple Signals and Male Spacing Affect Female Preference at Cocktail Parties in Treefrogs
AUTHORS: C.H. Richardson, T. Lengagne
YEAR: 2009
SOURCE: Proceedings of the Royal Society B: Biological Sciences
ABSTRACT: Effective acoustic communication in the face of intense conspecific background noise constitutes a constant sensory challenge in chorusing and colonial species. An evolutionary approach suggests that behavioural and environmental constraints in these species should have shaped signal design and signalling behaviour to enable communication in noisy conditions. This could be attained both through the use of multicomponent signals and through short-term adjustments in the spatial separation of calling males. We investigated these two hypotheses in a chorusing anuran, the hylid Hyla arborea, through a series of phonotaxis experiments conducted within a six-speaker arena in a high background noise situation, by presenting females with male calls containing either single or multiple attractive call components, and by modifying distances between speakers. We found that female ability to discriminate attractive calls increased when several attractive call components were available, providing novel evidence that the use of multicomponent signals enhances communication in complex acoustic conditions. Signal discrimination in females also improved with speaker separation, demonstrating that within natural choruses, spatial unmasking conditioned by male density and spatial separation probably improves female discrimination of competing males. Implications of these results for the accuracy of mate choice within choruses are discussed.

140.
TITLE: Noise-Dependent Vocal Plasticity in Domestic Fowl
AUTHORS: H. Brumm, R. Schmidt, L. Schrader
YEAR: 2009
SOURCE: Animal Behaviour
ABSTRACT: Since acoustic communication is considerably constrained by environmental noise, some animals have evolved adaptations to counteract its masking effects. Humans and New World monkeys increase the duration of brief vocalizations (below a few hundred milliseconds) as the background noise level rises, a behaviour that increases the detection probability of signals in noise by temporal summation. We found that domestic fowl, Gallus gallus domesticus, exhibited the Lombard effect, that is, a regulation of vocal amplitude depending on the background noise level. This vocal mechanism for communication in noise is also found in mammals and other bird species. However, in contrast to primates, the chickens did not regulate the duration of their brief call syllables. This evidence for a lack of regulation of syllable duration may hint at limitations in the degrees of freedom for signal coding. Overall, our findings indicate that the common problem of acoustic communication in noise has led to the evolution of a common solution, the Lombard effect, but also to special adaptations in different taxa.

141.
TITLE: Segregation of Task-Relevant Conditioned Stimuli from Background Stimuli by Associative Learning
AUTHORS: T. Rothe, M. Delano, H. Scheich, H. Stark
YEAR: 2009
SOURCE: Brain Research
ABSTRACT: In the real world, task-relevant, conditioned stimuli are often embedded in a varying background, from which they have to be segregated. Besides sensory mechanisms, associative learning assumingly plays an important role for the segregation of the conditioned from the background stimuli, especially if conditioned and background stimuli are spectro-temporally structured, and psychophysically similar. We therefore investigated the influence of spectro-temporally structured background tones on associative learning of conditioned tones depending on the complexity of the behavioral task and the psychophysical similarity between conditioned and background tones. Frequency-modulated tone sweeps were used as conditioned stimuli, and persisting frequency-modulated tones as background. In a shuttle-box, Mongolian gerbils were subjected to a simple detection task, or to a more complex discrimination task. In contrast to detection learning, introduction or change of background tones affected discrimination performance both during learning and at the stage of retrieval, especially when conditioned and background tones were spectro-temporally similar. The change from a familiar to a new background tone at the stage of retrieval caused a prefrontal dopamine increase and lead to relearning of task-relevant associations. We conclude that conditioned stimuli and background stimuli are processed concomitantly, which might provide contextual information, but requires additional cognitive processing.

142.
TITLE: Temporal Coherence in the Perceptual Organization and Cortical Representation of Auditory Scenes
AUTHORS: M. Elhilali, L. Ma, C. Micheyl, A.J. Oxenham, S. Shamma
YEAR: 2009
SOURCE: Neuron
ABSTRACT: Just as the visual system parses complex scenes into identifiable objects, the auditory system must organize sound elements scattered in frequency and time into coherent "streams." Current neurocomputational theories of auditory streaming rely on tonotopic organization of the auditory system to explain the observation that sequential spectrally distant sound elements tend to form separate perceptual streams. Here, we show that spectral components that are well separated in frequency are no longer heard as separate streams if presented synchronously rather than consecutively. In contrast, responses from neurons in primary auditory cortex of ferrets show that both synchronous and asynchronous tone sequences produce comparably segregated responses along the tonotopic axis. The results argue against tonotopic separation per se as a neural correlate of stream segregation. Instead we propose a computational model of stream segregation that can account for the data by using temporal coherence as the primary criterion for predicting stream formation.

143
TITLE: The Cocktail Party Problem
AUTHORS: J. McDermott
YEAR: 2009
SOURCE: Current Biology
ABSTRACT: Natural auditory environments, be they cocktail parties or rain forests, contain many things that concurrently make sounds. The cocktail party problem is the task of hearing a sound of interest, often a speech signal, in this sort of complex auditory setting (Figure 1). The problem is intrinsically quite difficult, and there has been longstanding interest in how humans manage to solve it.

144.
TITLE: Does Common Spatial Origin Promote the Auditory Grouping of Temporally Separated Signal Elements in Grey Treefrogs?
AUTHORS: M. Bee, K.K. Riemersma
YEAR: 2008
SOURCE: Animal Behaviour
ABSTRACT: 'Sequential integration' represents a form of auditory grouping in which temporally separated sounds produced by the same source are perceptually bound together over time into a coherent 'auditory stream'. In humans, sequential integration plays important roles in music and speech perception. In this study of the grey treefrog (Hyla chrysoscelis), we took advantage of female selectivity for advertisement calls with conspecific pulse rates to investigate common spatial location as a cue for sequential integration. We presented females with two temporally interleaved pulse sequences with pulse rates of 25 pulses/s, which is half the conspecific pulse rate and more similar to that of H. versicolor, a syntopically breeding heterospecific. We tested the hypothesis that common spatial origin between the two pulse sequences would promote their integration into a coherent auditory stream with an attractive conspecific pulse rate. As the spatial separation between the speakers broadcasting the interleaved pulse sequences decreased from 180° to 0°, more females responded and females exhibited shorter response latencies and travelled shorter distances en route to a speaker. However, even in the 180° condition, most females (74%) still responded. Detailed video analyses revealed no evidence to suggest that patterns of female phonotaxis resulted from impaired abilities to localize sound sources in the spatially separated conditions. Together, our results suggest that females were fairly permissive of spatial incoherence between the interleaved pulses sequences and that common spatial origin may be only a relatively weak cue for sequential integration in grey treefrogs.

145.
TITLE: Finding a Mate at a Cocktail Party: Spatial Release from Masking Improves Acoustic Mate Recognition in Grey Treefrogs
AUTHORS: M. Bee
YEAR: 2008
SOURCE: Animal Behaviour
ABSTRACT: The 'cocktail party problem' refers to the difficulty that humans have in recognizing speech in noisy social environments. Many non-human animals also communicate acoustically in noisy social aggregations, and thus also encounter - and solve - cocktail-party-like problems. Relatively few studies, however, have investigated the processes by which non-human animals solve sound source segregation problems in the behaviourally relevant context of acoustic communication. In humans, 'spatial release from masking' contributes to sound source segregation by improving the ability of listeners to recognize speech that is spatially separated from other sources of speech or 'speech-shaped' masking noise. Using a phonotaxis paradigm, I tested the hypothesis that spatial release from masking improves the ability of female grey treefrogs, Hyla chrysoscelis, to discriminate between conspecific and heterospecific calls that were spatially separated from two sources of 'chorus-shaped' masking noise by either 15° or 90°. As the signal-to-noise ratio (SNR) was decreased from +3 dB to -15 dB (by decreasing the signal level in 6-dB steps), fewer females made a choice and the likelihood of a female choosing the heterospecific call also increased. At a SNR of -3 dB, females oriented toward and chose the conspecific call in the 90° separation condition, but not when signals and maskers were separated by 15°. These results support the hypothesis that a well-known solution to the cocktail party problem in humans - spatial release from masking - also plays a role in acoustic signal recognition in animals that communicate in biological equivalents of cocktail-party-like environments.

146.
TITLE: Perceptual Bi-Stability in Auditory Streaming: How Much Do Stimulus Features Matter?
AUTHORS: I. Winkler
YEAR: 2008
SOURCE: Frontiers in Human Neuroscience
ABSTRACT: The auditory two-tone streaming paradigm has been used extensively to study the mechanisms that underlie the decomposition of the auditory input into coherent sound sequences. Using longer tone sequences than usual in the literature, we show that listeners hold their first percept of the sound sequence for a relatively long period, after which perception switches between two or more alternative sound organizations, each held on average for a much shorter duration. The first percept also differs from subsequent ones in that stimulus parameters influence its quality and duration to a far greater degree than the subsequent ones. We propose an account of auditory streaming in terms of rivalry between competing temporal associations based on two sets of processes. The formation of associations (discovery of alternative interpretations) mainly affects the first percept by determining which sound group is discovered first and how long it takes for alternative groups to be established. In contrast, subsequent percepts arise from stochastic switching between the alternatives, the dynamics of which are determined by competitive interactions between the set of coexisting interpretations.

147.
TITLE: Detecting Modulated Signals in Modulated Noise: Neural Thresholds in the Songbird Forebrain
AUTHORS: M. Bee, M. Buschermohle, G Klump.
YEAR: 2007
SOURCE: The European Journal of Neuroscience
ABSTRACT: Sounds in the real world fluctuate in amplitude. The vertebrate auditory system exploits patterns of amplitude fluctuations to improve signal detection in noise. One experimental paradigm demonstrating these general effects has been used in psychophysical studies of 'comodulation detection difference' (CDD). The CDD effect refers to the fact that thresholds for detecting a modulated, narrowband noise signal are lower when the envelopes of flanking bands of modulated noise are comodulated with each other, but fluctuate independently of the signal compared with conditions in which the envelopes of the signal and flanking bands are all comodulated. Here, we report results from a study of the neural correlates of CDD in European starlings (Sturnus vulgaris). We manipulated: (i) the envelope correlations between a narrowband noise signal and a masker comprised of six flanking bands of noise; (ii) the signal onset delay relative to masker onset; (iii) signal duration; and (iv) masker spectrum level. Masked detection thresholds were determined from neural responses using signal detection theory. Across conditions, the magnitude of neural CDD ranged between 2 and 8 dB, which is similar to that reported in a companion psychophysical study of starlings [U. Langemann & G.M. Klump (2007) Eur. J. Neurosci., 26, 1969-1978]. We found little evidence to suggest that neural CDD resulted from the across-channel processing of auditory grouping cues related to common envelope fluctuations and synchronous onsets between the signal and flanking bands. We discuss a within-channel model of peripheral processing that explains many of our results.

148.
TITLE: Sound Source Segregation in Grey Treefrogs: Spatial Release from Masking by the Sound of a Chorus
AUTHORS: M. Bee
YEAR: 2007
SOURCE: Animal Behaviour
ABSTRACT: Animals that communicate acoustically in noisy social environments face the problem of perceptually segregating behaviourally relevant signals from background noise. Studies of humans indicate improvements in speech perception tasks when target speech and a masking noise with the frequency spectrum of speech come from different locations. Thus for humans, spatial release from masking is an important mechanism of sound source segregation that functions in acoustic communication in noisy, real-world environments. Little previous work has investigated the mechanisms of sound source segregation in nonhuman animals that rely on acoustic signalling. I investigated spatial release from masking in the grey treefrog, Hyla chrysoscelis . Grey treefrogs form large breeding aggregations in which males produce loud advertisement calls that are necessary and sufficient for species recognition, source localization and selective mate choice by females. I tested the hypothesis that females would experience a release from masking when a synthetic advertisement call (target signal) and an artificial noise with the spectrum of a grey treefrog chorus (masker) were spatially separated. Using a phonotaxis paradigm, I assessed females' responses to the target signal at four signal-to-noise ratios (SNR) and two angular separations. Female responsiveness to the target signal increased as the SNR increased from −12 dB to +6 dB. More important, phonotaxis responses were faster at a 90° angle of signal–masker separation compared to one of just 7.5°. These results support the hypothesis that spatial release from masking is a potentially important mechanism for sound source segregation in animal acoustic communication.

149.
TITLE: Sound Texture Perception via Statistics of the Auditory Periphery: Evidence from Sound Synthesis
AUTHORS: J. McDermott, E.P. Simoncelli
YEAR: 2011
SOURCE: Neuron
ABSTRACT: Rainstorms, insect swarms, and galloping horses produce ‘‘sound textures’’—the collective result of many similar acoustic events. Sound textures are distinguished by temporal homogeneity, suggesting they could be recognized with time-averaged statistics. To test this hypothesis, we processed real-world textures with an auditory model containing filters tuned for sound frequencies and their modulations, and measured statistics of the resulting decomposition. We then assessed the realism and recognizability of novel sounds synthesized to have matching statistics. Statistics of individual frequency channels, capturing spectral power and sparsity, generally failed to produce compelling synthetic textures; however, combining them with correlations between channels produced identifiable and natural-sounding textures. Synthesis quality declined if statistics were computed from biologically implausible auditory models. The results suggest that sound texture perception is mediated by relatively simple statistics of early auditory representations, presumably computed by downstream neural populations. The synthesis methodology offers a powerful tool for their further investigation.

150.
TITLE: Wavesplit: End-to-End Speech Separation by Speaker Clustering
AUTHORS: N. Zeghidour, D. Grangier
YEAR: 2020
SOURCE: arXiv
ABSTRACT: We introduce Wavesplit, an end-to-end source separation system. From a single mixture, the model infers a representation for each source and then estimates each source signal given the inferred representations. The model is trained to jointly perform both tasks from the raw waveform. Wavesplit infers a set of source representations via clustering, which addresses the fundamental permutation problem of separation. For speech separation, our sequence-wide speaker representations provide a more robust separation of long, challenging recordings compared to prior work. Wavesplit redefines the state-of-the-art on clean mixtures of 2 or 3 speakers (WSJ0-2/3mix), as well as in noisy and reverberated settings (WHAM/WHAMR). We also set a new benchmark on the recent LibriMix dataset. Finally, we show that Wavesplit is also applicable to other domains, by separating fetal and maternal heart rates from a single abdominal electrocardiogram.

151.
TITLE: Dual-Path Transformer Network: Direct Context-Aware Modeling for End-to-End Monaural Speech Separation
AUTHORS: J. Chen, Q. Mao, D.Liu
YEAR: 2020
SOURCE: arXiv
ABSTRACT: The dominant speech separation models are based on complex recurrent or convolution neural network that model speech sequences indirectly conditioning on context, such as passing information through many intermediate states in recurrent neural network, leading to suboptimal separation performance. In this paper, we propose a dual-path transformer network (DPTNet) for end-to-end speech separation, which introduces direct context-awareness in the modeling for speech sequences. By introduces a improved transformer, elements in speech sequences can interact directly, which enables DPTNet can model for the speech sequences with direct context-awareness. The improved transformer in our approach learns the order information of the speech sequences without positional encodings by incorporating a recurrent neural network into the original transformer. In addition, the structure of dual paths makes our model efficient for extremely long speech sequence modeling. Extensive experiments on benchmark datasets show that our approach outperforms the current state-of-the-arts (20.6 dB SDR on the public WSj0-2mix data corpus).

152.
TITLE: Voice Separation with an Unknown Number of Multiple Speakers
AUTHORS: E. Nachmani, Y. Adi, L.Wolf
YEAR: 2020
SOURCE: arXiv
ABSTRACT: We present a new method for separating a mixed audio sequence, in which multiple voices speak simultaneously. The new method employs gated neural networks that are trained to separate the voices at multiple processing steps, while maintaining the speaker in each output channel fixed. A different model is trained for every number of possible speakers, and the model with the largest number of speakers is employed to select the actual number of speakers in a given sample. Our method greatly outperforms the current state of the art, which, as we show, is not competitive for more than two speakers.

153.
TITLE: Separation of Overlapping Sources in Bioacoustic Mixtures
AUTHORS: M.R. Izadi, R. Stevenson, L.N. Kloepper
YEAR: 2020
SOURCE: The Journal of the Acoustical Society of America
ABSTRACT: Source separation is an important step to study signals that are not easy or possible to record individually. Common methods such as deep clustering, however, cannot be applied to signals of an unknown number of sources and/or signals that overlap in time and/or frequency—a common problem in bioacoustic recordings. This work presents an approach, using a supervised learning framework, to parse individual sources from a spectrogram of a mixture that contains a variable number of overlapping sources. This method isolates individual sources in the time-frequency domain using only one function but in two separate steps, one for the detection of the number of sources and corresponding bounding boxes, and a second step for the segmentation in which masks of individual sounds are extracted. This approach handles the full separation of overlapping sources in both time and frequency using deep neural networks in an applicable manner to other tasks such as bird audio detection. This paper presents method and reports on its performance to parse individual bat signals from recordings containing hundreds of overlapping bat echolocation signals. This method can be extended to other bioacoustic recordings with a variable number of sources and signals that overlap in time and/or frequency.

154.
TITLE: Masked Conditional Neural Networks for Audio Classification
AUTHORS: F. Medhat, D. Chesmore, J. Robinson
YEAR: 2019
SOURCE: arXiv
ABSTRACT: We present the ConditionaL Neural Network (CLNN) and the Masked ConditionaL Neural Network (MCLNN) designed for temporal signal recognition. The CLNN takes into consideration the temporal nature of the sound signal and the MCLNN extends upon the CLNN through a binary mask to preserve the spatial locality of the features and allows an automated exploration of the features combination analogous to hand-crafting the most relevant features for the recognition task. MCLNN has achieved competitive recognition accuracies on the GTZAN and the ISMIR2004 music datasets that surpass several state-of-th-eart neural network based architectures and hand-crafted methods applied on both datasets.

155.
TITLE: Separating Overlapping Bat Calls with a Bi-Directional Long Short-Term Memory Network
AUTHORS: K. Zhang, et al.
YEAR: 2019
SOURCE: bioRxiv
ABSTRACT: Acquiring clear and usable audio recordings is critical for acoustic analysis of animal vocalizations. Bioacoustics studies commonly face the problem of overlapping signals, but the issue is often ignored, as there is currently no satisfactory solution. This study presents a bi-directional long short-term memory (BLSTM) network to separate overlapping bat calls and reconstruct waveform audio sounds. The separation quality was evaluated using seven temporal-spectrum parameters. The applicability of this method for bat calls was assessed using six different species. In addition, clustering analysis was conducted with separated echolocation calls from each population. Results showed that all syllables in the overlapping calls were separated with high robustness across species. A comparison between the seven temporal-spectrum parameters showed no significant difference and negligible deviation between the extracted and original calls, indicating high separation quality. Clustering analysis of the separated echolocation calls also produced an accuracy of 93.8%, suggesting the reconstructed waveform sounds could be reliably used. These results suggest the proposed technique is a convenient and automated approach for separating overlapping calls using a BLSTM network. This powerful deep neural network approach has the potential to solve complex problems in bioacoustics.

156.
TITLE: Deep Scattering Spectrum
AUTHORS: J. Anden
YEAR: 2013
SOURCE: arXiv
ABSTRACT: A scattering transform defines a locally translation invariant representation which is stable to time-warping deformations. It extends MFCC representations by computing modulation spectrum coefficients of multiple orders, through cascades of wavelet convolutions and modulus operators. Second-order scattering coefficients characterize transient phenomena such as attacks and amplitude modulation. A frequency transposition invariant representation is obtained by applying a scattering transform along log-frequency. State-the-of-art classification results are obtained for musical genre and phone classification on GTZAN and TIMIT databases, respectively.

157.
TITLE: Deep Audio Prior
AUTHORS: Y. Tian, C. Xu, D. Li
YEAR: 2019
SOURCE: arXiv
ABSTRACT: Deep convolutional neural networks are known to specialize in distilling compact and robust prior from a large amount of data. We are interested in applying deep networks in the absence of training dataset. In this paper, we introduce deep audio prior (DAP) which leverages the structure of a network and the temporal information in a single audio file. Specifically, we demonstrate that a randomly-initialized neural network can be used with carefully designed audio prior to tackle challenging audio problems such as universal blind source separation, interactive audio editing, audio texture synthesis, and audio co-separation. To understand the robustness of the deep audio prior, we construct a benchmark dataset \emph{Universal-150} for universal sound source separation with a diverse set of sources. We show superior audio results than previous work on both qualitative and quantitative evaluations. We also perform thorough ablation study to validate our design choices.

158.
TITLE: Source Separation in Underwater Acoustic Problems
AUTHORS: Z. Zhang
YEAR: 2016
SOURCE: University of Southampton
ABSTRACT: When conducting passive acoustic monitoring of humpback whale songs in St Marie channel, Madagascar, sometimes recordings containing multiple singers were obtained. In this case, separating the mixtures and obtaining a recording of an individual singer is of interest. The specific method that we utilized for source separation is adapted from the proposal by Sawada et al. This algorithm can effectively operate in conditions with strong reverberation. It can also potentially cope with underdetermined mixtures where source number exceeds hydrophone number. The effectiveness of the Sawada method is verified through separation of artificial humpback song mixtures generated by the impulse responses of underwater channel model. However, this method is unreliable for the separation of real humpback whale songs as a consequence of severe background noise. We propose a noise reduction method based on weighted median threshold scheme, which significantly improves source separation performance of real recording in severe noise environments. As to the Sawada algorithm, the number of sources need to be known in order to conduct source separation. However, in reality, the number of source is unknown, Hence, we need to estimate it before performing sources separation. Various methods for automatically estimating the number of sources are investigated in this thesis, and the units counting method turns out to be the most promising one.

159.
TITLE: Blind Nonnegative Source Separation Using Biological Neural Networks
AUTHORS: C. Pehlevan, S. Mohan, D.B. Chklovskii
YEAR: 2017
SOURCE: arXiv
ABSTRACT: Blind source separation, i.e. extraction of independent sources from a mixture, is an important problem for both artificial and natural signal processing. Here, we address a special case of this problem when sources (but not the mixing matrix) are known to be nonnegative, for example, due to the physical nature of the sources. We search for the solution to this problem that can be implemented using biologically plausible neural networks. Specifically, we consider the online setting where the dataset is streamed to a neural network. The novelty of our approach is that we formulate blind nonnegative source separation as a similarity matching problem and derive neural networks from the similarity matching objective. Importantly, synaptic weights in our networks are updated according to biologically plausible local learning rules.

160.
TITLE: The Separation of Overlapped Dolphin Signature Whistle Based on Blind Source Separation
AUTHORS: X. Deng, Y. Tao, X. Tu, X. Xu
YEAR: 2017
SOURCE: 2017 IEEE International Conference on Signal Processing, Communications and Computing
ABSTRACT: Signature whistle is individually distinctive vocalization that plays a vital role in the context of social interactions and group cohesion. However, sometimes the signals overlap recorded by hydrophones when several dolphins squawk simultaneously. This impedes the further research of signature whistle in vocal interactions. An efficient blind source separation technique that can identify and separate mixed signals is necessary. Here the joint approximate diagonalization of eigenmatrix (JADE) method is adopted to separate mixed signals in each frequency bin. To address the permutation problem induced by this algorithm, the spectral consistency of signature whistle is exploited. In simulation, we take the manually mixed dolphin whistles, which have been distorted by underwater acoustic channels, for separation. Results show that the original sources are effectively separated and the signal-to-noise ratio (SNR) is significantly improved.

161.
TITLE: Multi-Context Blind Source Separation by Error-Gated Hebbian Rule
AUTHORS: T. Isomura and T. Toyoizumi
YEAR: 2019
SOURCE: Scientific Reports
ABSTRACT: Animals need to adjust their inferences according to the context they are in. This is required for the multi-context blind source separation (BSS) task, where an agent needs to infer hidden sources from their context-dependent mixtures. The agent is expected to invert this mixing process for all contexts. Here, we show that a neural network that implements the error-gated Hebbian rule (EGHR) with sufciently redundant sensory inputs can successfully learn this task. After training, the network can perform the multi-context BSS without further updating synapses, by retaining memories of all experienced contexts. This demonstrates an attractive use of the EGHR for dimensionality reduction by extracting low-dimensional sources across contexts. Finally, if there is a common feature shared across contexts, the EGHR can extract it and generalize the task to even inexperienced contexts. The results highlight the utility of the EGHR as a model for perceptual adaptation in animals.

162.
TITLE: Music Source Separation in the Waveform Domain
AUTHORS: A. Defossez, N. Usunier, L. Bottou
YEAR: 2019
SOURCE: arXiv
ABSTRACT: Source separation for music is the task of isolating contributions, or stems, from different instruments recorded individually and arranged together to form a song.Such components include voice, bass, drums and any other accompaniments. While end-to-end models that directly generate the waveform are state-of-the-art in many audio synthesis problems, the best multi-instrument source separation models generate masks on the magnitude spectrum and achieve performances far above current end-to-end, waveform-to-waveform models. We present an in-depth analysis of a new architecture, which we will refer to as Demucs, based on a (transposed) convolutional autoencoder, with a bidirectional LSTM at the bottleneck layer and skip-connections as in U-Networks (Ronneberger et al., 2015). Compared to the state-of-the-art waveform-to-waveform model, Wave-U-Net (Stoller et al., 2018), the main features of our approach in addition of the bi-LSTM are the use of trans-posed convolution layers instead of upsampling-convolution blocks, the use of gated linear units, exponentially growing the number of channels with depth and a new careful initialization of the weights.  Results on the MusDB dataset show that our architecture achieves a signal-to-distortion ratio (SDR) nearly 2.2 points higher than the best waveform-to-waveform competitor (from 3.2 to 5.4 SDR). This makes our model match the state-of-the-art performances on this dataset, bridging the performance gap between models that operate on the spectrogram and end-to-end approaches.

163.
TITLE: A Comparative Study of Blind Source Separation for Bioacoustics Sounds based on FastICA, PCA and NMF
AUTHORS: N. Hassan, D.A. Ramli
YEAR: 2018
SOURCE: Procedia Computer Science
ABSTRACT: Blind Source Separation (BSS) is a task of separating a set of source signals from mixed signal without (or very little information) of both the sources and the mixing process. This paper addresses the problem of BSS in bio-acoustic mixed signals. In a noisy acoustic environment, animal species recognition based on vocalization remains a challenging task. In order to robustly recognize the specific species, the source signals of interest need to be separated from the mixed signals. This separation process is a significant pre-processing step before the recognition process takes place. In this paper, three different source separation methods namely Fast Fixed-Point Independent Component Analysis algorithms (FastICA), Principal Component Analysis (PCA) and Non-Negative Matrix Factorization (NMF) are implemented. In this experiment, the mixtures of frog sound signals are used as input. The quality of separated source signals using FastICA, PCA and NMF algorithms are compared and evaluated according to BSS_EVAL toolbox metrics. These metrics consist of signal to distortion ratio (SDR), signal to interference ratio (SIR) and signal to artifacts ratio (SAR). The results show that FastICA with negentropy technique for finding a maximum non-gaussianity has the best performances in separating mixed signals.

164.
TITLE: Investigating Deep Neural Transformations for Spectrogram-Based Musical Source Separation
AUTHORS: W. Choi, M. Kim, J. Chung, D. Lee, S. Jung
YEAR: 2019
SOURCE: arXiv
ABSTRACT: Musical Source Separation (MSS) is a signal processing task that tries to separate the mixed musical signal into each acoustic sound source, such as singing voice or drums. Recently many machine learning-based methods have been proposed for the MSS task, but there were no existing works that evaluate and directly compare various types of networks. In this paper, we aim to design a variety of neural transformation methods, including time-invariant methods, time-frequency methods, and mixtures of two different transformations. Our experiments provide abundant material for future works by comparing several transformation methods. We train our models on raw complex-valued STFT outputs and achieve state-of-the-art SDR performance on the MUSDB singing voice separation task by a large margin of 1.0 dB.

165.
TITLE: Ensemble of Convolutional Neural Networks to Improve Animal Audio Classification
AUTHORS: L. Nanni, et al.
YEAR: 2020
SOURCE: EURASIP Journal on Audio, Speech, and Music
ABSTRACT: In this work, we present an ensemble for automated audio classification that fuses different types of features extracted from audio files. These features are evaluated, compared, and fused with the goal of producing better classification accuracy than other state-of-the-art approaches without ad hoc parameter optimization. We present an ensemble of classifiers that performs competitively on different types of animal audio datasets using the same set of classifiers and parameter settings. To produce this general-purpose ensemble, we ran a large number of experiments that fine-tuned pretrained convolutional neural networks (CNNs) for different audio classification tasks (bird, bat, and whale audio datasets). Six different CNNs were tested, compared, and combined. Moreover, a further CNN, trained from scratch, was tested and combined with the fine-tuned CNNs. To the best of our knowledge, this is the largest study on CNNs in animal audio classification. Our results show that several CNNs can be fine-tuned and fused for robust and generalizable audio classification. Finally, the ensemble of CNNs is combined with handcrafted texture descriptors obtained from spectrograms for further improvement of performance. The MATLAB code used in our experiments will be provided to other researchers for future comparisons at https://github.com/LorisNanni.

166.
TITLE: Kymatio: Scattering Transforms in Python
AUTHORS: M. Andreux, et al.
YEAR: 2019
SOURCE: arXiv
ABSTRACT: The wavelet scattering transform is an invariant signal representation suitable for many signal processing and machine learning applications. We present the Kymatio software package, an easy-to-use, high-performance Python implementation of the scattering transform in 1D, 2D, and 3D that is compatible with modern deep learning frameworks. All transforms may be executed on a GPU (in addition to CPU), offering a considerable speed up over CPU implementations. The package also has a small memory footprint, resulting inefficient memory usage. The source code, documentation, and examples are available undera BSD license at this https URL

167.
TITLE: Audio Source Separation with Discriminative Scattering Networks
AUTHORS: P. Sprechmann, J. Bruna, Y. LeCun
YEAR: 2015
SOURCE: arXiv
ABSTRACT: In this report we describe an ongoing line of research for solving single-channel source separation problems. Many monaural signal decomposition techniques proposed in the literature operate on a feature space consisting of a time-frequency representation of the input data. A challenge faced by these approaches is to effectively exploit the temporal dependencies of the signals at scales larger than the duration of a time-frame. In this work we propose to tackle this problem by modeling the signals using a time-frequency representation with multiple temporal resolutions. The proposed representation consists of a pyramid of wavelet scattering operators, which generalizes Constant Q Transforms (CQT) with extra layers of convolution and complex modulus. We first show that learning standard models with this multi-resolution setting improves source separation results over fixed-resolution methods. As study case, we use Non-Negative Matrix Factorizations (NMF) that has been widely considered in many audio application. Then, we investigate the inclusion of the proposed multi-resolution setting into a discriminative training regime. We discuss several alternatives using different deep neural network architectures.

168.
TITLE: Audio Texture Synthesis with Scattering Moments
AUTHORS: J. Bruna, S. Mallat
YEAR: 2013 
SOURCE: arXiv
ABSTRACT: We introduce an audio texture synthesis algorithm based on scattering moments. A scattering transform is computed by iteratively decomposing a signal with complex wavelet filter banks and computing their amplitude envelop. Scattering moments provide general representations of stationary processes computed as expected values of scattering coefficients. They are estimated with low variance estimators from single realizations. Audio signals having prescribed scattering moments are synthesized with a gradient descent algorithms. Audio synthesis examples show that scattering representation provide good synthesis of audio textures with much fewer coefficients than the state of the art.

169.
TITLE: Invariant Scattering Convolution Networks
AUTHORS: J. Bruna, S. Mallat
YEAR: 2012
SOURCE: arXiv
ABSTRACT: A wavelet scattering network computes a translation invariant image representation, which is stable to deformations and preserves high frequency information for classification. It cascades wavelet transform convolutions with non-linear modulus and averaging operators. The first network layer outputs SIFT-type descriptors whereas the next layers provide complementary invariant information which improves classification. The mathematical analysis of wavelet scattering networks explains important properties of deep convolution networks for classification. A scattering representation of stationary processes incorporates higher order moments and can thus discriminate textures having the same Fourier power spectrum. State of the art classification results are obtained for handwritten digits and texture discrimination, using a Gaussian kernel SVM and a generative PCA classifier.

170.
TITLE: Online PLCA for Real-Time Semi-supervised Source Separation
AUTHORS: Z. Duan, G.J. Mysore, P. Smaragdis
YEAR: 2012
SOURCE: Lecture Notes in Computer Science
ABSTRACT: Non-negative spectrogram factorization algorithms such as probabilistic latent component analysis (PLCA) have been shown to be quite powerful for source separation. When training data for all of the sources are available, it is trivial to learn their dictionaries beforehand and perform supervised source separation in an online fashion. However, in many real-world scenarios (e.g. speech denoising), training data for one of the sources can be hard to obtain beforehand (e.g. speech). In these cases, we need to perform semi-supervised source separation and learn a dictionary for that source during the separation process. Existing semi-supervised separation approaches are generally offline, i.e. they need to access the entire mixture when updating the dictionary. In this paper, we propose an online approach to adaptively learn this dictionary and separate the mixture over time. This enables us to perform online semi-supervised separation for real-time applications. We demonstrate this approach on real-time speech denoising.

171.
TITLE: Majorization-Minimization Algorithm for Smooth Itakura-Saito Nonnegative Matrix Factorization
AUTHORS: C. Fevotte
YEAR: 2011
SOURCE: 2011 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: Nonnegative matrix factorization (NMF) with the Itakura-Saito divergence has proven efficient for audio source separation and music transcription, where the signal power spectrogram is factored into a "dictionary" matrix times an "activation" matrix. Given the nature of audio signals it is expected that the activation coefficients exhibit smoothness along time frames. This may be enforced by penalizing the NMF objective function with an extra term reflecting smoothness of the activation coefficients. We propose a novel regularization term that solves some deficiencies of our previous work and leads to an efficient implementation using a majorization-minimization procedure

172.
TITLE: Deep Learning for Monoaural Speech Separation
AUTHORS: P. Huang, M. Kim, M, Hasegawa-Johnson, P. Smaragdis
YEAR: 2014
SOURCE: 2014 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: Monaural source separation is useful for many real-world applications though it is a challenging problem. In this paper, we study deep learning for monaural speech separation. We propose the joint optimization of the deep learning models (deep neural networks and recurrent neural networks) with an extra masking layer, which enforces a reconstruction constraint. Moreover, we explore a discriminative training criterion for the neural networks to further enhance the separation performance. We evaluate our approaches using the TIMIT speech corpus for a monaural speech separation task. Our proposed models achieve about 3.8⇠4.9 dB SIR gain compared to NMF models, while maintaining better SDRs and SARs.

173.
TITLE: Singing-Voice Separation from Monaural Recordings using Deep Recurrent Neural Networks
AUTHORS: P. Huang, M. Kim, M, Hasegawa-Johnson, P. Smaragdis
YEAR: 2014
SOURCE: ISMIR 2014
ABSTRACT: Monaural source separation is important for many real world applications. It is challenging since only single channel information is available. In this paper, we explore using deep recurrent neural networks for singing voice separation from monaural recordings in a supervised setting. Deep recurrent neural networks with different temporal connections are explored. We propose jointly optimizing the networks for multiple source signals by including the separation step as a nonlinear operation in the last layer. Different discriminative training objectives are further explored to enhance the source to interference ratio. Our proposed system achieves the state-of-the-art performance, 2.30∼2.48 dB GNSDR gain and 4.32∼5.42 dB GSIR gain compared to previous models, on the MIR-1K dataset.

174.
TITLE: Learning Parts of Objects by Non-Negative Matrix Factorization
AUTHORS: D.D. Lee, H. Sebastian Seung
YEAR: 1999
SOURCE: Nature
ABSTRACT: Is perception of the whole based on perception of its parts? There is psychological1 and physiological2,3 evidence for parts-based representations in the brain, and certain computational theories of object recognition rely on such representations4,5. But little is known about how brains or computers might learn the parts of objects. Here we demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces and semantic features of text. This is in contrast to other methods, such as principal components analysis and vector quantization, that learn holistic, not parts-based, representations. Non-negative matrix factorization is distinguished from the other methods by its use of non-negativity constraints. These constraints lead to a parts-based representation because they allow only additive, not subtractive, combinations. When non-negative matrix factorization is implemented as a neural network, parts-based representations emerge by virtue of two properties: the firing rates of  neurons are never negative and synaptic strengths do not change sign.

175.
TITLE: Supervised and Unsupervised Speech Enhancement Using Nonnegative Matrix Factorization
AUTHORS: N. Mohammadiha, P. Smaragdis, A. Leijon
YEAR: 2017
SOURCE: arXiv
ABSTRACT: Reducing the interference noise in a monaural noisy speech signal has been a challenging task for many years. Compared to traditional unsupervised speech enhancement methods, e.g., Wiener filtering, supervised approaches, such as algorithms based on hidden Markov models (HMM), lead to higher-quality enhanced speech signals. However, the main practical difficulty of these approaches is that for each noise type a model is required to be trained a priori. In this paper, we investigate a new class of supervised speech denoising algorithms using nonnegative matrix factorization (NMF). We propose a novel speech enhancement method that is based on a Bayesian formulation of NMF (BNMF). To circumvent the mismatch problem between the training and testing stages, we propose two solutions. First, we use an HMM in combination with BNMF (BNMF-HMM) to derive a minimum mean square error (MMSE) estimator for the speech signal with no information about the underlying noise type. Second, we suggest a scheme to learn the required noise BNMF model online, which is then used to develop an unsupervised speech enhancement system. Extensive experiments are carried out to investigate the performance of the proposed methods under different conditions. Moreover, we compare the performance of the developed algorithms with state-of-the-art speech enhancement schemes using various objective measures. Our simulations show that the proposed BNMF-based methods outperform the competing algorithms substantially.

176.
TITLE: A Non-Negative Approach to Semi-Supervised Separation of Speech from Noise with the Use of Temporal Dynamics
AUTHORS: G.J. Mysore, P. Smaragdis
YEAR: 2011
SOURCE: 2011 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: We present a semi-supervised source separation methodology to denoise speech by modeling speech as one source and noise as the other source. We model speech using the recently pro posed non-negative hidden Markov model, which uses multiple non-negative dictionaries and a Markov chain to jointly model spectral structure and temporal dynamics of speech. We perform separation of the speech and noise using the recently proposed non-negative factorial hidden Markov model. Although the speech model is learned from training data, the noise model is learned during the separation process and re quires no training data. We show that the proposed method achieves superior results to using non-negative spectrogram factorization, which ignores the non-stationarity and temporal dynamics of speech.

177.
TITLE: Single-Channel Speech Separation using Sparse Non-Negative Matrix Factorization
AUTHORS: K.N. Schmidt, R.K. Olsson
YEAR: 2006
SOURCE: Interspeech 2006
ABSTRACT: We apply machine learning techniques to the problem of separating multiple speech sources from a single microphone recording. The method of choice is a sparse non-negative matrix factorization algorithm, which in an unsupervised manner can learn sparse representations of the data. This is applied to the learning of personalized dictionaries from a speech corpus, which in turn are used to separate the audio stream into its components. We show that computational savings can be achieved by segmenting the training data on a phoneme level. To split the data, a conventional speech recognizer is used. The performance of the unsupervised and supervised adaptation schemes result in significant improvements in terms of the target-to-masker ratio.

178.
TITLE: Sparse Overcomplete Decomposition for Single Channel Speaker Separation
AUTHORS: M.V.S. Shashanka, B. Raj, P. Smaragdis
YEAR: 2007
SOURCE: 2007 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: We present an algorithm for separating multiple speakers from a mixed single channel recording. The algorithm is based on a model proposed by Raj and Smaragdis (2005). The idea is to extract certain characteristic spectra-temporal basis functions from training data for individual speakers and decompose the mixed signals as linear combinations of these learned bases. In other words, their model extracts a compact code of basis functions that can explain the space spanned by spectral vectors of a speaker. In our model, we generate a sparse-distributed code where we have more basis functions than the dimensionality of the space. We propose a probabilistic framework to achieve sparsity. Experiments show that the resulting sparse code better captures the structure in data and hence leads to better separation.

179.
TITLE: Static and Dynamic Source Separation Using Nonnegative Factorizations: A Unified View
AUTHORS: P. Smaragdis, C. Fevotte, G.J. Mysore, N. Mohammadiha, M. Hoffman
YEAR: 2014
SOURCE: IEEE Signal Processing Magazine
ABSTRACT: Source separation models that make use of nonnegativity in their parameters have been gaining increasing popularity in the last few years, spawning a significant number of publications on the topic. Although these techniques are conceptually similar to other matrix decompositions, they are surprisingly more effective in extracting perceptually meaningful sources from complex mixtures. In this article, we will examine the various methodologies and extensions that make up this family of approaches and present them under a unified framework. We will begin with a short description of the basic concepts and in the subsequent sections we will delve in more details and explore some of the latest extensions.

180.
TITLE: Learnable Low Rank Sparse Models for Speech Denoising
AUTHORS: P. Sprechmann, A. Bronstein, M. Bronstein, G. Sapiro
YEAR: 2013
SOURCE: 2013 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: In this paper we present a framework for real time enhancement of speech signals. Our method leverages a new process-centric approach for sparse and parsimonious models, where the representation pursuit is obtained applying a deterministic function or process rather than solving an optimization problem. We first propose a rank-regularized robust version of non-negative matrix factorization (NMF) for modeling time-frequency representations of speech signals in which the spectral frames are decomposed as sparse linear combinations of atoms of a low-rank dictionary. Then, a parametric family of pursuit processes is derived from the iteration of the proximal descent method for solving this model. We present several experiments showing successful results and the potential of the proposed framework. Incorporating discriminative learning makes the proposed method significantly outperform exact NMF algorithms, with fixed latency and at a fraction of it's computational complexity.

181.
TITLE: Supervised Non-Euclidean Sparse NMF via Bilevel Optimization with Applications to Speech Enhancement
AUTHORS: P. Sprechmann, A. Bronstein, G. Sapiro
YEAR: 2014
SOURCE: 2014 4th Joint Workshop on Hands-free Speech Communication and Microphone Arrays
ABSTRACT: Traditionally, NMF algorithms consist of two separate stages: a training stage, in which a generative model is learned; and a testing stage in which the pre-learned model is used in a high level task such as enhancement, separation, or classification. As an alternative, we propose a task-supervised NMF method for the adaptation of the basis spectra learned in the first stage to enhance the performance on the specific task used in the second stage. We cast this problem as a bilevel optimization program that can be efficiently solved via stochastic gradient descent. The proposed approach is general enough to handle sparsity priors of the activations, and allow non-Euclidean data terms such as β-divergences. The framework is evaluated on single-channel speech enhancement tasks.

182.
TITLE: Real-time Online Singing Voice Separation from Monaural Recordings Using Robust Low-rank Modeling
AUTHORS: P. Sprechmann, A. Bronstein, G. Sapiro
YEAR: 2012
SOURCE: ISMIR 2012
ABSTRACT: Separating the leading vocals from the musical accompaniment is a challenging task that appears naturally in several music processing applications. Robust principal component analysis (RPCA) has been recently employed to this problem producing very successful results. The method decomposes the signal into a low-rank component corresponding to the accompaniment with its repetitive structure, and a sparse component corresponding to the voice with its quasiharmonic structure. In this paper we first introduce a non-negative variant of RPCA, termed as robust lowrank non-negative matrix factorization (RNMF). This new framework better suits audio applications. We then propose two efficient feed-forward architectures that approximate the RPCA and RNMF with low latency and a fraction of the complexity of the original optimization method. These approximants allow incorporating elements of unsupervised, semi- and fullysupervised learning into the RPCA and RNMF frameworks. Our basic implementation shows several orders of magnitude speedup compared to the exact solvers with no performance degradation, and allows online and faster-than-real-time processing. Evaluation on the MIR-1K dataset demonstrates state-of-the-art performance.

183.
TITLE: Performance Measurement in Blind Audio Source Separation
AUTHORS: E. Vincent, R. Gribonval, C. Fevotte
YEAR: 2006
SOURCE: IEEE Transactions on Audio, Speech, and Language Processing
ABSTRACT: In this paper, we discuss the evaluation of blind audio source separation (BASS) algorithms. Depending on the exact application, different distortions can be allowed between an estimated source and the wanted true source. We consider four different sets of such allowed distortions, from time-invariant gains to time-varying filters. In each case, we decompose the estimated source into a true source part plus error terms corresponding to interferences, additive noise, and algorithmic artifacts. Then, we derive a global performance measure using an energy ratio, plus a separate performance measure for each error term. These measures are computed and discussed on the results of several BASS problems with various difficulty levels

184.
TITLE: Discriminative NMF and Its Application to Single-Channel Source Separation
AUTHORS: F. Weninger, J. Le Roux, J. Hershey, S. Watanabe
YEAR: 2014
SOURCE: Interspeech 2014
ABSTRACT: The objective of single-channel source separation is to accurately recover source signals from mixtures. Non-negative matrix factorization (NMF) is a popular approach for this task, yet previous NMF approaches have not optimized directly this objective, despite some efforts in this direction. Our paper introduces discriminative training of the NMF basis functions such that, given the coefficients obtained on a mixture, a desired source is optimally recovered. We approach this optimization by generalizing the model to have separate analysis and reconstruction basis functions. This generalization frees us to optimize reconstruction objectives that incorporate the filtering step and SNR performance criteria. A novel multiplicative update algorithm is presented for the optimization of the reconstruction basis functions according to the proposed discriminative objective functions. Results on the 2nd CHiME Speech Separation and Recognition Challenge task indicate significant gains in source-to-distortion ratio with respect to sparse NMF, exemplar-based NMF, as well as a previously proposed discriminative NMF criterion.

185.
TITLE: Discriminatively Trained Recurrent Neural Networks for Single-Channel Speech Separation
AUTHORS: F. Weninger, J. Hershey, J. Le Roux, B. Schuller
YEAR: 2014
SOURCE: 2014 IEEE Global Conference on Signal and Information Processing
ABSTRACT: This paper describes an in-depth investigation of training criteria, network architectures and feature representations for regression-based single-channel speech separation with deep neural networks (DNNs). We use a generic discriminative training criterion corresponding to optimal source reconstruction from time-frequency masks, and introduce its application to speech separation in a reduced feature space (Mel domain). A comparative evaluation of time-frequency mask estimation by DNNs, recurrent DNNs and non-negative matrix factorization on the 2nd CHiME Speech Separation and Recognition Challenge shows consistent improvements by discriminative training, whereas long short-term memory recurrent DNNs obtain the overall best results. Furthermore, our results confirm the importance of fine-tuning the feature representation for DNN training.

186.
TITLE: Speech Denoising Using Nonnegative Matrix Factorization with Priors
AUTHORS: K.W. Wilson, B. Raj, P. Smaragdis, A. Divakaran
YEAR: 2008
SOURCE: 2008 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: We present a technique for denoising speech using nonnegative matrix factorization (NMF) in combination with statistical speech and noise models. We compare our new technique to standard NMF and to a state-of-the-art Wiener filter implementation and show improvements in speech quality across a range of interfering noise types.

187.
TITLE: Examining the Mapping Functions of Denoising Autoencoders in Music Source Separation
AUTHORS: S. Ioannis Mimilakis, K. Drossos, E. Cano, G. Schuller
YEAR: 2019
SOURCE: arXiv
ABSTRACT: The goal of this work is to investigate what music source separation approaches based on neural networks learn from the data. We examine the mapping functions of neural networks that are based on the denoising autoencoder (DAE) model, and conditioned on the mixture magnitude spectra. For approximating the mapping functions, we propose an algorithm that is inspired by the knowledge distillation and is denoted as the neural couplings algorithm (NCA). The NCA yields a matrix that expresses the mapping of the mixture to the target source magnitude information. Using the NCA we examine the mapping functions of three fundamental DAE models in music source separation; one with single layer encoder and decoder, one with multi-layer encoder and single layer decoder, and one using the skip-filtering connections (SF) with a single encoding and decoding layer. We first train these models with realistic data to estimate the singing voice magnitude spectra from the corresponding mixture. We then use the optimized models and test spectral data as input to the NCA. Our experimental findings show that approaches based on the DAE model learn scalar filtering operators, exhibiting a predominant diagonal structure in their corresponding mapping functions, limiting the exploitation of inter-frequency structure of music data. In contrast, skip-filtering connections are shown to assist the DAE model in learning filtering operators that exploit richer inter-frequency structure.

188.
TITLE: Speech Enhancement Using progressive Learning-Based Convolutional Recurrent Neural Network
AUTHORS: A. Li, M. Yuan, C. Zheng, X. Li
YEAR: 2020
SOURCE: Applied Acoustics
ABSTRACT: Recently, progressive learning has shown its capacity to improve speech quality and speech intelligibility when it is combined with deep neural network (DNN) and long short-term memory (LSTM) based monaural speech enhancement algorithms, especially in low signal-to-noise ratio (SNR) conditions. Nevertheless, due to a large number of parameters and high computational complexity, it is hard to implement in current resource-limited micro-controllers and thus, it is essential to significantly reduce both the number of parameters and the computational load for practical applications. For this purpose, we propose a novel progressive learning framework with causal convolutional recurrent neural networks called PL-CRNN, which takes advantage of both convolutional neural networks and recurrent neural networks to drastically reduce the number of parameters and simultaneously improve speech quality and speech intelligibility. Numerous experiments verify the effectiveness of the proposed PL-CRNN model and indicate that it yields consistent better performance than the PL-DNN and PL-LSTM algorithms and also it gets results close even better than the CRNN in terms of objective measurements. Compared with PL-DNN, PL-LSTM, and CRNN, the proposed PL-CRNN algorithm can reduce the number of parameters up to 93%, 97%, and 92%, respectively.

189.
TITLE: An Overview of Deep-Learning-Based Audio-Visual Speech Enhancement and Separation
AUTHORS: D. Michelsanti, et al.
YEAR: 2020
SOURCE: arXiv
ABSTRACT: Speech enhancement and speech separation are two related tasks, whose purpose is to extract either one or more target speech signals, respectively, from a mixture of sounds generated by several sources. Traditionally, these tasks have been tackled using signal processing and machine learning techniques applied to the available acoustic signals. More recently, visual information from the target speakers, such as lip movements and facial expressions, has been introduced to speech enhancement and speech separation systems, because the visual aspect of speech is essentially unaffected by the acoustic environment. In order to efficiently fuse acoustic and visual information, researchers have exploited the flexibility of data-driven approaches, specifically deep learning, achieving state-of-the-art performance. The ceaseless proposal of a large number of techniques to extract features and fuse multimodal information has highlighted the need for an overview that comprehensively describes and discusses audio-visual speech enhancement and separation based on deep learning. In this paper, we provide a systematic survey of this research topic, focusing on the main elements that characterise the systems in the literature: visual features; acoustic features; deep learning methods; fusion techniques; training targets and objective functions. We also survey commonly employed audio-visual speech datasets, given their central role in the development of data-driven approaches, and evaluation methods, because they are generally used to compare different systems and determine their performance. In addition, we review deeplearning-based methods for speech reconstruction from silent videos and audio-visual sound source separation for non-speech signals, since these methods can be more or less directly applied to audio-visual speech enhancement and separation.

190.
TITLE: Real Time Speech Enhancement in the Waveform Domain
AUTHORS: A. Defossez, G. Synnaeve, Y. Adi
YEAR: 2020
SOURCE: arXiv
ABSTRACT: We present a causal speech enhancement model working on the raw waveform that runs in real-time on a laptop CPU. The proposed model is based on an encoder-decoder architecture with skip-connections. It is optimized on both time and frequency domains, using multiple loss functions. Empirical evidence shows that it is capable of removing various kinds of background noise including stationary and non-stationary noises, as well as room reverb. Additionally, we suggest a set of data augmentation techniques applied directly on the raw waveform which further improve model performance and its generalization abilities. We perform evaluations on several standard benchmarks, both using objective metrics and human judgements. The proposed model matches state-of-the-art performance of both causal and non causal methods while working directly on the raw waveform.

191.
TITLE: Deep Speech Extraction with Time-Varying Spatial Filtering Guided By Desired Direction Attractor
AUTHORS: Y. Nakagome, M. Togami, T. Ogawa, T. Kobayashi
YEAR: 2020
SOURCE: 2020 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: In this investigation, a deep neural network (DNN) based speech extraction method is proposed to enhance a speech signal propagating from the desired direction. The proposed method integrates knowledge based on a sound propagation model and the time-varying characteristics of a speech source, into a DNN-based separation framework. This approach outputs a separated speech source using time-varying spatial filtering, which achieves superior speech extraction performance compared with time-invariant spatial filtering. Given that the gradient of all modules can be calculated, back-propagation can be performed to maximize the speech quality of the output signal in an end-to-end manner. Guided information is also modeled based on the sound propagation model, which facilitates disentangled representations of the target speech source and noise signals. The experimental results demonstrate that the proposed method can extract the target speech source more accurately than conventional DNN-based speech source separation and conventional speech extraction using time-invariant spatial filtering.

192.
TITLE: Beamformed Feature for Learning-based Dual-channel Speech Separation
AUTHORS: H. Li, X. Zhang, G. Gao
YEAR: 2020
SOURCE: 2020 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: This paper deals with the problem of separating target speech signal from reverberant and noisy environment with dual microphones, where the target speech comes from a predefined direction range. First, we apply two differential beamformers with opposite directions to dual-channel inputs. Then, the power spectra of beamforming outputs are used as input feature of deep learning architecture. As input features, the beamformer outputs reflect not only spectral information but also directional information by their power level difference. And the calculation is very simple. Systematic evaluation and comparison show that the proposed system achieves very good separation performance and substantially outperforms related algorithms under very challenging environments where both interfering speaker, noise and reverberations are present.

193.
TITLE: A Study of Child Speech Extraction Using Joint Speech Enhancement and Separation in Realistic Conditions
AUTHORS: X. Wang, J. Du, A. Cristia, L. Sun, C. Lee
YEAR: 2020
SOURCE: 2020 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: In this paper, we design a novel joint framework of speech enhancement and speech separation for child speech extraction in realistic conditions, targeting the problem of extracting child speech from daily conversations in BabyTrain mega corpus. To the best of our knowledge, it is the first discussion of a feasible method for child speech extraction in realistic conditions. First, we make detailed analysis of the BabyTrain mega corpus, which is recorded in adverse environments. We observe problems of background noises, reverberations and child speech that is partially obscured by adult speech (for instance due to speaker overlap but also imitation by the adult). Motivated by this, we conduct a joint framework of speech enhancement and speech separation for child speech extraction. To measure the extraction results in realistic conditions, we propose several objective measurements to evaluate the performance of the our system, which is different from those commonly used for simulation data. Compared with the unprocessed approach and classification approach, our proposed approach can yield the best performance among all subsets of BabyTrain.

194.
TITLE: Monaural Speech Enhancement Using Intra-Spectral Recurrent Layers in the Magnitude and Phase Responses
AUTHORS: K.M. Nayem, D.S. Williamson
YEAR: 2020
SOURCE: 2020 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: Speech enhancement has greatly benefited from deep learning. Currently, the best performing deep architectures use long short-term memory (LSTM) recurrent neural networks (RNNs) to model short and long temporal dependencies. These approaches, however, underutilize or ignore spectral-level dependencies within the magnitude and phase responses, respectively. In this paper, we propose a deep learning architecture that leverages both temporal and spectral dependencies within the magnitude and phase responses. More specifically, we first train a LSTM network to predict both the spectral-magnitude response and group delay, where this model captures temporal correlations. We then introduce Markovian recurrent connections in the output layers to capture spectral dependencies within the magnitude and phase responses. We compare our approach with traditional enhancement approaches and approaches that consider spectral dependencies within a single time frame. The results show that considering the within-frame spectral dependencies leads to improvements.

195.
TITLE: Two-Stage Model and Optimal SI-SNR for Monoaural Multi-Speaker Speech Separation in Noisy Environment
AUTHORS: C. Ma, D. Li, X. Jia
YEAR: 2020
SOURCE: arXiv
ABSTRACT: In daily listening environments, speech is always distorted by background noise, room reverberation and interference speakers. With the developing of deep learning approaches, much progress has been performed on monaural multi-speaker speech separation. Nevertheless, most studies in this area focus on a simple problem setup of laboratory environment, which background noises and room reverberations are not considered. In this paper, we propose a two-stage model based on conv-TasNet to deal with the notable effects of noises and interference speakers separately, where enhancement and separation are conducted sequentially using deep dilated temporal convolutional networks (TCN). In addition, we develop a new objective function named optimal scale-invariant signal-noise ratio (OSI-SNR), which are better than original SI-SNR at any circumstances. By jointly training the two-stage model with OSI-SNR, our algorithm outperforms one-stage separation baselines substantially.

196.
TITLE: Multichannel Singing Voice Separation by Deep Neural Network Informed DOA Constrained CNMF
AUTHORS: A. Munoz-Montoro, J.J. Carabias-Orti, A. Politis, K. Drossos
YEAR: 2020
SOURCE: arXiv
ABSTRACT: This work addresses the problem of multichannel source separation combining two powerful approaches, multichannel spectral factorization with recent monophonic deep-learning (DL) based spectrum inference. Individual source spectra at different channels are estimated with a Masker-Denoiser Twin Network (MaD TwinNet), able to model long-term temporal patterns of a musical piece. The monophonic source spectrograms are used within a spatial covariance mixing model based on Complex Non-Negative Matrix Factorization (CNMF) that predicts the spatial characteristics of each source. The proposed framework is evaluated on the task of singing voice separation with a large multichannel dataset. Experimental results show that our joint DL+CNMF method outperforms both the individual monophonic DL-based separation and the multichannel CNMF baseline methods.

197.
TITLE: SNR-Based Features and Diverse Training Data for Robust DNN-Based Speech Enhancement
AUTHORS: R. Rehr, T. Gerkmann
YEAR: 2020
SOURCE: arXiv
ABSTRACT: This paper analyzes the generalization of speech enhancement algorithms based on deep neural networks (DNNs) with respect to (1) the chosen features, (2) the size and diversity of the training data, and (3) different network architectures. To address (1), we compare three input features, namely logarithmized noisy periodograms, noise aware training (NAT) and signal-to-noise ratio (SNR) based noise aware training (SNR-NAT). For improving the generalization over noisy periodograms, NAT appends an estimate of the noise power spectral density (PSD) to these features, whereas the proposed SNR-NAT uses the noise PSD estimate for normalization. To address (2), we train networks on the Hu noise corpus (limited size), the CHiME 3 noise corpus (limited diversity) and also propose a large and diverse dataset collected based on freely available sounds. On the one hand, we show that increasing the amount and the diversity of training data helps DNNs to generalize. On the other hand, via experimental results and an analysis using t-distributed stochastic neighbor embedding (t-SNE) we show that SNR-NAT features are robust even if the size and the diversity of the input data are limited. To address (3), we compare a fully-connected feed-forward DNN and an long short-term memory (LSTM) and show that the LSTM generalizes better for limited training data and simplistic features

198.
TITLE: Multi-Source Classification: A DOA-Based Deep Learning Approach
AUTHORS: J. Hu, Q. Mo, Z. Liu, Li-han
YEAR: 2020
SOURCE: 2020 International Conference on Computer Engineering and Application
ABSTRACT: Neural networks provide powerful tools to assist in the conventional field of signal processing. In this paper, to promote the capability of a microphone array in distinguishing acoustical signals from different locations, we extract the direction of arrival (DOA) information and perform the multi label classification based on deep learning. More specifically, by extracting the directional features from the generalized cross-correlation (GCC) vectors and assigning multiple labels to the estimated DOA, we transform the traditional DOA estimation problem into a multilabel classifycation problem. The transformed problem is further solved by applying a simple convolutional neural network (CNN). Experimental results on both real and simulated data show the validity and superiority of the proposed algorithm.

199.
TITLE: Efficient Trainable Front-Ends for Neural Speech Enhancement
AUTHORS: J. Casebeer, U. Isik, S. Venkataramani, A. Krishnaswamy
YEAR: 2020
SOURCE: 2020 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: Many neural speech enhancement and source separation systems operate in the time-frequency domain. Such models often benefit from making their Short-Time Fourier Transform (STFT) front-ends trainable. In current literature, these are implemented as large Discrete Fourier Transform matrices; which are prohibitively inefficient for low-compute systems. We present an efficient, trainable front-end based on the butterfly mechanism to compute the Fast Fourier Transform, and show its accuracy and efficiency benefits for low-compute neural speech enhancement models. We also explore the effects of making the STFT window trainable. 

200.
TITLE: Efficient, End-to-End and Self-Supervised Methods for Speech Processing and Generation
AUTHORS: S. Pascual de la Puente
YEAR: 2020
SOURCE: University Politecnica de Catalunya
ABSTRACT: Deep learning has affected the speech processing and generation fields in many directions. First, end-to-end architectures allow the direct injection and synthesis of waveform samples. Secondly, the exploration of efficient solutions allow to implement these systems in computationally restricted environments, like smartphones. Finally, the latest trends exploit audio-visual data with least supervision. In this thesis these three directions are explored. Firstly, we propose the use of recent pseudo-recurrent structures, like self-attention models and quasi-recurrent networks, to build acoustic models for text-to-speech. The proposed system, QLAD, turns out to synthesize faster on CPU and GPU than its recurrent counterpart whilst preserving the good synthesis quality level, which is competitive with state of the art vocoder-based models. Then, a generative adversarial network is proposed for speech enhancement, named SEGAN. This model works as a speech-to-speech conversion system in time-domain, where a single inference operation is needed for all samples to operate through a fully convolutional structure. This implies an increment in modeling efficiency with respect to other existing models, which are auto-regressive and also work in time-domain. SEGAN achieves prominent results in noise supression and preservation of speech naturalness and intelligibility when compared to the other classic and deep regression based systems. We also show that SEGAN is efficient in transferring its operations to new languages and noises. A SEGAN trained for English performs similarly to this language on Catalan and Korean with only 24 seconds of adaptation data. Finally, we unveil the generative capacity of the model to recover signals from several distortions. We hence propose the concept of generalized speech enhancement. First, the model proofs to be effective to recover voiced speech from whispered one. Then the model is scaled up to solve other distortions that require a recomposition of damaged parts of the signal, like extending the bandwidth or recovering lost temporal sections, among others. The model improves by including additional acoustic losses in a multi-task setup to impose a relevant perceptual weighting on the generated result. Moreover, a two-step training schedule is also proposed to stabilize the adversarial training after the addition of such losses, and both components boost SEGAN's performance across distortions.Finally, we propose a problem-agnostic speech encoder, named PASE, together with the framework to train it. PASE is a fully convolutional network that yields compact representations from speech waveforms. These representations contain abstract information like the speaker identity, the prosodic features or the spoken contents. A self-supervised framework is also proposed to train this encoder, which suposes a new step towards unsupervised learning for speech processing. Once the encoder is trained, it can be exported to solve different tasks that require speech as input. We first explore the performance of PASE codes to solve speaker recognition, emotion recognition and speech recognition. PASE works competitively well compared to well-designed classic features in these tasks, specially after some supervised adaptation. Finally, PASE also provides good descriptors of identity for multi-speaker modeling in text-to-speech, which is advantageous to model novel identities without retraining the model.

201.
TITLE: Finding Strength in Weakness: Learning to Separate Sounds With Weak Supervision
AUTHORS: F. Pishdadian, G. Wichern, J. Le Roux
YEAR: 2020
SOURCE: IEEE/ACM Transactions on Audio, Speech, and Language Processing
ABSTRACT: While there has been much recent progress using deep learning techniques to separate speech and music audio signals, these systems typically require large collections of isolated sources during the training process. When extending audio source separation algorithms to more general domains such as environmental monitoring, it may not be possible to obtain isolated signals for training. Here, we propose objective functions and network architectures that enable training a source separation system with weak labels. In this scenario, weak labels are defined in contrast with strong time-frequency (TF) labels such as those obtained from isolated sources, and refer either to frame-level weak labels where one only has access to the time periods when different sources are active in an audio mixture, or to clip-level weak labels that only indicate the presence or absence of sounds in an entire audio clip. We train a separator that estimates a TF mask for each type of sound event, using a sound event classifier as an assessor of the separator's performance to bridge the gap between the TF-level separation and the ground truth weak labels only available at the frame or clip level. Our objective function requires the separator to estimate a source such that the classifier applied to it will assign high probability to the class corresponding to that source and low probability to all other classes. The objective function also enforces that the separated sources sum up to the mixture. We benchmark the performance of our algorithm using synthetic mixtures of overlapping events created from a database of sounds recorded in urban environments, and show that the method can also be applied to other tasks such as music source separation. Compared to training a network using isolated sources, our model achieves somewhat lower but still significant SI-SDR improvement, even in scenarios with significant sound event overlap.

202.
TITLE: Transfer Learning Algorithm for Enhancing the Unlabeled Speech
AUTHORS: R. Liang, Z. Liang, J. Cheng, Y. Xie, Q. Wang
YEAR: 2020
SOURCE: IEEE Access
ABSTRACT: To improve the generalization ability of speech enhancement algorithms for unlabeled noisy speech, a speech enhancement transfer learning model based on the feature-attention multi-kernel maximum mean discrepancy (FA-MK-MMD) is proposed. To obtain a representation of the shared subspace (the part related with clean speech in the feature extracted by shared encoder) between source domain (speech with known noise and labels) and target domain (speech with unknown noise and no labels), the algorithm takes MK-MMD as loss function for reducing distribution differences between these two domains, which could improve the adaptability to the unknown noise. Furthermore, considering that different noise have different influence on the representation of shared subspace, the attention mechanism is applied to feature dimension to screen out the information less polluted by noise, which is helpful for reconstructing the clean speech. In the term of speech with unknown noise and no labels, the experiments demonstrate that the proposed algorithm has improved the frequency-weighed segmental signal-to-noise ratio (fwsegSNR), the perceptual evaluation of the speech quality (PESQ) and the short time objective intelligibility (STOI) compared with the baseline algorithm.

203.
TITLE: Learning Complex Spectral Mapping With Gated Convolutional Recurrent Networks for Monaural Speech Enhancement
AUTHORS: K. Tan, D. Wang
YEAR: 2020
SOURCE: IEEE/ACM Transactions on Audio, Speech, and Language Processing
ABSTRACT: Phase is important for perceptual quality of speech. However, it seems intractable to directly estimate phase spectra through supervised learning due to their lack of spectrotemporal structure in it. Complex spectral mapping aims to estimate the real and imaginary spectrograms of clean speech from those of noisy speech, which simultaneously enhances magnitude and phase responses of speech. Inspired by multi-task learning, we propose a gated convolutional recurrent network (GCRN) for complex spectral mapping, which amounts to a causal system for monaural speech enhancement. Our experimental results suggest that the proposed GCRN substantially outperforms an existing convolutional neural network (CNN) for complex spectral mapping in terms of both objective speech intelligibility and quality. Moreover, the proposed approach yields significantly higher STOI and PESQ than magnitude spectral mapping and complex ratio masking. We also find that complex spectral mapping with the proposed GCRN provides an effective phase estimate.

204.
TITLE: Speech Enhancement Based on Simple Recurrent Unit Network
AUTHORS: X. Cui, Z. Chen, F. Yin
YEAR: 2020
SOURCE: Applied Acoustics
ABSTRACT: Abstract Speech enhancement is a crucial and challenging task in many applications. A novel speech enhancement method based on the simple recurrent unit (SRU) is proposed in this paper. First, the log-power spectra of noisy and clean speeches are extracted. Then, the mapping relationship between noisy and clean speech spectra is learned by a multiple-layer stacked SRU network. Finally, the well-trained model is used to predict the corresponding clean speech spectra from the noisy speech spectra and the whole clean speech waveform can be recovered. Compared with the existing algorithms, DNN, LSTM and GRU, the proposed method achieves significant improvements at training speed and has capability to balance the performance and the training time. Experimental results demonstrate the validity and robustness of the proposed method.

205.
TITLE: Using a Neural Network Codec Approximation Loss to Improve Source Separation Performance in Limited Capacity Networks
AUTHORS: I. Ananthabhotla, S. Ewert, J. Paradiso
YEAR: 2020
SOURCE:
ABSTRACT: A growing need for on-device machine learning has led to an increased interest in light-weight neural networks that lower model complexity while retaining performance. While a variety of general-purpose techniques exist in this context, very few approaches exploit domain-specific properties to further improve upon the capacity-performance trade-off. In this paper, extending our prior work [1], we train a network to emulate the behaviour of an audio codec and use this network to construct a loss. By approximating the psychoacoustic model underlying the codec, our approach enables light-weight neural networks to focus on perceptually relevant properties without wasting their limited capacity on imperceptible signal components. We adapt our method to two audio source separation tasks, demonstrate an improvement in performance for small-scale networks via listening tests, characterize the behaviour of the loss network in detail, and quantify the relationship between performance gain and model capacity. Our work illustrates the potential for incorporating perceptual principles into objective functions for neural networks.

206.
TITLE: A Multi-Target SNR-Progressive Learning Approach to Regression Based Speech Enhancement
AUTHORS: Y. Tu, J. Du, T. Gao, C. Lee
YEAR: 2020
SOURCE: IEEE/ACM Transactions on Audio, Speech, and Language Processing
ABSTRACT: We propose a multi-target, signal-to-noise-ratio (SNR)-progressive learning (SNR-PL) framework for regression based speech enhancement (SE). At low SNR levels, it is often not easy to directly learn the complicated regression required in SE. We therefore decompose the original SE problem of mapping noisy to clean speech features, with a large SNR gap, into a series of sub-problems, each with a small SNR increment and presumably easier to learn. In our configurations, each hidden layer of the proposed regression neural network is guided to explicitly learn an intermediate target with a specified but small SNR gain. Tested on both deep neural network (DNN) and long short-term memory (LSTM) architectures, SNR-PL consistently outperforms the conventional “black box” DNN framework in terms of both objective measure superiority and network model compactness. Furthermore, with the best configured LSTM-based SNR-PL model, we often observe that the performance is easily saturated or even degraded when increasing the number of intermediate targets, due to the fact that useful information is lost in dimension reduction when involving more target layers. Accordingly, to address this information loss issue, we explore densely connected networks on top of the LSTM structure where the input and the preceding intermediate targets are concatenated together to learn the next target. Finally, to fully utilize the rich and complementary information of intermediate targets, a simple post-processing strategy is adopted to further improve the performance. Evaluated on the simulation speech data, experimental results in unseen noises cases demonstrate that the proposed approach consistently performs better than the conventional LSTM approach in terms of objective speech enhancement measures for speech intelligibility and quality. Furthermore, when evaluated on real data provided by the CHiME-4 Challenge for automatic speech recognition (ASR) of noisy microphone array speech, we show that the proposed approach with intermediate outputs can directly improve the ASR performance, while the conventional LSTM approach increases the word error rate.

207.
TITLE: PSD and Signal Approximation-LSTM Based Speech Enhancement
AUTHORS: Y. Li, Y. Sun, S.M. Naqvi
YEAR: 2019
SOURCE: 2019 13th International Conference on Signal Processing and Communication Systems
ABSTRACT: Monaural speech enhancement is a challenging problem because the desired signal is estimated from singlechannel recordings. Numbers of methods have been proposed, however, due to the ignored pertinence of the specific frequency range of speech signals, the performance of the current approaches is limited. In this paper, we divide the speech mixture into two subbands and extract the desired speech signal from each frequency band based on the power spectral density (PSD) of noise mixtures. The proposed method trains two long short-term memory (LSTM) recurrent neural networks (RNNs) in parallel for the subband short time Fourier transform (STFT) of speech segments. The proposed LSTM RNN-based signal approximation (SA) method is evaluated with the IEEE and the TIMIT datasets with various noise interferences from the NOISEX dataset. The evaluation results confirm that the proposed method outperforms the state-of-the-art.

208.
TITLE: Speech Enhancement Based on Teacher–Student Deep Learning Using Improved Speech Presence Probability for Noise-Robust Speech Recognition
AUTHORS: Y. Tu, C. Lee
YEAR: 2019
SOURCE: IEEE/ACM Transactions on Audio, Speech, and Language Processing
ABSTRACT: In this paper, we propose a novel teacher-student learning framework for the preprocessing of a speech recognizer, leveraging the online noise tracking capabilities of improved minima controlled recursive averaging (IMCRA) and deep learning of nonlinear interactions between speech and noise. First, a teacher model with deep architectures is built to learn the target of ideal ratio masks (IRMs) using simulated training pairs of clean and noisy speech data. Next, a student model is trained to learn an improved speech presence probability by incorporating the estimated IRMs from the teacher model into the IMCRA approach. The student model can be compactly designed in a causal processing mode having no latency with the guidance of a complex and noncausal teacher model. Moreover, the clean speech requirement, which is difficult to meet in real-world adverse environments, can be relaxed for training the student model, implying that noisy speech data can be directly used to adapt the regression-based enhancement model to further improve speech recognition accuracies for noisy speech collected in such conditions. Experiments on the CHiME-4 challenge task show that our best student model with bidirectional gated recurrent units (BGRUs) can achieve a relative word error rate (WER) reduction of 18.85% for the real test set when compared to unprocessed system without acoustic model retraining. However, the traditional teacher model degrades the performance of the unprocessed system in this case. In addition, the student model with a deep neural network (DNN) in causal mode having no latency yields a relative WER reduction of 7.94% over the unprocessed system with 670 times less computing cycles when compared to the BGRU-equipped student model. Finally, the conventional speech enhancement and IRM-based deep learning method destroyed the ASR performance when the recognition system became more powerful. While our proposed approach could still improve the ASR performance even in the more powerful recognition system.

209.
TITLE: Time-Domain Speech Enhancement Using Generative Adversarial Networks
AUTHORS: S. Pascual,J.Serra, A. Bonafonte
YEAR: 2019
SOURCE: Speech Communication
ABSTRACT: Speech enhancement improves recorded voice utterances to eliminate noise that might be impeding their intelligibility or compromising their quality. Typical speech enhancement systems are based on regression approaches that subtract noise or predict clean signals. Most of them do not operate directly on waveforms. In this work, we propose a generative approach to regenerate corrupted signals into a clean version by using generative adversarial networks on the raw signal. We also explore several variations of the proposed system, obtaining insights into proper architectural choices for an adversarially trained, convolutional autoencoder applied to speech. We conduct both objective and subjective evaluations to assess the performance of the proposed method. The former helps us choose among variations and better tune hyperparameters, while the latter is used in a listening experiment with 42 subjects, confirming the effectiveness of the approach in the real world. We also demonstrate the applicability of the approach for more generalized speech enhancement, where we have to regenerate voices from whispered signals.

210.
TITLE: Deep Learning for Talker-Dependent Reverberant Speaker Separation: An Empirical Study
AUTHORS: M. Delfarah, D. Wang
YEAR: 2019
SOURCE: IEEE/ACM Transactions on Audio, Speech, and Language Processing
ABSTRACT: Speaker separation refers to the problem of separating speech signals from a mixture of simultaneous speakers. Previous studies are limited to addressing the speaker separation problem in anechoic conditions. This paper addresses the problem of talker-dependent speaker separation in reverberant conditions, which are characteristic of real-world environments. We employ recurrent neural networks with bidirectional long short-term memory (BLSTM) to separate and dereverberate the target speech signal. We propose two-stage networks to effectively deal with both speaker separation and speech dereverberation. In the two-stage model, the first stage separates and dereverberates two-talker mixtures and the second stage further enhances the separated target signal. We have extensively evaluated the two-stage architecture, and our empirical results demonstrate large improvements over unprocessed mixtures and clear performance gain over single-stage networks in a wide range of target-to-interferer ratios and reverberation times in simulated as well as recorded rooms. Moreover, we show that time-frequency masking yields better performance than spectral mapping for reverberant speaker separation.

211.
TITLE: Separated Noise Suppression and Speech Restoration: Lstm-Based Speech Enhancement in Two Stages
AUTHORS: M. Strake, B. Defraene, K. Fluyt, W. Tirry, T. Flingscheidt
YEAR: 2019
SOURCE: 2019 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics
ABSTRACT: Regression based on neural networks (NNs) has led to considerable advances in speech enhancement under non-stationary noise conditions. Nonetheless, speech distortions can be introduced when employing NNs trained to provide strong noise suppression. We propose to address this problem by first suppressing noise and subsequently restoring speech with specifically chosen NN topologies for each of these distinct tasks. A mask-estimating long short-term memory (LSTM) network is employed for noise suppression, while the speech restoration is performed by a fully convo-lutional encoder-decoder (CED) network, where we introduce temporal modeling capabilities by using a convolutional LSTM layer in the bottleneck. We show considerable performance gains over reference methods of up to 0.26 MOS points (PESQ) and the ability to significantly improve intelligibility in terms of STOI for low-SNR conditions.

212.
TITLE: Incorporating Intra-Spectral Dependencies with a Recurrent Output Layer for Improved Speech Enhancement
AUTHORS: K.M. Nayem, D.S. Williamson
YEAR: 2019
SOURCE: 2019 IEEE 29th International Workshop on Machine Learning for Signal Processing
ABSTRACT: Deep-learning based speech enhancement systems have offered tremendous gains, where the best performing approaches use long short-term memory (LSTM) recurrent neural networks (RNNs) to model temporal speech correlations. These models, however, do not consider the frequency-level correlations within a single time frame, as spectral dependencies along the frequency axis are often ignored. This results in inaccurate frequency responses that negatively affect perceptual quality and intelligibility. We propose a deep-learning approach that considers temporal and frequency-level dependencies. More specifically, we enforce spectral-level dependencies within each spectral time frame through the introduction of a recurrent output layer that models the Markovian assumption along the frequency axis. We evaluate our approach in a variety of speech and noise environments, and objectively show that this recurrent spectral layer offers performance gains over traditional approaches. We also show that our approach outperforms recent approaches that consider frequency-level dependencies

213.
TITLE: Eigenvector-Based Speech Mask Estimation for Multi-Channel Speech Enhancement
AUTHORS: L. Pfeifenberger, M. Bohrer, F. Pernkopf
YEAR: 2019
SOURCE: IEEE/ACM Transactions on Audio, Speech, and Language Processing
ABSTRACT: We present the Eigennet architecture for estimating a gain mask from noisy, multi-channel microphone observations. While existing mask estimators use magnitude features, our system also exploits the spatial information embedded in the phase of the data. The mask is used to obtain the Minimum Variance Distortionless Response (MVDR) and Generalized Eigenvalue (GEV) beamformers. We also derive the Phase Aware Normalization (PAN) postfilter, which corrects both magnitude and phase distortions caused by the GEV. Further, we demonstrate the properties of our eigenvector features, and compare their performance with three state-of-the-art reference systems. We report their performance in terms of SNR improvement and Word Error Rate (WER) using Google and Kaldi Speech-to-Text API. Experiments are performed on the WSJ0 and CHiME4 corpora, where competitive performance in both WER and SNR is achieved.

214.
TITLE: Investigation of Cost Function for Supervised Monaural Speech Separation
AUTHORS: Y. Liu, H. Zhang, X. Zhang, Y. Cao
YEAR: 2019
SOURCE: Interspeech 2019
ABSTRACT: Speech separation aims to improve the speech quality of noisy speech. Deep learning based speech separation methods usually use mean square error (MSE) as the cost function, which measures the distance between model output and training target. However, the MSE does not match the evaluation metrics perfectly. Optimizing the MSE does not directly lead to improvement in the commonly used metrics, such as shorttime objective intelligibility (STOI), perceptual evaluation of speech quality (PESQ), signal-to-noise ratio (SNR) and sourceto-distortion ratio (SDR). In this study, we inspect some other cost function candidates which are based on divergence, e.g., Kullback-Leibler and Itakura-Saito divergence. A conjecture about the correlation between cost function and evaluation metrics is proposed and examined to explain why these cost functions behave differently. On the basis of the proposed conjecture, the optimal cost function candidate is selected. The experimental results validate our conjecture.

215.
TITLE: Jointly Adversarial Enhancement Training for Robust End-to-End Speech Recognition
AUTHORS: B. Liu, et al.
YEAR: 2019
SOURCE: Interspeech 2019
ABSTRACT: Recently, the end-to-end system has made significant breakthroughs in the field of speech recognition. However, this single end-to-end architecture is not especially robust to the input variations interfered of noises and reverberations, resulting in performance degradation dramatically in reality. To alleviate this issue, the mainstream approach is to use a well-designed speech enhancement module as the front-end of ASR. However, enhancement modules would result in speech distortions and mismatches to training, which sometimes degrades the ASR performance. In this paper, we propose a jointly adversarial enhancement training to boost robustness of end-to-end systems. Specifically, we use a jointly compositional scheme of maskbased enhancement network, attention-based encoder-decoder network and discriminant network during training. The discriminator is used to distinguish between the enhanced features from enhancement network and clean features, which could guide enhancement network to output towards the realistic distribution. With the joint optimization of the recognition, enhancement and adversarial loss, the compositional scheme is expected to learn more robust representations for the recognition task automatically. Systematic experiments on AISHELL-1 show that the proposed method improves the noise robustness of end-to-end systems and achieves the relative error rate reduction of 4.6% over the multi-condition training.

216.
TITLE: Monaural Source Separation Based on Sequentially Trained LSTMs in Real Room Environments
AUTHORS: Y.H. Li, Y. Sun, S.M. Naqvi
YEAR: 2019
SOURCE: 2019 27th European Signal Processing Conference
ABSTRACT: In recent studies on Monaural Source Separation (MSS), the long short-term memory (LSTM) network has been introduced to solve this problem, however, its performance is still limited particularly in real room environments. According to the training objectives, the LSTM-based MSS is categorized into three aspects, namely mapping, masking and signal approximation (SA) based methods. In this paper, we introduce dereverberation mask (DM) and establish a system to train two SA-LSTMs sequentially, which dereverberate speech mixture and improve the separation performance. The DM is exploited as the training target of the first LSTM. Then, the enhanced ratio mask (ERM) is proposed and set as the training target of the second LSTM. We evaluate the proposed method with the IEEE and the TIMIT datasets with real room impulse responses and noise interferences from the NOISEX dataset. The detailed evaluations confirm that the proposed method outperforms the state-of-the-art.

217.
TITLE: Data-Dependent Ensemble of Magnitude Spectrum Predictions for Single Channel Speech Enhancement
AUTHORS: P. Pertila
YEAR: 2019
SOURCE: 2019 IEEE 21st International Workshop on Multimedia Signal Processing
ABSTRACT: Applying a predicted time-frequency mask to the noisy speech spectrogram and directly predicting the clean speech magnitude spectrogram are two common deep learning-based speech enhancement approaches. Ensemble techniques such as averaging and neural network-based fusion of magnitude spectra obtained with these two approaches have been shown to improve the objective perceptual quality of speech using synthetic mixtures of data. This work generalizes the averaging ensemble approach by proposing neural network layers to predict time-frequency varying weights for the combination of the two magnitude spectra obtained by time-frequency masking and by direct prediction. In order to combine the best individual magnitude spectrum estimates, the proposed weight prediction layers are trained after the time-frequency mask and magnitude spectrum networks layers have been separately trained for their corresponding objectives and their weights have been fixed. Using the publicly available CHiME3-challenge data, which consists of both simulated and real speech recordings in everyday environments with noise and interference, the proposed approach leads to significantly higher noise suppression in terms of segmental source-to-distortion ratio over the alternative approaches. In addition, the approach achieves similar improvements in the average objective instrumentally measured intelligibility scores with respect to the best achieved scores.

218.
TITLE: Convolutional Recurrent Neural Network Based Progressive Learning for Monaural Speech Enhancement
AUTHORS: A. Li, C. Zheng, X. Li
YEAR: 2019
SOURCE: arXiv
ABSTRACT: Recently, progressive learning has shown its capacity of improving speech quality and speech intelligibility when it is combined with deep neural network (DNN) and long short-term memory (LSTM) based monaural speech enhancement algorithms, especially in low signal-to-noise ratio (SNR) conditions. Nevertheless, due to a large number of parameters and highly computational complexity, it is hard to implement in current resource-limited micro-controllers and thus, it is important to significantly reduce both the amount of parameters and the computational load for practical applications. For this purpose, we propose a novel progressive learning framework with convolutional recurrent neural networks called PL-CRNN, which takes advantages of both convolutional neural networks and recurrent neural networks to drastically reduce the amount of parameters and simultaneously improve speech quality and speech intelligibility. Numerous experiments verify the effectiveness of proposed PL-CRNN model and indicate that it yields consistent better performance than the PL-DNN and PL-LSTM algorithms and also it gets results close even better than the CRNN in terms of various evaluation metrics. Compared with PL-DNN, PL-LSTM and state-of-the-art CRNN models, the proposed PL-CRNN algorithm can reduce the amount of parameters up to 77\%, 93\% and 93\%, respectively.

219.
TITLE: Audio Query-based Music Source Separation
AUTHORS: J.H. Lee, H.S. Choi, K. Li
YEAR: 2019
SOURCE: ISMIR 2019
ABSTRACT: In recent years, music source separation has been one of the most intensively studied research areas in music information retrieval. Improvements in deep learning lead to a big progress in music source separation performance. However, most of the previous studies are restricted to separating a few limited number of sources, such as vocals, drums, bass, and other. In this study, we propose a network for audio query-based music source separation that can explicitly encode the source information from a query signal regardless of the number and/or kind of target signals. The proposed method consists of a Query-net and a Separator: given a query and a mixture, the Query-net encodes the query into the latent space, and the Separator estimates masks conditioned by the latent vector, which is then applied to the mixture for separation. The Separator can also generate masks using the latent vector from the training samples, allowing separation in the absence of a query. We evaluate our method on the MUSDB18 dataset, and experimental results show that the proposed method can separate multiple sources with a single network. In addition, through further investigation of the latent space we demonstrate that our method can generate continuous outputs via latent vector interpolation.

220.
TITLE: Interleaved Multitask Learning for Audio Source Separation with Independent Databases
AUTHORS: C.S.J. Doire, O. Okubadejo
YEAR: 2019
SOURCE: arXiv
ABSTRACT: Deep Neural Network-based source separation methods usually train independent models to optimize for the separation of individual sources. Although this can lead to good performance for well-defined targets, it can also be computationally expensive. The multitask alternative of a single network jointly optimizing for all targets simultaneously usually requires the availability of all target sources for each input. This requirement hampers the ability to create large training databases. In this paper, we present a model that decomposes the learnable parameters into a shared parametric model (encoder) and independent components (decoders) specific to each source. We propose an interleaved training procedure that optimizes the sub-task decoders independently and thus does not require each sample to possess a ground truth for all of its composing sources. Experimental results on MUSDB18 with the proposed method show comparable performance to independently trained models, with less trainable parameters, more efficient inference, and an encoder transferable to future target objectives. The results also show that using the proposed interleaved training procedure leads to better Source-to-Interference energy ratios when compared to the simultaneous optimization of all training objectives, even when all composing sources are available.

221.
TITLE: WHAM!: Extending Speech Separation to Noisy Environments
AUTHORS: G. Wichern, et al.
YEAR: 2019
SOURCE: arXiv
ABSTRACT: Recent progress in separating the speech signals from multiple overlapping speakers using a single audio channel has brought us closer to solving the cocktail party problem. However, most studies in this area use a constrained problem setup, comparing performance when speakers overlap almost completely, at artificially low sampling rates, and with no external background noise. In this paper, we strive to move the field towards more realistic and challenging scenarios. To that end, we created the WSJ0 Hipster Ambient Mixtures (WHAM!) dataset, consisting of two speaker mixtures from the wsj0-2mix dataset combined with real ambient noise samples. The samples were collected in coffee shops, restaurants, and bars in the San Francisco Bay Area, and are made publicly available. We benchmark various speech separation architectures and objective functions to evaluate their robustness to noise. While separation performance decreases as a result of noise, we still observe substantial gains relative to the noisy signals for most approaches.

222.
TITLE: Neural Network Based Phase Compensation Methods on Monaural Speech Separation
AUTHORS: C. Wang, J. Zhu
YEAR: 2019
SOURCE: 2019 IEEE International Conference on Multimedia and Expo
ABSTRACT: Monaural speech separation is a very challenging task, for the limitation of information from only a single channel. After Deep Learning is introduced into the monaural speech separation, researchers have made great improvement on the separation performance with the progress of different network structure. Among various of separation methods, especially methods with masking-based training targets, the influence of phase were always neglected during the separation process. In this paper, the impact of phase on monaural speech separation is discussed and proved using theoretical explanations and examples. New training targets in complement of existing magnitude training targets were trained through neural network methods to compensate for phase of target in order to achieve better separation performance. Comparisons and evaluations show that using phase compensation in separation boosts separation effect to a certain degree.

223.
TITLE: A Speaker-Dependent Approach to Separation of Far-Field Multi-Talker Microphone Array Speech for Front-End Processing in the CHiME-5 Challenge
AUTHORS: L. Sun, J. Du, Y. Fang, F. Ma, C. Lee
YEAR: 2019
SOURCE: IEEE Journal of Selected Topics in Signal Processing
ABSTRACT: We propose a novel speaker-dependent speech separation framework for the challenging CHiME-5 acoustic environments, exploiting advantages of both deep learning based and conventional preprocessing techniques to prepare data effectively for separating target speech from multi-talker mixed speech collected with multiple microphone arrays. First, a series of multi-channel operations is conducted to reduce existing reverberation and noise, and a single-channel deep learning based speech enhancement model is used to predict speech presence probabilities. Next, a two-stage supervised speech separation approach, using oracle speaker diarization information from CHiME-5, is proposed to separate speech of a target speaker from interference speakers in mixed speech. Given a set of three estimated masks of the background noise, the target speaker and the interference speakers from single-channel speech enhancement and separation models, a complex Gaussian mixture model based generalized eigenvalue beamformer is then used for enhancing the signal at the reference array while avoiding the speaker permutation issue. Furthermore, the proposed front-end can generate a large variety of processed data for an ensemble of speech recognition results. Experiments on the development set have shown that the proposed two-stage approach can yield significant improvements of recognition performance over the official baseline system and achieved top accuracies in all four competing evaluation categories among all systems submitted to the CHiME-5 Challenge.

224.
TITLE: A Joint Learning Algorithm for Complex-Valued T-F Masks in Deep Learning-Based Single-Channel Speech Enhancement Systems
AUTHORS:  J. Lee, H.G. Kang
YEAR: 2019
SOURCE: IEEE/ACM Transactions on Audio, Speech, and Language Processing
ABSTRACT: This paper presents a joint learning algorithm for complex-valued time-frequency (T-F) masks in single-channel speech enhancement systems. Most speech enhancement algorithms operating in a single-channel microphone environment aim to enhance the magnitude component in a T-F domain, while the input noisy phase component is used directly without any processing. Consequently, the mismatch between the processed magnitude and the unprocessed phase degrades the sound quality. To address this issue, a learning method of targeting a T-F mask that is defined in a complex domain has recently been proposed. However, due to a wide dynamic range and an irregular spectrogram pattern of the complex-valued T-F mask, the learning process is difficult even with a large-scale deep learning network. Moreover, the learning process targeting the T-F mask itself does not directly minimize the distortion in spectra or time domains. In order to address these concerns, we focus on three issues: 1) an effective estimation of complex numbers with a wide dynamic range; 2) a learning method that is directly related to speech enhancement performance; and 3) a way to resolve the mismatch between the estimated magnitude and phase spectra. In this study, we propose objective functions that can solve each of these issues and train the network by minimizing them with a joint learning framework. The evaluation results demonstrate that the proposed learning algorithm achieves significant performance improvement in various objective measures and subjective preference listening test.

225.
TITLE: Deep-Learning-Based Audio-Visual Speech Enhancement in Presence of Lombard Effect
AUTHORS: D. Michelsanti, Z. Tan, S. Sigurdsson, J. Jensen
YEAR: 2019
SOURCE: Speech Communication
ABSTRACT: When speaking in presence of background noise, humans reflexively change their way of speaking in order to improve the intelligibility of their speech. This reflex is known as Lombard effect. Collecting speech in Lombard conditions is usually hard and costly. For this reason, speech enhancement systems are generally trained and evaluated on speech recorded in quiet to which noise is artificially added. Since these systems are often used in situations where Lombard speech occurs, in this work we perform an analysis of the impact that Lombard effect has on audio, visual and audio-visual speech enhancement, focusing on deep-learning-based systems, since they represent the current state of the art in the field. We conduct several experiments using an audio-visual Lombard speech corpus consisting of utterances spoken by 54 different talkers. The results show that training deep-learning-based models with Lombard speech is beneficial in terms of both estimated speech quality and estimated speech intelligibility at low signal to noise ratios, where the visual modality can play an important role in acoustically challenging situations. We also find that a performance difference between genders exists due to the distinct Lombard speech exhibited by males and females, and we analyse it in relation with acoustic and visual features. Furthermore, listening tests conducted with audio-visual stimuli show that the speech quality of the signals processed with systems trained using Lombard speech is statistically significantly better than the one obtained using systems trained with non-Lombard speech at a signal to noise ratio of -5 dB. Regarding speech intelligibility, we find a general tendency of the benefit in training the systems with Lombard speech.

226.
TITLE: A Two-stage Single-channel Speaker-dependent Speech Separation Approach for Chime-5 Challenge
AUTHORS: L. Sun, et al.
YEAR: 2019
SOURCE: 2019 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: In this paper, we design a two-stage single-channel speaker-dependent speech separation approach for the CHiME-5 Challenge, targeting the problem of far-field and multi-talker conversational speech recognition in dinner party scenarios involving background noises, reverberations and overlapping speech. First, we make detailed analysis of the CHiME-5 data and observe problems of inaccurate human annotations and low-resource useable data for target speakers. Motivated by this, we conduct a first-stage speaker-dependent speech separation with a learning target for aggressive segregation to generate more and purer target speech data. Then a second-stage speaker-dependent speech separation with a new learning target is performed to obtain the final speech masks, which can be directly fed to back-end acoustic model. Compared with the official baseline, our proposed approach can yield an absolute word error rate reduction of 5.3%, namely from 81.3% to 76.0% in development test set. To the best of our knowledge, it is the first time to discuss a feasible method of single-channel speaker-dependent speech separation for such a challenging task although we make an assumption of oracle speaker diarization following the challenge rules. By integrating this crucial technique, our submitted systems achieved the first place of all four tasks in the CHiME-5 challenge.

227.
TITLE: The Phasebook: Building Complex Masks via Discrete Representations for Source Separation
AUTHORS: J. Le Roux, G. Wichern, S. Watanabe, A.M. Sarrroff, J. Hershey
YEAR: 2019
SOURCE: 2019 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: Deep learning based speech enhancement and source separation systems have recently reached unprecedented levels of quality, to the point that performance is reaching a new ceiling. Most systems rely on estimating the magnitude of a target source, either directly or by computing a real-valued mask to be applied to a time-frequency representation of the mixture signal. A limiting factor in such approaches is a lack of phase estimation: the phase of the mixture is most often used when reconstructing the estimated time-domain signal. We propose to estimate phase using "phasebook", a new type of layer based on a discrete representation of the phase difference between the mixture and the target. We also introduce "combook", a similar type of layer that directly estimates a complex mask. We present various training and inference schemes involving these representations, and explain in particular how to include them in an end-to-end learning framework. We also present an oracle study to assess upper bounds on performance for various types of masks using discrete phase representations. We evaluate the proposed methods on the wsj0-2mix dataset, a well-studied corpus for single-channel speaker-independent speaker separation, matching the performance of state-of-the-art mask-based approaches without requiring additional phase reconstruction steps.

228.
TITLE: Supervised Speech Enhancement with Real Spectrum Approximation
AUTHORS: Y. Liu, H. Zhang, X. Zhang, L. Yang
YEAR: 2019
SOURCE: 2019 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: Speech enhancement aims to separate a target speech from background noise. Recently, speech enhancement has been formulated as a supervised learning problem, in which a learning machine is trained to estimate the target spectrum denoted as mapping-based method or a time-frequency mask denoted as masking-based method. Signal approximation methods indirectly estimate the target spectrum via the mask estimation, which combines the advantages of both mapping based and masking based methods. Moreover, conventional methods usually ignore the phase which is also important to the speech quality. To consider the phase, the complex number spectrum needs to be modeled. However, modeling may be difficult. In this work, a pure real number spectrum is used as an alternative representation of the complex number spectrum, and a signal approximation method is used for speech enhancement. Experimental results show that the proposed method outperforms other commonly used methods.

229.
TITLE: Incremental Binarization on Recurrent Neural Networks for Single-channel Source Separation
AUTHORS: S. Kim, M. Maity, M. Kim
YEAR: 2019
SOURCE: 2019 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: This paper proposes a Bitwise Gated Recurrent Unit (BGRU) network for the single-channel source separation task. Recurrent Neural Networks (RNN) require several sets of weights within its cells, which significantly increases the computational cost compared to the fully-connected networks. To mitigate this increased computation, we focus on the GRU cells and quantize the feedforward procedure with binarized values and bitwise operations. The BGRU network is trained in two stages. The real-valued weights are pretrained and transferred to the bitwise network, which are then incrementally binarized to minimize the potential loss that can occur from a sudden introduction of quantization. As the proposed binarization technique turns only a few randomly chosen parameters into their binary versions, it gives the network training procedure a chance to gently adapt to the partly quantized version of the network. It eventually achieves the full binarization by incrementally increasing the amount of binarization over the iterations. Our experiments show that the proposed BGRU method produces source separation results greater than that of a real-valued fully connected network, with 11-12 dB mean Signal-to-Distortion Ratio (SDR). A fully binarized BGRU still outperforms a Bitwise Neural Network (BNN) by 1-2 dB even with less number of layers.

230.
TITLE: Online Singing Voice Separation Using a Recurrent One-dimensional U-NET Trained with Deep Feature Losses
AUTHORS: C.S.J. Doire
YEAR: 2019
SOURCE: 2019 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: This paper proposes an online approach to the singing voice separation problem. Based on a combination of one-dimensional convolutional layers along the frequency axis and recurrent layers to enforce temporal coherency, state-of-the-art performance is achieved. The concept of using deep features in the loss function to guide training and improve the model’s performance is also investigated.

231.
TITLE: Universal Sound Separation
AUTHORS: I. Kavalerov, et al.
YEAR: 2019
SOURCE: 2019 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics
ABSTRACT: Recent deep learning approaches have achieved impressive performance on speech enhancement and separation tasks. However, these approaches have not been investigated for separating mixtures of arbitrary sounds of different types, a task we refer to as universal sound separation, and it is unknown how performance on speech tasks carries over to non-speech tasks. To study this question, we develop a dataset of mixtures containing arbitrary sounds, and use it to investigate the space of mask-based separation architectures, varying both the overall network architecture and the framewise analysis-synthesis basis for signal transformations. These network architectures include convolutional long short-term memory networks and time-dilated convolution stacks inspired by the recent success of time-domain enhancement networks like ConvTasNet. For the latter architecture, we also propose novel modifications that further improve separation performance. In terms of the frame-wise analysis-synthesis basis, we explore both a short-time Fourier transform (STFT) and a learnable basis, as used in ConvTasNet. For both of these bases, we also examine the effect of window size. In particular, for STFTs, we find that longer windows (25-50 ms) work best for speech/non-speech separation, while shorter windows (2.5 ms) work best for arbitrary sounds. For learnable bases, shorter windows (2.5 ms) work best on all tasks. Surprisingly, for universal sound separation, STFTs outperform learnable bases. Our best methods produce an improvement in scale-invariant signal-to-distortion ratio of over 13 dB for speech/non-speech separation and close to 10 dB for universal sound separation.

232.
TITLE: Augmented Time-frequency Mask Estimation in Cluster-based Source Separation Algorithms
AUTHORS: Y. Luo, N. Mesgarani
YEAR: 2019
SOURCE: 2019 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: Time-frequency mask estimation with various clustering approaches has proven effective in solving the audio source separation problem. In this framework, the time-frequency bins of the mixture spectrogram are represented in a high-dimensional embedding space, where various methods can be applied to group the embedded points to calculate either hard or soft source assignments and subsequently the time-frequency masks. However, the mismatch between the assignment algorithm during the training and inference phases in majority of the current approaches leads to a suboptimal solution, because the assignment objective that is used during the training (e.g. ideal binary mask) is not the same as the one used during the inference phase (e.g. k-means clustering). We propose a method to reduce the mismatch between these two conditions where the source embedding is trained such that the source assignment during training and inference phases results in similar outcomes. Our results show that matching the source assignment during training- and inference-phase results in more accurate and consistent mask estimation in the inference phase which significantly improves the source separation accuracy for various hard and soft clustering methods.

233.
TITLE: A Statistically Principled and Computationally Efficient Approach to Speech Enhancement using Variational Autoencoders
AUTHORS: M. Pariente, A. Deleforge, E. Vincent
YEAR: 2019
SOURCE: arXiv
ABSTRACT: Recent studies have explored the use of deep generative models of speech spectra based of variational autoencoders (VAEs), combined with unsupervised noise models, to perform speech enhancement. These studies developed iterative algorithms involving either Gibbs sampling or gradient descent at each step, making them computationally expensive. This paper proposes a variational inference method to iteratively estimate the power spectrogram of the clean speech. Our main contribution is the analytical derivation of the variational steps in which the en-coder of the pre-learned VAE can be used to estimate the varia-tional approximation of the true posterior distribution, using the very same assumption made to train VAEs. Experiments show that the proposed method produces results on par with the afore-mentioned iterative methods using sampling, while decreasing the computational cost by a factor 36 to reach a given performance.

234.
TITLE: Monaural Source Separation in Complex Domain With Long Short-Term Memory Neural Network
AUTHORS: Y. Sun, Y. Xian, W. Wang, S.M. Naqvi
YEAR: 2019
SOURCE: IEEE Journal of Selected Topics in Signal Processing
ABSTRACT: In recent research, deep neural network (DNN) has been used to solve the monaural source separation problem. According to the training objectives, DNN-based monaural speech separation is categorized into three aspects, namely masking, mapping, and signal approximation based techniques. However, the performance of the traditional methods is not robust due to variations in real-world environments. Besides, in the vanilla DNN-based methods, the temporal information cannot be fully utilized. Therefore, in this paper, the long short-term memory (LSTM) neural network is applied to exploit the long-term speech contexts. Then, we propose the complex signal approximation (cSA), which is operated in the complex domain to utilize the phase information of the desired speech signal to improve the separation performance. The IEEE and the TIMIT corpora are used to generate mixtures with noise and speech interferences to evaluate the efficacy of the proposed method. The experimental results demonstrate the advantages of the proposed cSA-based LSTM recurrent neural network method in terms of different objective performance measures.

235.
TITLE: Data-driven Design of Perfect Reconstruction Filterbank for DNN-based Sound Source Enhancement
AUTHORS: D. Takeuchi, K. Yatabe, Y. Koizumi, Y. Oikawa, N. Harada
YEAR: 2019
SOURCE: 2019 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: We propose a data-driven design method of perfect-reconstruction filterbank (PRFB) for sound-source enhancement (SSE) based on deep neural network (DNN). DNNs have been used to estimate a time-frequency (T-F) mask in the short-time Fourier transform (STFT) domain. Their training is more stable when a simple cost function as mean-squared error (MSE) is utilized comparing to some advanced cost such as objective sound quality assessments. However, such a simple cost function inherits strong assumptions on the statistics of the target and/or noise which is often not satisfied, and the mismatch of assumption results in degraded performance. In this paper, we propose to design the frequency scale of PRFB from training data so that the assumption on MSE is satisfied. For designing the frequency scale, the warped filterbank frame (WFBF) is considered as PRFB. The frequency characteristic of learned WFBF was in between STFT and the wavelet transform, and its effectiveness was confirmed by comparison with a standard STFT-based DNN whose input feature is compressed into the mel scale.

236.
TITLE: End-to-End Multi-Task Denoising for joint SDR and PESQ Optimization
AUTHORS: J. Kim, M. El-Khamy, J. Lee
YEAR: 2019
SOURCE: arXiv
ABSTRACT: Supervised learning based on a deep neural network recently has achieved substantial improvement on speech enhancement. Denoising networks learn mapping from noisy speech to clean one directly, or to a spectrum mask which is the ratio between clean and noisy spectra. In either case, the network is optimized by minimizing mean square error (MSE) between ground-truth labels and time-domain or spectrum output. However, existing schemes have either of two critical issues: spectrum and metric mismatches. The spectrum mismatch is a well known issue that any spectrum modification after short-time Fourier transform (STFT), in general, cannot be fully recovered after inverse short-time Fourier transform (ISTFT). The metric mismatch is that a conventional MSE metric is sub-optimal to maximize our target metrics, signal-to-distortion ratio (SDR) and perceptual evaluation of speech quality (PESQ). This paper presents a new end-to-end denoising framework with the goal of joint SDR and PESQ optimization. First, the network optimization is performed on the time-domain signals after ISTFT to avoid spectrum mismatch. Second, two loss functions which have improved correlations with SDR and PESQ metrics are proposed to minimize metric mismatch. The experimental result showed that the proposed denoising scheme significantly improved both SDR and PESQ performance over the existing methods.

237.
TITLE: On Training Targets and Objective Functions for Deep-Learning-Based Audio-Visual Speech Enhancement
AUTHORS: D. Michelsanti, Z. Tan, S. Sigurdsson, J. Jensen
YEAR: 2019
SOURCE: 2019 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: Audio-visual speech enhancement (AV-SE) is the task of improving speech quality and intelligibility in a noisy environment using audio and visual information from a talker. Recently, deep learning techniques have been adopted to solve the AV-SE task in a supervised manner. In this context, the choice of the target, i.e. the quantity to be estimated, and the objective function, which quantifies the quality of this estimate, to be used for training is critical for the performance. This work is the first that presents an experimental study of a range of different targets and objective functions used to train a deep-learning-based AV-SE system. The results show that the approaches that directly estimate a mask perform the best overall in terms of estimated speech quality and intelligibility, although the model that directly estimates the log magnitude spectrum performs as good in terms of estimated speech quality.

238.
TITLE: SDR - Half-Baked or Well Done?
AUTHORS: J. Le Roux, S. Wisdom, J. Hershey
YEAR: 2019
SOURCE: 2019 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: In speech enhancement and source separation, signal-to-noise ratio is a ubiquitous objective measure of denoising/separation quality. A decade ago, the BSS_eval toolkit was developed to give researchers worldwide a way to evaluate the quality of their algorithms in a simple, fair, and hopefully insightful way: it attempted to account for channel variations, and to not only evaluate the total distortion in the estimated signal but also split it in terms of various factors such as remaining interference, newly added artifacts, and channel errors. In recent years, hundreds of papers have been relying on this toolkit to evaluate their proposed methods and compare them to previous works, often arguing that differences on the order of 0.1 dB proved the effectiveness of a method over others. We argue here that the signal-to-distortion ratio (SDR) implemented in the BSS_eval toolkit has generally been improperly used and abused, especially in the case of single-channel separation, resulting in misleading results. We propose to use a slightly modified definition, resulting in a simpler, more robust measure, called scale-invariant SDR (SI-SDR). We present various examples of critical failure of the original SDR that SI-SDR overcomes.

239.
TITLE: Concatenated Identical DNN (CI-DNN) to Reduce Noise-Type Dependence in DNN-Based Speech Enhancement
AUTHORS: Z. Xu, M. Strake, T. Fingscheidt
YEAR: 2019
SOURCE: 2019 27th European Signal Processing Conference
ABSTRACT: Estimating time-frequency domain masks for speech enhancement using deep learning approaches has recently become a popular field of research. In this paper, we propose a mask-based speech enhancement framework by using concatenated identical deep neural networks (CI-DNNs). The idea is that a single DNN is trained under multiple input and output signal-to-noise power ratio (SNR) conditions, using targets that provide a moderate SNR gain with respect to the input and therefore achieve a balance between speech component quality and noise suppression. We concatenate this single DNN several times without any retraining to provide enough noise attenuation. Simulation results show that our proposed CI-DNN outperforms enhancement methods using classical spectral weighting rules w.r.t. total speech quality and speech intelligibility. Moreover, our approach shows similar or even a little bit better performance with much fewer trainable parameters compared with a noisy-target single DNN approach of the same size. A comparison to the conventional clean-target single DNN approach shows that our proposed CI-DNN is better in speech component quality and much better in residual noise component quality. Most importantly, our new CI-DNN generalized best to an unseen noise type, if compared to the other tested deep learning approaches.

240.
TITLE: Phasebook and Friends: Leveraging Discrete Representations for Source Separation
AUTHORS: J. Le Roux, G. Wichern, S. Watanabe, A.M. Sarroff, J. Hershey
YEAR: 2019
SOURCE: IEEE Journal of Selected Topics in Signal Processing
ABSTRACT: Speech enhancement and source separation systems based on deep learning have recently reached unprecedented levels of quality, to the point that performance is reaching a new ceiling. Most systems rely on estimating the magnitude of a target source by estimating a real-valued mask to be applied to a time-frequency representation of the mixture signal. A limiting factor in such approaches is a lack of phase estimation: the phase of the mixture is most often used when reconstructing the estimated time-domain signal. Here, we propose “magbook,” “phasebook,” and “combook,” three new types of layers based on discrete representations that can be used to estimate complex time-frequency masks. Magbook layers extend classical sigmoidal units and a recently introduced convex softmax activation for mask-based magnitude estimation. Phasebook layers use a similar structure to give an estimate of the phase mask without suffering from phase wrapping issues. Combook layers are an alternative to the magbook–phasebook combination that directly estimate complex masks. We present various training and inference schemes involving these representations, and explain in particular how to include them in an end-to-end learning framework. We also present an oracle study to assess upper bounds on performance for various types of masks using discrete phase representations. We evaluate the proposed methods on the wsj0-2mix dataset, a well-studied corpus for single-channel speaker-independent speaker separation, matching the performance of state-of-the-art mask-based approaches without requiring additional phase reconstruction steps.

241.
TITLE: Speech Denoising with Deep Feature Losses
AUTHORS: F. Germain, Q. Chen, V. Koltun
YEAR: 2019
SOURCE: arXiv
ABSTRACT: We present an end-to-end deep learning approach to denoising speech signals by processing the raw waveform directly. Given input audio containing speech corrupted by an additive background signal, the system aims to produce a processed signal that contains only the speech content. Recent approaches have shown promising results using various deep network architectures. In this paper, we propose to train a fully-convolutional context aggregation network using a deep feature loss. That loss is based on comparing the internal feature activations in a different network, trained for acoustic environment detection and domestic audio tagging. Our approach outperforms the state-of-the-art in objective speech quality metrics and in large-scale perceptual experiments with human listeners. It also outperforms an identical network trained using traditional regression losses. The advantage of the new approach is particularly pronounced for the hardest data with the most intrusive background noise, for which denoising is most needed and most challenging.

242.
TITLE: Two-Stage Deep Learning for Noisy-Reverberant Speech Enhancement
AUTHORS: Y. Zhao, Z. Wang, D. Wang
YEAR: 2019
SOURCE: IEEE/ACM Transactions on Audio, Speech, and Language Processing
ABSTRACT: In real-world situations, speech reaching our ears is commonly corrupted by both room reverberation and background noise. These distortions are detrimental to speech intelligibility and quality, and also pose a serious problem to many speech-related applications, including automatic speech and speaker recognition. In order to deal with the combined effects of noise and reverberation, we propose a two-stage strategy to enhance corrupted speech, where denoising and dereverberation are conducted sequentially using deep neural networks. In addition, we design a new objective function that incorporates clean phase during model training to better estimate spectral magnitudes, which would in turn yield better phase estimates when combined with iterative phase reconstruction. The two-stage model is then jointly trained to optimize the proposed objective function. Systematic evaluations and comparisons show that the proposed algorithm improves objective metrics of speech intelligibility and quality substantially, and significantly outperforms previous one-stage enhancement systems.

243.
TITLE: Enhanced Feature Network for Monaural Singing Voice Separation
AUTHORS: W. Yuan, B. He, S. Wang, J. Wang, M. Unoki
YEAR: 2019
SOURCE: Speech Communication
ABSTRACT: Abstract Deep Recurrent Neural Network (DRNN) based monaural singing voice separation (MSVS) methods have recently obtained impressive separation results. Most of DRNN based methods directly take the magnitude spectra of the mixture signal as the input feature, which has high dimensionality and contains redundant information. The DRNN based models, however, cannot extract the effective low-dimensional and de-redundant representations from the magnitude spectra. In this paper, we propose an Enhanced Feature Network (EFN) to extract effective representations of the magnitude spectra, i.e., enhanced-feature, for MSVS. The generation of enhanced-feature includes two consecutive stages: (i) modeling the local and contextual information explicitly by Convolutional Neural Network (CNN); (ii) extracting the high-level sequential feature by Highway Network and bi-directional Recurrent Neural Network (RNN). In the first stage, the EFN generates an enhanced-sequence consisting of the high-resolution magnitude spectra and its low-dimensional representations, where the low-dimensional part avoids the high cost of spectra decomposition and the high-resolution part mitigates problems of information loss. In the second stage, the enhanced-sequence is used to extract the enhanced-feature which are more suitable for MSVS. Experiments on the MIR-1K dataset have shown that the enhanced-feature can be used to obtain better separation effects than the magnitude spectra or its low-dimensional representations. The proposed method obtains 0.16–0.31 dB GNSDR gain and 0.48–0.71 dB GSAR gain, as compared with the previously proposed DRNN based methods. Moreover, the separation module of EFN which adopts only one hidden layer of GRU RNN can increase the training speed obviously.

244.
TITLE: Phase-Aware Speech Enhancement Based on Deep Neural Networks
AUTHORS: N. Zheng, X. Zhang
YEAR: 2019
SOURCE: IEEE/ACM Transactions on Audio, Speech, and Language Processing
ABSTRACT: Short-time frequency transform (STFT) is fundamental in speech processing. Because of the difficulty of processing highly unstructured STFT phase, most speech-processing algorithms only operate with STFT magnitude, leaving the STFT phase far from explored. However, with the recent development of deep neural network (DNN) based speech processing, e.g., speech enhancement and recognition, phase processing is becoming more important than ever before as a new growing point of DNN-based methods. In this paper, we propose a phase-aware speech enhancement algorithm based on DNN. Specifically, in the training stage, when incorporating phase as a target, our core idea is to transform an unstructured phase spectrogram to its derivative along the time axis, i.e., instantaneous frequency deviation (IFD), which has a similar structure with its corresponding magnitude spectrogram. We further propose to optimize both IFD and magnitude jointly in a multiobjective learning framework. In the test stage, we propose a postprocessing method to recover the phase spectrogram from the estimated IFD. Experimental results demonstrate the effectiveness of the proposed method.

245.
TITLE: Role of Deep Neural Network in Speech Enhancement: A Review
AUTHORS: D. Hepsiba, J. Justin
YEAR: 2018
SOURCE: Communications in Computer and Information Science
ABSTRACT: This paper presents a review on different methodologies adopted in speech enhancement and the role of Deep Neural Networks (DNN) in enhancement of speech. Mostly, a speech signal is distorted by background noise, environmental noise and reverberations. To enhance speech, certain processing techniques like Short-Time Fourier Transform, Short-time Autocorrelation and Short-time energy can be adopted. Features such as Logarithmic Power Spectrum (LPS), Mel-Frequency Cepstral Coefficients (MFCC) and Gammatone Frequency Cepstral Coefficient (GFCC) can be extracted and given to DNN for noise classification, so that the noise in the speech can be eliminated. DNN plays a major role in speech enhancement by creating a model with a large amount of training data and the performance of the enhanced speech is evaluated using certain performance metrics.

246.
TITLE: Multi-Channel Overlapped Speech Recognition with Location Guided Speech Extraction Network
AUTHORS: Z. Chen, X. Xiao, T. Yoshioka, H. Erdogan, J. Li, Y. Gong
YEAR: 2018
SOURCE: 2018 IEEE Spoken Language Technology Workshop
ABSTRACT: Although advances in close-talk speech recognition have resulted in relatively low error rates, the recognition performance in far-field environments is still limited due to low signal-to-noise ratio, reverberation, and overlapped speech from simultaneous speakers which is especially more difficult. To solve these problems, beamforming and speech separation networks were previously proposed. However, they tend to suffer from leakage of interfering speech or limited generalizability. In this work, we propose a simple yet effective method for multi-channel far-field overlapped speech recognition. In the proposed system, three different features are formed for each target speaker, namely, spectral, spatial, and angle features. Then a neural network is trained using all features with a target of the clean speech of the required speaker. An iterative update procedure is proposed in which the mask-based beamforming and mask estimation are performed alternatively. The proposed system were evaluated with real recorded meetings with different levels of overlapping ratios. The results show that the proposed system achieves more than 24% relative word error rate (WER) reduction than fixed beamforming with oracle selection. Moreover, as overlap ratio rises from 20% to 70+%, only 3.8% WER increase is observed for the proposed system.

249.
TITLE: Online LSTM-based Iterative Mask Estimation for Multi-Channel Speech Enhancement and ASR
AUTHORS: Y. Tu, J. Du, N. Zhou, C.H. Lee
YEAR: 2018
SOURCE: 2018 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference
ABSTRACT: Accurate steering vector estimation is the key point for a beamformer which suppresses the background noise to improve the noisy speech quality and intelligibility. Recently, time-frequency masking approach, which estimates the steering vectors that are utilized for a beamformer, is popular in this field. In particular, we have proposed an iterative mask estimation (IME) approach to improve the complex Gaussian mixture model (CGMM) based beamforming and yield the best system for multi-channel ASR in CHiME-4 challenge [1]. And in [2], we also demonstrated that our algorithm could improve the speech quality (PESQ) and intelligibility (STOI) for multi-channel speech enhancement. In this study, we focus on the online processing of our IME algorithm for multi-channel speech enhancement and ASR, which achieves comparable performance to the offline version. In addition, a regression long short-term memory recurrent neural network (LSTM-RNN) for a multiple-target joint learning is utilized, denoted as LSTM-MT, to replace two separate models in [2]. Experiments on the CHiME-4 simulation data show that the online IME algorithm can improve the enhancement performance, e.g., PESQ from 2.18 to 2.58 and STOI from 86.85 to 94.51, which is comparable to those obtained by offline IME. Furthermore, the LSTM-MT based post-processing can achieve an additional PESQ improvement from 2.58 to 2.71. And experiments on the CHiME-4 real data show that the online IME approach outperforms the online CGMM-based approach, with a relative word error reduction (WER) of 14.49%.

250.
TITLE: Deep Learning Based Speech Separation via NMF-Style Reconstructions
AUTHORS: S. Nie, S. Liang, W. Liu, X. Zhang, J. Tao
YEAR: 2018
SOURCE: IEEE/ACM Transactions on Audio, Speech, and Language Processing
ABSTRACT: Deep learning based speech separation usually uses a supervised algorithm to learn a mapping function from noisy features to separation targets. These separation targets, either ideal masks or magnitude spectrograms, have prominent spectro-temporal structures. Nonnegative matrix factorization (NMF) is a well-known representation learning technique that is capable of capturing the basic spectral structures. Therefore, the combination of deep learning and NMF as an organic whole is a smart strategy. However, previous methods typically use deep neural networks (DNN) and NMF for speech separation in a separate manner. In this paper, we propose a jointly combinatorial scheme to concentrate the strengths of both DNN and NMF for speech separation. NMF is used to learn the basis spectra that then are integrated into a DNN to directly reconstruct the magnitude spectrograms of speech and noise. Instead of predicting activation coefficients inferred by NMF, which is used as an intermediate target by the previous methods, DNN directly optimizes an actual separation objective in our system, so that the accumulated errors could be alleviated. Moreover, we explore a discriminative training objective with sparsity constraints to suppress noise and preserve more speech components further. Systematic experiments show that the proposed models are competitive with the previous methods.

251.
TITLE: A Progressive Deep Learning Approach to Child Speech Separation
AUTHORS: X. Wang, J. Du, L. Sun, Q. Wang, C. Lee
YEAR: 2018
SOURCE: 2018 11th International Symposium on Chinese Spoken Language Processing
ABSTRACT: We propose a progressive leaning approach to separating child speech from signals with mixed adult speech in a speaker-independent manner based on a densely connected long short-term memory (LSTM) architecture to deal with limited training data issue in child speech. First, by measuring the speech dissimilarities between children and adults using i-vectors, we demonstrate that distances between child and adult speech are large enough to warrant a possible separation through establishing child and adult speech groups. Accordingly, we present a novel LSTM design with densely connected hidden layers and stacked inputs containing progressively obtained intermediate targets that are learnt via multiple-target learning for speech separation between child and adult groups. Experimental results on a simulation corpus show that the proposed framework can yield consistent and significant gains of objective measures over the LSTM baseline for child speech separation. Further-more, our preliminary results on the SeedLing corpus with realistic recordings for child language acquisition show that our approach can achieve better overall separation performances than LSTM baseline when comparing spectrograms of separated speech, implying a potential for speaker diarization involving child speech.

252.
TITLE: Investigation of Monaural Front-End Processing for Robust ASR without Retraining or Joint-Training
AUTHORS: Z. Du, X. Zhang, J.Han
YEAR: 2018
SOURCE: arXiv
ABSTRACT: In recent years, monaural speech separation has been formulated as a supervised learning problem, which has been systematically researched and shown the dramatical improvement of speech intelligibility and quality for human listeners. However, it has not been well investigated whether the methods can be employed as the front-end processing and directly improve the performance of a machine listener, i.e., an automatic speech recognizer, without retraining or joint-training the acoustic model. In this paper, we explore the effectiveness of the independent front-end processing for the multi-conditional trained ASR on the CHiME-3 challenge. We find that directly feeding the enhanced features to ASR can make 36.40% and 11.78% relative WER reduction for the GMM-based and DNN-based ASR respectively. We also investigate the affect of noisy phase and generalization ability under unmatched noise condition.

253.
TITLE: Single-Channel Dereverberation Using Direct MMSE Optimization and Bidirectional LSTM Networks
AUTHORS: W. Mack, et al.
YEAR: 2018
SOURCE: Interspeech 2018
ABSTRACT: Dereverberation is useful in hands-free communication and voice controlled devices for distant speech acquisition. Single channel dereverberation can be achieved by applying a time frequency (TF) mask to the short-time Fourier transform (STFT) representation of a reverberant signal. Recent approaches have used deep neural networks (DNNs) to estimate such masks. Previously proposed DNN-based mask estimation methods train a DNN to minimize the mean-squared-error (MSE) between the desired and estimated masks. Recent TF mask estimation methods for signal separation directly minimize instead the MSE between the desired and estimated STFT magnitudes. We apply this direct optimization concept to dereverberation. Moreover, as reverberation exceeds the duration of a single STFT frame, we propose to use a bidirectional long short-term memory (LSTM) network which is able to take the relation between multiple STFT frames into account. We evaluated our method for different reverberation times and source-microphone distances using simulated as well as measured room impulse responses of different rooms. An evaluation of the proposed method and a comparison with a state-of-the-art method demonstrate the superiority of our approach and its robustness to different acoustic conditions.

254.
TITLE: Investigations on Data Augmentation and Loss Functions for Deep Learning Based Speech-Background Separation
AUTHORS: H. Erdogan, T. Yoshioka
YEAR: 2018
SOURCE: Interspeech 2018
ABSTRACT: A successful deep learning-based method for separation of a speech signal from an interfering background audio signal is based on neural network prediction of time-frequency masks which multiply noisy signal’s short-time Fourier transform (STFT) to yield the STFT of an enhanced signal. In this paper, we investigate training strategies for mask-prediction based speech-background separation systems. First, we examine the impact of mixing speech and noise files on the fly during training, which enables models to be trained on virtually infinite amount of data. We also investigate the effect of using a novel signal-to-noise ratio related loss function, instead of mean-squared error which is prone to scaling differences among utterances. We evaluate bi-directional long-short term memory (BLSTM) networks as well as a combination of convolutional and BLSTM (CNN+BLSTM) networks for mask prediction and compare performances of real and complex-valued mask prediction. Data-augmented training combined with a novel loss function yields significant improvements in signal to distortion ratio (SDR) and perceptual evaluation of speech quality (PESQ) as compared to the best published result on CHiME-2 medium vocabulary data set when using a CNN+BLSTM network.

255.
TITLE: Error Modeling via Asymmetric Laplace Distribution for Deep Neural Network Based Single-Channel Speech Enhancement
AUTHORS: L. Chai, J. Du, C. Lee
YEAR: 2018
SOURCE: Interspeech 2018
ABSTRACT: The minimum mean squared error (MMSE) as a conventional training criterion for deep neural network (DNN) based speech enhancement has been found many problems. In our recent work, a maximum likelihood (ML) approach to parameter learning by modeling the prediction error vector as a Gaussian density was proposed. In this study, our preliminary statistical analysis reveals the super-Gaussianity and asymmetricity of the prediction error distribution. Consequently, we adopt the asymmetric Laplace distribution (ALD) instead of the Gaussian distribution (GD) to model the prediction error vectors. Then the new derivation for optimizing the the proposed ML-ALD-DNN with both DNN and ALD parameters is presented. Moreover, we can well interpret the asymmetry parameter of ALD as the balance control between noise reduction and speech preservation from both formulations and experiments. This implies that the customization of DNN models for the different noise types and levels is possible by the setting of the asymmetry parameter. Finally, our ML-ALD-DNN approach achieves better STOI and SSNR measures over both MMSE-DNN and ML-GD-DNN approaches.

256.
TITLE: DNN-Based Near- and Far-Field Source Separation Using Spherical-Harmonic-Analysis-Based Acoustic Features
AUTHORS: S. Nishiguchi, Y. Koizumi, N. Harada, K. Itou
YEAR: 2018
SOURCE: 2018 16th International Workshop on Acoustic Signal Enhancement
ABSTRACT: We propose the combination of a physical-model-based and a deep-learning (DL)-based source separation for near- and far-field source separation. The DL-based near- and far-field source separation method uses spherical-harmonic-analysis-based acoustic features. Deep learning is a state-of-the-art technique for source separation. In this approach, a deep neural network (D NN) is used to predict a time-frequency (T-F) mask. To accurately predict a T-F mask, it is necessary to use acoustic features that have high mutual information with the oracle T-F mask. However, the effective acoustic features to separate near- and far-field sources are unknown. In this study, low-frequency-band near- and far-field sources are estimated based on spherical harmonic analysis and used as acoustic features. Subsequently, a DNN predicts a T-F mask to separate all frequency bands. Our experimental results show that the proposed method improved the signal-to-distortion-rate by 6–8 dB compared to the harmonic-analysis-based method.

257.
TITLE: Monaural Speech Separation Using a Phase-Aware Deep Denoising Auto Encoder 
AUTHORS: D.S. Williamson
YEAR: 2018
SOURCE: 2018 IEEE 28th International Workshop on Machine Learning for Signal Processing 
ABSTRACT: Traditional deep denoising autoencoders (DDAE) use magnitude domain features and training targets to separate speech from background noise. Phase enhancement, however, has recently been shown to improve perceptual and objective speech quality. We present an approach that uses a DDAE to estimate phase-aware training targets from phase-aware input features. This network is denoted as a phase-aware deep denoising autoencoder (paDDAE). The short-time Fourier transform (STFT) of noisy speech is the network input, and the network estimates a phase-aware time-frequency mask. The proposed approach is evaluated across multiple conditions, including various signal-to-noise ratios (SNRs), noise types, and speakers. The results show that the paDDAE offers improvements over traditional DDAEs in terms of objective speech quality and intelligibility.

258.
TITLE: Single-Microphone Speech Enhancement and Separation Using Deep Learning
AUTHORS: M. Kolbaek
YEAR: 2018
SOURCE: arXiv
ABSTRACT: The cocktail party problem comprises the challenging task of listening to and understanding a speech signal in a complex acoustic environment, where multiple speakers and background noise signals simultaneously interfere with the speech signal of interest. A signal processing algorithm that can effectively increase the speech intelligibility and quality of speech signals in such complicated acoustic situations is highly desirable. Especially for applications involving mobile communication devices and hearing assistive devices, increasing speech intelligibility and quality of noisy speech signals has been a goal for scientists and engineers for more than half a century. Due to the re-emergence of machine learning techniques, today, known as deep learning, the challenges involved with such algorithms might be overcome. In this PhD thesis, we study and develop deep learning-based techniques for two major sub-disciplines of the cocktail party problem: single-microphone speech enhancement and single-microphone multi-talker speech separation. Specifically, we conduct in-depth empirical analysis of the generalizability capability of modern deep learning-based single-microphone speech enhancement algorithms. We show that performance of such algorithms is closely linked to the training data, and good generalizability can be achieved with carefully designed training data. Furthermore, we propose utterancelevel Permutation Invariant Training (uPIT), a deep learning-based algorithm for single-microphone speech separation and we report state-of-the-art results on a speaker-independent multi-talker speech separation task. Additionally, we show that uPIT works well for joint speech separation and enhancement without explicit prior knowledge about the noise type or number of speakers, which, at the time of writing, is a capability only shown by uPIT. Finally, we show that deep learning-based speech enhancement algorithms designed to minimize the classical short-time spectral amplitude mean squared error leads to enhanced speech signals which are essentially optimal in terms of Short-Time Objective Intelligibility (STOI), a state-of-theart speech intelligibility estimator. This is important as it suggests that no additional improvements in STOI can be achieved by a deep learning-based speech enhancement algorithm, which is designed to maximize STOI.

259.
TITLE: Deep Neural Network Based Speech Separation Optimizing an Objective Estimator of Intelligibility for Low Latency Applications
AUTHORS: G. Naithani, J. Nikunen, L. Bramslow, T. Virtanen
YEAR: 2018
SOURCE: 2018 16th International Workshop on Acoustic Signal Enhancement
ABSTRACT: Mean square error (MSE) has been the preferred choice as loss function in the current deep neural network (DNN) based speech separation techniques. In this paper, we propose a new cost function with the aim of optimizing the extended short time objective intelligibility (ESTOI) measure. We focus on applications where low algorithmic latency (≤ 10 ms) is important. We use long short-term memory networks (LSTM) and evaluate our proposed approach on four sets of two-speaker mixtures from extended Danish hearing in noise (HINT) dataset. We show that the proposed loss function can offer improved or at par objective intelligibility (in terms of ESTOI) compared to an MSE optimized baseline while resulting in lower objective separation performance (in terms of the source to distortion ratio (SDR)). We then proceed to propose an approach where the network is first initialized with weights optimized for MSE criterion and then trained with the proposed ESTOI loss criterion. This approach mitigates some of the losses in objective separation performance while preserving the gains in objective intelligibility.

260.
TITLE: Listen and Look: Audio–Visual Matching Assisted Speech Source Separation
AUTHORS: R. Lu, Z. Duan, C. Zhang
YEAR: 2018
SOURCE: IEEE Signal Processing Letters
ABSTRACT: Source permutation, i.e., assigning separated signal snippets to wrong sources over time, is a major issue in the state-of-the-art speaker-independent speech source separation methods. In addition to auditory cues, humans also leverage visual cues to solve this problem at cocktail parties: matching lip movements with voice fluctuations helps humans to better pay attention to the speaker of interest. In this letter, we propose an audio–visual matching network to learn the correspondence between voice fluctuations and lip movements. We then propose a framework to apply this network to address the source permutation problem and improve over audio-only speech separation methods. The modular design of this framework makes it easy to apply the matching network to any audio-only speech separation method. Experiments on two-talker mixtures show that the proposed approach significantly improves the separation quality over the state-of-the-art audio-only method. This improvement is especially pronounced on mixtures that the audio-only method fails, in which the speakers often have similar voice characteristics.

261.
TITLE: Phase-Sensitive Joint Learning Algorithms for Deep Learning-Based Speech Enhancement
AUTHORS: J. Lee, J. Skoglund, T. Shabestary, H.G. Kang
YEAR: 2018
SOURCE: IEEE Signal Processing Letters
ABSTRACT: This letter presents a phase-sensitive joint learning algorithm for single-channel speech enhancement. Although a deep learning framework that estimates the time-frequency (T-F) domain ideal ratio masks demonstrates a strong performance, it is limited in the sense that the enhancement process is performed only in the magnitude domain, while the phase spectra remain unchanged. Thus, recent studies have been conducted to involve phase spectra in speech enhancement systems. A phase-sensitive mask (PSM) is a T-F mask that implicitly represents phase-related information. However, since the PSM has an unbounded value, the networks are trained to target its truncated values rather than directly estimating it. To effectively train the PSM, we first approximate it to have a bounded dynamic range under the assumption that speech and noise are uncorrelated. We then propose a joint learning algorithm that trains the approximated value through its parameterized variables in order to minimize the inevitable error caused by the truncation process. Specifically, we design a network that explicitly targets three parameterized variables: 1) speech magnitude spectra; 2) noise magnitude spectra; and 3) phase difference of clean to noisy spectra. To further improve the performance, we also investigate how the dynamic range of magnitude spectra controlled by a warping function affects the final performance in joint learning algorithms. Finally, we examined how the proposed additional constraint that preserves the sum of the estimated speech and noise power spectra affects the overall system performance. The experimental results show that the proposed learning algorithm outperforms the conventional learning algorithm with the truncated phase-sensitive approximation.

262.
TITLE: Towards Automated Single Channel Source Separation using Neural Networks
AUTHORS: A. Gang, P. Biyani, A. Soni
YEAR: 2018
SOURCE: Interspeech 2018
ABSTRACT: Many applications of single channel source separation (SCSS) including automatic speech recognition (ASR), hearing aids etc. require an estimation of only one source from a mixture of many sources. Treating this special case as a regular SCSS problem where in all constituent sources are given equal priority in terms of reconstruction may result in a suboptimal separation performance. In this paper, we tackle the one source separation problem by suitably modifying the orthodox SCSS framework and focus only on one source at a time. The proposed approach is a generic framework that can be applied to any existing SCSS algorithm, improves performance, and scales well when there are more than two sources in the mixture unlike most existing SCSS methods. Additionally, existing SCSS algorithms rely on fine hyper-parameter tuning hence making them difficult to use in practice. Our framework takes a step towards automatic tuning of the hyper-parameters thereby making our method better suited for the mixture to be separated and thus practically more useful. We test our framework on a neural network based algorithm and the results show an improved performance in terms of SDR and SAR.

263.
TITLE: Multi-View Networks for Denoising of Arbitrary Numbers of Channels
AUTHORS: H. Casebeer, B. Luc, P. Smaragdis
YEAR: 2018
SOURCE: 2018 16th International Workshop on Acoustic Signal Enhancement
ABSTRACT: We propose a set of denoising neural networks capable of operating on an arbitrary number of channels at runtime, irrespective of how many channels they were trained on. We coin the proposed models multi-view networks since they operate using multiple views of the same data. We explore two such architectures and show how they outperform traditional denoising models in multi-channel scenarios. Additionally, we demonstrate how multi-view networks can leverage information provided by additional recordings to make better predictions, and how they are able to generalize to a number of recordings not seen in training.

264.
TITLE: DNN-Based Source Enhancement to Increase Objective Sound Quality Assessment Score
AUTHORS: Y. Koizumi, K. Niwa, Y. Hioka, K. Kobayashi, Y. Haneda
YEAR: 2018
SOURCE: Interspeech 2018
ABSTRACT: We propose a training method for deep neural network (DNN) based source enhancement to increase objective sound quality assessment (OSQA) scores such as the perceptual evaluation of speech quality. In many conventional studies, DNNs have been used as a mapping function to estimate time–frequency masks and trained to minimize an analytically tractable objective function such as the mean squared error (MSE). Since OSQA scores have been used widely for sound-quality evaluation, constructing DNNs to increase OSQA scores would be better than using the minimum MSE to create high-quality output signals. However, since most OSQA scores are not analytically tractable, i.e., they are black boxes, the gradient of the objective function cannot be calculated by simply applying backpropagation. To calculate the gradient of the OSQA-based objective function, we formulated a DNN optimization scheme on the basis of black-box optimization, which is used for training a computer that plays a game. For a black-box-optimization scheme, we adopt the policy gradient method for calculating the gradient on the basis of a sampling algorithm. To simulate output signals using the sampling algorithm, DNNs are used to estimate the probability density function of the output signals that maximize OSQA scores. The OSQA scores are calculated from the simulated output signals, and the DNNs are trained to increase the probability of generating the simulated output signals that achieve high OSQA scores. Through several experiments, we found that OSQA scores significantly increased by applying the proposed method, even though the MSE was not minimized.

265.
TITLE: End-to-End Speech Separation with Unfolded Iterative Phase Reconstruction
AUTHORS: Z. Wang, J. Le Roux, D. Wang, J. Hershey
YEAR: 2018
SOURCE: arXiv
ABSTRACT: This paper proposes an end-to-end approach for single-channel speaker-independent multi-speaker speech separation, where time-frequency (T-F) masking, the short-time Fourier transform (STFT), and its inverse are represented as layers within a deep network. Previous approaches, rather than computing a loss on the reconstructed signal, used a surrogate loss based on the target STFT magnitudes. This ignores reconstruction error introduced by phase inconsistency. In our approach, the loss function is directly defined on the reconstructed signals, which are optimized for best separation. In addition, we train through unfolded iterations of a phase reconstruction algorithm, represented as a series of STFT and inverse STFT layers. While mask values are typically limited to lie between zero and one for approaches using the mixture phase for reconstruction, this limitation is less relevant if the estimated magnitudes are to be used together with phase reconstruction. We thus propose several novel activation functions for the output layer of the T-F masking, to allow mask values beyond one. On the publicly-available wsj0-2mix dataset, our approach achieves state-of-the-art 12.6 dB scale-invariant signal-to-distortion ratio (SI-SDR) and 13.1 dB SDR, revealing new possibilities for deep learning based phase reconstruction and representing a fundamental progress towards solving the notoriously-hard cocktail party problem.

266.
TITLE: Multiple-Input Neural Network-Based Residual Echo Suppression
AUTHORS: G. Carbajal, R. Serizel, E. Vincent, E. Humbert
YEAR: 2018
SOURCE: 2018 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: A residual echo suppressor (RES) aims to suppress the residual echo in the output of an acoustic echo canceler (AEC). Spectral-based RES approaches typically estimate the magnitude spectra of the near-end speech and the residual echo from a single input, that is either the far-end speech or the echo computed by the AEC, and derive the RES filter coefficients accordingly. These single inputs do not always suffice to discriminate the near-end speech from the remaining echo. In this paper, we propose a neural network-based approach that directly estimates the RES filter coefficients from multiple inputs, including the AEC output, the far-end speech, and/or the echo computed by the AEC. We evaluate our system on real recordings of acoustic echo and near-end speech acquired in various situations with a smart speaker. We compare it to two single-input spectral-based approaches in terms of echo reduction and near-end speech distortion.

267.
TITLE: Speech Enhancement of Noisy and Reverberant Speech for Text-to-Speech
AUTHORS: C. Valentini-Botinhao, J. Yamagishi
YEAR: 2018
SOURCE: IEEE/ACM Transactions on Audio, Speech, and Language Processing
ABSTRACT: Text-to-speech voices created from noisy and reverberant recordings are of lower quality. A simple way to improve this is to increase the quality of the recordings prior to text-to-speech training with speech enhancement methods such as noise suppression and dereverberation. In this paper, we opted for this approach and to perform the enhancement, we used a recurrent neural network. The network is trained with parallel data of clean and lower quality recordings of speech. The lower quality data was artificially created by adding recordings of environmental noise to studio-quality recordings of speech and by convolving room impulse responses with these clean recordings. We trained separate networks with noise-only, reverberation-only, and both reverberation and additive noise data. The quality of voices trained with lower quality data that has been enhanced using these networks was significantly higher in all cases. For the noise-only case, the enhanced synthetic voice ranked as high as the voice trained with clean data. For the most realistic and challenging scenario, when both noise and reverberation were present, the improvements were more modest, but still significant.

268.
TITLE: End-to-End Sound Source Enhancement Using Deep Neural Network in the Modified Discrete Cosine Transform Domain
AUTHORS: Y. Koizumi, N. Harada, Y. Haneda, Y. Hioka, K. Kobayashi
YEAR: 2018
SOURCE: 2018 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: This paper presents an end-to-end deep neural network (DNN)-based source enhancement on the basis of a time-frequency (T-F) mask processing in the modified discrete cosine transform (MDCT)-domain. To retrieve the target signal perfectly in the discrete Fourier transform (DFT)-domain, both amplitude and phase of the spectrum need to be manipulated. However, since it is difficult to deal with complex values by neural network straightforward way, a real-valued T-F mask is commonly estimated and only amplitude spectrum is manipulated. In this study, we use the MDCT instead of the DFT and estimate real-valued T-F masks in the MDCT-domain. The perfect retrieval can be achieved by manipulating only the real-valued MDCT-spectra. To reduce time-domain aliasing arises from manipulating the MDCT spectrum, we build an end-to-end DNN-based source enhancement using T-F mask and train the DNN to minimize an objective function defined in the time-domain. In experiments using several kinds of objective sound quality scores, we observed that the scores were significantly improved.

269.
TITLE: On SDW-MWF and Variable Span Linear Filter with Application to Speech Recognition in Noisy Environments
AUTHORS: Z. Wang, L. Yin, J. Li, Y. Yan
YEAR: 2018
SOURCE: 2018 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: Neural network based spectral mask estimation for acoustic beamforming, which consists of linear filtering and mask estimation, has shown to be a promising approach for robust speech recognition in noisy environments. Nevertheless, few improvements are made on the linear filtering. In this paper, we investigate the Speech Distortion Weighted Multichannel Wiener Filter (SDW-MWF) and the variable span linear filter, and prove that they can be linked by Generalized Eigenvalue Decomposition (GEVD) of the speech covariance matrix. The resulting GEVD based SDW-MWF largely reduces the word error rate and even achieves competitive recognition performance with the state-of-the-art generalized eigenvalue beamformer. Furthermore, we found that the recent signal approximation is no better than mask approximation when combined in calculating the linear filter coefficients.

270.
TITLE: A Hybrid Approach to Combining Conventional and Deep Learning Techniques for Single-Channel Speech Enhancement and Recognition
AUTHORS: Y. Tu, I. Tashev, S. Zarar, C. Lee
YEAR: 2018
SOURCE: 2018 IEEE International Conference on Acoustics, Speech and Signal Processing 
ABSTRACT: Conventional speech-enhancement techniques employ statistical signal-processing algorithms. They are computationally efficient and improve speech quality even under unknown noise conditions. For these reasons, they are preferred for deployment in unpredictable environments. One limitation of these algorithms is that they fail to suppress non-stationary noise. This hinders their broad usage. Emerging algorithms based on deep-learning promise to overcome this limitation of conventional methods. However, these algorithms under-perform when presented with noise conditions that were not captured in the training data. In this paper, we propose a single-channel speech-enhancement technique that combines the benefits of both worlds to achieve the best listening-quality and recognition-accuracy under conditions of noise that are both unknown and nonstationary. Our method utilizes a conventional speech-enhancement algorithm to produce an intermediate representation of the input data by multiplying noisy input spectrogram features with gain vectors (known as the suppression rule). We process this intermediate representation through a recurrent neural-network based on long short-term memory (LSTM) units. Furthermore, we train this network to jointly learn two targets: a direct estimate of clean-speech features and a noise-reduction mask. Based on this LSTM multi-style training (LSTM-MT) architecture, we demonstrate PESQ improvement of 0.76 and a relative word-error rate reduction of 47.73%.

271.
TITLE: Multichannel Speech Separation with Recurrent Neural Networks from High-Order Ambisonics Recordings
AUTHORS: L. Perotin, R. Serizel, E. Vincent, A. Guerin
YEAR: 2018
SOURCE: 2018 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: We present a source separation system for high-order ambisonics (HOA) contents. We derive a multichannel spatial filter from a mask estimated by a long short-term memory (LSTM) recurrent neural network. We combine one channel of the mixture with the outputs of basic HOA beamformers as inputs to the LSTM, assuming that we know the directions of arrival of the directional sources. In our experiments, the speech of interest can be corrupted either by diffuse noise or by an equally loud competing speaker. We show that adding as input the output of the beamformer steered toward the competing speech in addition to that of the beamformer steered toward the target speech brings significant improvements in terms of word error rate.

272.
TITLE: Perceptually Guided Speech Enhancement Using Deep Neural Networks
AUTHORS: Y. Zhao, B. Xu, R. Giri, T.Zhang
YEAR: 2018
SOURCE: 2018 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: Human listeners often have difficulties understanding speech in the presence of background noise in the real world. Recently, supervised learning based speech enhancement approaches have achieved substantial success, and show significant improvements over the conventional approaches. However, existing supervised learning based approaches often try to minimize the mean squared error between the enhanced output and the pre-defined training target (e.g., the log power spectrum of clean speech), even though the purpose of such speech enhancement is to improve speech understanding in noise. In this paper, we propose a new deep neural networks based enhancement approach by incorporating a speech perception model into the loss function. Specifically, we use the short-time objective intelligibility metric in the loss in addition to the mean squared error. Optimizing the proposed perceptually guided loss is expected to improve speech intelligibility further. Systematic evaluations show that our proposed approach is able to improve speech intelligibility in a wide range of signal-to-noise ratios and noise types while maintaining speech quality.

273.
TITLE: Light Gated Recurrent Units for Speech Recognition
AUTHORS: M. Ravanelli, P. Brakel, M. Omologo, Y. Bengio
YEAR: 2018
SOURCE: IEEE Transactions on Emerging Topics in Computational Intelligence
ABSTRACT: A field that has directly benefited from the recent advances in deep learning is automatic speech recognition (ASR). Despite the great achievements of the past decades, however, a natural and robust human–machine speech interaction still appears to be out of reach, especially in challenging environments characterized by significant noise and reverberation. To improve robustness, modern speech recognizers often employ acoustic models based on recurrent neural networks (RNNs) that are naturally able to exploit large time contexts and long-term speech modulations. It is thus of great interest to continue the study of proper techniques for improving the effectiveness of RNNs in processing speech signals. In this paper, we revise one of the most popular RNN models, namely, gated recurrent units (GRUs), and propose a simplified architecture that turned out to be very effective for ASR. The contribution of this work is twofold: First, we analyze the role played by the reset gate, showing that a significant redundancy with the update gate occurs. As a result, we propose to remove the former from the GRU design, leading to a more efficient and compact single-gate model. Second, we propose to replace hyperbolic tangent with rectified linear unit activations. This variation couples well with batch normalization and could help the model learn long-term dependencies without numerical issues. Results show that the proposed architecture, called light GRU, not only reduces the per-epoch training time by more than 30% over a standard GRU, but also consistently improves the recognition accuracy across different tasks, input features, noisy conditions, as well as across different ASR paradigms, ranging from standard DNN-HMM speech recognizers to end-to-end connectionist temporal classification models.

274.
TITLE: Conditional Generative Model for Speech Enhancement
AUTHORS: Z. Li, L.R. Dai, Y. Song, I. Mcloughlin
YEAR: 2018
SOURCE: Circuits, Systems, and Signal Processing
ABSTRACT: Deep learning-based speech enhancement approaches like deep neural networks (DNN) and Long Short-Term Memory (LSTM) have already demonstrated superior results to classical methods. However, these methods do not take full advantage of temporal context information. While DNN and LSTM consider temporal context in the noisy source speech, it does not do so for the estimated clean speech. Both DNN and LSTM also have a tendency to over-smooth spectra, which causes the enhanced speech to sound muffled. This paper proposes a novel architecture to address both issues, which we term a conditional generative model (CGM). By adopting an adversarial training scheme applied to a generator of deep dilated convolutional layers, CGM is designed to model the joint and symmetric conditions of both noisy and estimated clean spectra. We evaluate CGM against both DNN and LSTM in terms of Perceptual Evaluation of Speech Quality (PESQ) and Short-Time Objective Intelligibility (STOI) on TIMIT sentences corrupted by ITU-T P.501 and NOISEX-92 noise in a range of matched and mismatched noise conditions. Results show that both the CGM architecture and the adversarial training mechanism lead to better PESQ and STOI in all tested noise conditions. In addition to yielding significant improvements in PESQ and STOI, CGM and adversarial training both mitigate against over-smoothing.

275.
TITLE: Deep Neural Network Based Multichannel Audio Source Separation
AUTHORS: A.A. Nigraha, A. Liutkus, E. Vincent
YEAR: 2018
SOURCE: Signals and Communication Technology
ABSTRACT: This chapter presents a multichannel audio source separation framework where deep neural networks (DNNs) are used to model the source spectra and combined with the classical multichannel Gaussian model to exploit the spatial information. The parameters are estimated in an iterative expectation-maximization (EM) fashion and used to derive a multichannel Wiener filter. Different design choices and their impact on the performance are discussed. They include the cost functions for DNN training, the number of parameter updates, the use of multiple DNNs, and the use of weighted parameter updates. Finally, we present its application to a speech enhancement task and a music separation task. The experimental results show the benefit of the multichannel DNN-based approach over a single-channel DNN-based approach and the multichannel nonnegative matrix factorization based iterative EM framework.

276.
TITLE: Deep Learning Based Speech Beamforming
AUTHORS: K. Qian, et al.
YEAR: 2018
SOURCE: 2018 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: Multi-channel speech enhancement with ad-hoc sensors has been a challenging task. Speech model guided beamforming algorithms are able to recover natural sounding speech, but the speech models tend to be oversimplified or the inference would otherwise be too complicated. On the other hand, deep learning based enhancement approaches are able to learn complicated speech distributions and perform efficient inference, but they are unable to deal with variable number of input channels. Also, deep learning approaches introduce a lot of errors, particularly in the presence of unseen noise types and settings. We have therefore proposed an enhancement framework called DEEPBEAM, which combines the two complementary classes of algorithms. DEEPBEAM introduces a beamforming filter to produce natural sounding speech, but the filter coefficients are determined with the help of a monaural speech enhancement neural network. Experiments on synthetic and real-world data show that Deepbeam is able to produce clean, dry and natural sounding speech, and is robust against unseen noise.

277.
TITLE: Monaural Speech Enhancement Using Deep Neural Networks by Maximizing a Short-Time Objective Intelligibility Measure
AUTHORS: M. Kolbaek, Z. Tan, J. Jensen
YEAR: 2018
SOURCE: 2018 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: In this paper we propose a Deep Neural Network (DNN) based Speech Enhancement (SE) system that is designed to maximize an approximation of the Short-Time Objective Intelligibility (STOI) measure. We formalize an approximate-STOI cost function and derive analytical expressions for the gradients required for DNN training and show that these gradients have desirable properties when used together with gradient based optimization techniques. We show through simulation experiments that the proposed SE system achieves large improvements in estimated speech intelligibility, when tested on matched and unmatched natural noise types, at multiple signal-to-noise ratios. Furthermore, we show that the SE system, when trained using an approximate-STOI cost function performs on par with a system trained with a mean square error cost applied to short-time temporal envelopes. Finally, we show that the proposed SE system performs on par with a traditional DNN based Short- Time Spectral Amplitude (STSA) SE system in terms of estimated speech intelligibility. These results are important because they suggest that traditional DNN based STSA SE systems might be optimal in terms of estimated speech intelligibility.

278.
TITLE: Speech Dereverberation With Context-Aware Recurrent Neural Networks
AUTHORS: J.F. Santos, T. Falk
YEAR: 2018
SOURCE: IEEE/ACM Transactions on Audio, Speech, and Language Processing
ABSTRACT: In this paper, we propose a model to perform speech dereverberation by estimating its spectral magnitude from the reverberant counterpart. Our models are capable of extracting features that take into account both short- and long-term dependencies in the signal through a convolutional encoder (which extracts features from a short, bounded context of frames) and a recurrent neural network for extracting long-term information. Our model outperforms a recently proposed model that uses different context information depending on the reverberation time, without requiring any sort of additional input, yielding improvements of up to 0.4 on perceptual evaluation of speech quality, 0.3 on short-time objective intelligibility, and 1.0 on perceptual objective listening quality assessment relative to reverberant speech. We also show our model is able to generalize to real room impulse responses even when only trained with simulated room impulse responses, different speakers, and high reverberation times. Finally, listening tests show the proposed method outperforming benchmark models in reduction of perceived reverberation.

279.
TITLE: Separation of Moving Sound Sources Using Multichannel NMF and Acoustic Tracking
AUTHORS: J. Nikunen, A. Diment, T. Virtanen
YEAR: 2018
SOURCE: IEEE/ACM Transactions on Audio, Speech, and Language Processing
ABSTRACT: In this paper, we propose a method for separation of moving sound sources. The method is based on first tracking the sources and then estimation of source spectrograms using multichannel nonnegative matrix factorization (NMF) and extracting the sources from the mixture by single-channel Wiener filtering. We propose a novel multichannel NMF model with time-varying mixing of the sources denoted by spatial covariance matrices (SCM) and provide update equations for optimizing model parameters minimizing squared Frobenius norm. The SCMs of the model are obtained based on estimated directions of arrival of tracked sources at each time frame. The evaluation is based on established objective separation criteria and using real recordings of two and three simultaneous moving sound sources. The compared methods include conventional beamforming and ideal ratio mask separation. The proposed method is shown to exceed the separation quality of other evaluated blind approaches according to all measured quantities. Additionally, we evaluate the method's susceptibility toward tracking errors by comparing the separation quality achieved using annotated ground truth source trajectories. 

280.
TITLE: End-to-End Waveform Utterance Enhancement for Direct Evaluation Metrics Optimization by Fully Convolutional Neural Networks
AUTHORS: S. Fu, T. Wang, Y. Tsao, X. Lu, H. Kawai
YEAR: 2018
SOURCE: EEE/ACM Transactions on Audio, Speech, and Language Processing
ABSTRACT: Speech enhancement model is used to map a noisy speech to a clean speech. In the training stage, an objective function is often adopted to optimize the model parameters. However, in the existing literature, there is an inconsistency between the model optimization criterion and the evaluation criterion for the enhanced speech. For example, in measuring speech intelligibility, most of the evaluation metric is based on a short-time objective intelligibility (STOI) measure, while the frame based mean square error (MSE) between estimated and clean speech is widely used in optimizing the model. Due to the inconsistency, there is no guarantee that the trained model can provide optimal performance in applications. In this study, we propose an end-to-end utterance-based speech enhancement framework using fully convolutional neural networks (FCN) to reduce the gap between the model optimization and the evaluation criterion. Because of the utterance-based optimization, temporal correlation information of long speech segments, or even at the entire utterance level, can be considered to directly optimize perception-based objective functions. As an example, we implemented the proposed FCN enhancement framework to optimize the STOI measure. Experimental results show that the STOI of a test speech processed by the proposed approach is better than conventional MSE-optimized speech due to the consistency between the training and the evaluation targets. Moreover, by integrating the STOI into model optimization, the intelligibility of human subjects and automatic speech recognition system on the enhanced speech is also substantially improved compared to those generated based on the minimum MSE criterion.

281.
TITLE: Supervised Speech Separation Based on Deep Learning: An Overview
AUTHORS: D. Wang, J. Chen
YEAR: 2018
SOURCE: IEEE/ACM Transactions on Audio, Speech, and Language Processing
ABSTRACT: Speech separation is the task of separating target speech from background interference. Traditionally, speech separation is studied as a signal processing problem. A more recent approach formulates speech separation as a supervised learning problem, where the discriminative patterns of speech, speakers, and background noise are learned from training data. Over the past decade, many supervised separation algorithms have been put forward. In particular, the recent introduction of deep learning to supervised speech separation has dramatically accelerated progress and boosted separation performance. This paper provides a comprehensive overview of the research on deep learning based supervised speech separation in the last several years. We first introduce the background of speech separation and the formulation of supervised separation. Then, we discuss three main components of supervised separation: learning machines, training targets, and acoustic features. Much of the overview is on separation algorithms where we review monaural methods, including speech enhancement (speech-nonspeech separation), speaker separation (multitalker separation), and speech dereverberation, as well as multimicrophone techniques. The important issue of generalization, unique to supervised learning, is discussed. This overview provides a historical perspective on how advances are made. In addition, we discuss a number of conceptual issues, including what constitutes the target source.

282.
TITLE: Speaker-Independent Speech Separation With Deep Attractor Network
AUTHORS: Y. Luo, Z. Chen, N. Mesgarani
YEAR: 2018
SOURCE: IEEE/ACM Transactions on Audio, Speech, and Language Processing
ABSTRACT: Despite the recent success of deep learning for many speech processing tasks, single-microphone, speaker-independent speech separation remains challenging for two main reasons. The first reason is the arbitrary order of the target and masker speakers in the mixture (permutation problem), and the second is the unknown number of speakers in the mixture (output dimension problem). We propose a novel deep learning framework for speech separation that addresses both of these issues. We use a neural network to project the time-frequency representation of the mixture signal into a high-dimensional embedding space. A reference point (attractor) is created in the embedding space to represent each speaker which is defined as the centroid of the speaker in the embedding space. The time-frequency embeddings of each speaker are then forced to cluster around the corresponding attractor point which is used to determine the time-frequency assignment of the speaker. We propose three methods for finding the attractors for each source in the embedding space and compare their advantages and limitations. The objective function for the network is standard signal reconstruction error which enables end-to-end operation during both training and test phases. We evaluated our system using the Wall Street Journal dataset (WSJ0) on two and three speaker mixtures and report comparable or better performance than other state-of-the-art deep learning methods for speech separation.

283.
TITLE: Deep Learning for Environmentally Robust Speech Recognition
AUTHORS: Z. Zhang, J. Geiger, J. Pohjalainen, A. Mousa, B. Schuller
YEAR: 2018
SOURCE: ACM Transactions on Intelligent Systems and Technology
ABSTRACT: Eliminating the negative effect of non-stationary environmental noise is a long-standing research topic for automatic speech recognition but still remains an important challenge. Data-driven supervised approaches, especially the ones based on deep neural networks, have recently emerged as potential alternatives to traditional unsupervised approaches and with sufficient training, can alleviate the shortcomings of the unsupervised methods in various real-life acoustic environments. In this light, we review recently developed, representative deep learning approaches for tackling non-stationary additive and convolutional degradation of speech with the aim of providing guidelines for those involved in the development of environmentally robust speech recognition systems. We separately discuss single- and multi-channel techniques developed for the front-end and back-end of speech recognition systems, as well as joint front-end and back-end training frameworks. In the meanwhile, we discuss the pros and cons of these approaches and provide their experimental results on benchmark databases. We expect that this overview can facilitate the development of the robustness of speech recognition systems in acoustic noisy environments.

284.
TITLE: On the Importance of Super-Gaussian Speech Priors for Machine-Learning Based Speech Enhancement
AUTHORS: R. Reher, T. Gerkmann
YEAR: 2018
SOURCE: IEEE/ACM Transactions on Audio, Speech, and Language Processing
ABSTRACT: For enhancing noisy signals, machine-learning based single-channel speech enhancement schemes exploit prior knowledge about typical speech spectral structures. To ensure a good generalization and to meet requirements in terms of computational complexity and memory consumption, certain methods restrict themselves to learning speech spectral envelopes. We refer to these approaches as machine-learning spectral envelope (MLSE)-based approaches. In this paper, we show by means of theoretical and experimental analyses that for MLSE-based approaches, super-Gaussian priors allow for a reduction of noise between speech spectral harmonics which is not achievable using Gaussian estimators such as the Wiener filter. For the evaluation, we use a deep neural network based phoneme classifier and a low-rank nonnegative matrix factorization framework as examples of MLSE-based approaches. A listening experiment and instrumental measures confirm that while super-Gaussian priors yield only moderate improvements for classic enhancement schemes, for MLSE-based approaches super-Gaussian priors clearly make an important difference and significantly outperform Gaussian priors.

285.
TITLE: Sound Signal Processing with Seq2Tree Network
AUTHORS: W. Ma, K. Cao, Z. Ni, P. Chin, X. Li
YEAR: 2018
SOURCE: LREC 2018
ABSTRACT: Long Short-Term Memory (LSTM) and its variants have been the standard solution to sequential data processing tasks because of their ability to preserve previous information weighted on distance. This feature provides the LSTM family with additional information in predictions, compared to regular Recurrent Neural Networks (RNNs) and Bag-of-Words (BOW) models. In other words, LSTM networks assume the data to be chain-structured. The longer the distance between two data points, the less related the data points are. However, this is usually not the case for real multimedia signals including text, sound and music. In real data, this chain-structured dependency exists only across meaningful groups of data units but not over single units directly. For example, in a prediction task over sound signals, a meaningful word could give a strong hint to its following word as a whole but not the first phoneme of that word. This undermines the ability of LSTM networks in modeling multimedia data, which is pattern-rich. In this paper we take advantage of Seq2Tree network, a dynamically extensible tree-structured neural network architecture which helps solve the problem LSTM networks face in sound signal processing tasks—the unbalanced connections among data units inside and outside semantic groups. Experiments show that Seq2Tree network outperforms the state-of-the-art Bidirectional LSTM (BLSTM) model on a signal and noise separation task (CHiME Speech Separation and Recognition Challenge).

286.
TITLE: DNN Based Mask Estimation for Supervised Speech Separation
AUTHORS: J. Chen, D. Wang
YEAR: 2018
SOURCE: Signals and Communication Technology
ABSTRACT: This chapter introduces deep neural network (DNN) based mask estimation for supervised speech separation. Originated in computational auditory scene analysis (CASA), we treat speech separation as a mask estimation problem. Given a time-frequency (T-F) representation of noisy speech, the ideal binary mask (IBM) or ideal ratio mask (IRM) is defined to differentiate speech-dominant T-F units from noise-dominant ones. Mask estimation is then formulated as a problem of supervised learning, which learns a mapping function from acoustic features extracted from noisy speech to an ideal mask. Three main aspects of supervised learning are learning machines, training targets, and features, which are discussed in separate sections. Subsequently, we describe several representative supervised algorithms, mainly for monaural speech separation. For supervised separation, generalization to unseen conditions is a critical issue. The generalization capability of supervised speech separation is also discussed. 

287.
TITLE: Explorer Speech Enhancement for a Noise-Robust Text-to-Speech Synthesis System using Deep Recurrent Neural Networks
AUTHORS: C. Valentini-Botinhao, X. Wang, S. Takaki, J. Yamagishi
YEAR: 2016
SOURCE: Interspeech 2016
ABSTRACT: Quality of text-to-speech voices built from noisy recordings is diminished. In order to improve it we propose the use of a recurrent neural network to enhance acoustic parameters prior to training. We trained a deep recurrent neural network using a parallel database of noisy and clean acoustics parameters as input and output of the network. The database consisted of multiple speakers and diverse noise conditions. We investigated using text-derived features as an additional input of the network. We processed a noisy database of two other speakers using this network and used its output to train an HMM acoustic text-to-synthesis model for each voice. Listening experiment results showed that the voice built with enhanced parameters was ranked significantly higher than the ones trained with noisy speech and speech that has been enhanced using a conventional enhancement system. The text-derived features improved results only for the female voice, where it was ranked as highly as a voice trained with clean speech. 

288.
TITLE: Deep Neural Networks for Source Separation and Noise-Robust Speech Recognition
AUTHORS: A.A. Nugraha
YEAR: 2017
SOURCE: Universitr de Lorraine
ABSTRACT: This thesis addresses the problem of multichannel audio source separation by exploiting deep neural networks (DNNs). We build upon the classical expectation-maximization (EM) based source separation framework employing a multichannel Gaussian model, in which the sources are characterized by their power spectral densities and their source spatial covariance matrices. We explore and optimize the use of DNNs for estimating these spectral and spatial parameters. Employing the estimated source parameters, we then derive a time-varying multichannel Wiener filter for the separation of each source. We extensively study the impact of various design choices for the spectral and spatial DNNs. We consider different cost functions, time-frequency representations, architectures, and training data sizes. Those cost functions notably include a newly proposed task-oriented signal-to-distortion ratio cost function for spectral DNNs. Furthermore, we present a weighted spatial parameter estimation formula, which generalizes the corresponding exact EM formulation. On a singing-voice separation task, our systems perform remarkably close to the current state-of-the-art method and provide up to 2 dB improvement of the source-to-interference ratio. On a speech enhancement task, our systems outperforms the state-of-the-art GEV-BAN beamformer by 14%, 7%, and 1% relative word error rate improvement on 6-channel, 4-channel, and 2-channel data, respectively.

289.
TITLE: Learning Speaker Representation for Neural Network Based Multichannel Speaker Extraction
AUTHORS: K. Zmolikova, et al.  
YEAR: 2017
SOURCE: 2017 IEEE Automatic Speech Recognition and Understanding Workshop
ABSTRACT: Recently, schemes employing deep neural networks (DNNs) for extracting speech from noisy observation have demonstrated great potential for noise robust automatic speech recognition. However, these schemes are not well suited when the interfering noise is another speaker. To enable extracting a target speaker from a mixture of speakers, we have recently proposed to inform the neural network using speaker information extracted from an adaptation utterance from the same speaker. In our previous work, we explored ways how to inform the network about the speaker and found a speaker adaptive layer approach to be suitable for this task. In our experiments, we used speaker features designed for speaker recognition tasks as the additional speaker information, which may not be optimal for the speaker extraction task. In this paper, we propose a usage of a sequence summarizing scheme enabling to learn the speaker representation jointly with the network. Furthermore, we extend the previous experiments to demonstrate the potential of our proposed method as a front-end for speech recognition and explore the effect of additional noise on the performance of the method.

290.
TITLE: Speech Separation by Cost-Sensitive Deep Learning
AUTHORS: X.L. Zhang
YEAR: 2017
SOURCE: 2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference
ABSTRACT: Deep learning based speech separation has demonstrated good performance in adverse environments. Recent study shows that multi-condition training, which trains a model with several noise scenarios, shows good generalization in test. However, treating all noise scenarios with the same training cost is usually not a good choice: A common problem is that, when training data contain a wide range of SNR, the data in low SNR environments suffer from large training loss, which results in a performance drop when test SNRs are low. In this paper, we propose three cost-sensitive deep learning methods to improve the performance of speech separation methods at low SNRs, which are the methods of (i) learning with a cost-sensitive objective, (ii) learning with cost-sensitive oversampling of training data, and (iii) learning with cost-sensitive undersampling of training data. We also propose to aggregate the three methods to a cost- sensitive deep ensemble learning method. Experimental results demonstrate the effectiveness of the proposed methods.

291.
TITLE: Multi-Microphone Speech Recognition Integrating Beamforming, Robust Feature Extraction, and Advanced DNN/RNN Backend
AUTHORS: T. Hori, et al.
YEAR: 2017
SOURCE: Computer Speech and Language
ABSTRACT: This paper gives an in-depth presentation of the multi-microphone speech recognition system we submitted to the 3rd CHiME speech separation and recognition challenge (CHiME-3) and its extension. The proposed system takes advantage of recurrent neural networks (RNNs) throughout the model from the front-end speech enhancement to the language modeling. Three different types of beamforming are used to combine multi-microphone signals to obtain a single higher-quality signal. The beamformed signal is further processed by a single-channel long short-term memory (LSTM) enhancement network, which is used to extract stacked mel-frequency cepstral coefficients (MFCC) features. In addition, the beamformed signal is processed by two proposed noise-robust feature extraction methods. All features are used for decoding in speech recognition systems with deep neural network (DNN) based acoustic models and large-scale RNN language models to achieve high recognition accuracy in noisy environments. Our training methodology includes multi-channel noisy data training and speaker adaptive training, whereas at test time model combination is used to improve generalization. Results on the CHiME-3 benchmark show that the full set of techniques substantially reduced the word error rate (WER). Combining hypotheses from different beamforming and robust-feature systems ultimately achieved 5.05% WER for the real-test data, an 84.7% reduction relative to the baseline of 32.99% WER and a 44.5% reduction from our official CHiME-3 challenge result of 9.1% WER. Furthermore, this final result is better than the best result (5.8% WER) reported in the CHiME-3 challenge.

292.
TITLE: Seq2Tree: A Tree-Structured Extension of LSTM Network
AUTHORS: W. Ma, Z. Ni, K. Cao, X. Li, S. Chin
YEAR: 2017
SOURCE:
ABSTRACT: Long Short-Term Memory network(LSTM) has attracted much attention on sequence modeling tasks, because of its ability to preserve longer term information in a sequence, compared to ordinary Recurrent Neural Networks(RNN’s). The basic LSTM structure assumes a chain structure of the input sequence. However, audio streams often show a trend of combining phonemes into meaningful units, which could be words in speech processing task, or a certain type of noise in signal and noise separation task. We introduce Seq2Tree network, a modification of the LSTM network which constructs a tree structure from an input sequence. Experiments show that Seq2Tree network outperforms the state-of-the-art Bidirectional LSTM(BLSTM) model on the signal and noise separation task, namely CHiME Speech Separation and Recognition Challenge.

293.
TITLE: An Image-based Deep Spectrum Feature Representation for the Recognition of Emotional Speech
AUTHORS: N. Cummins, et al.
YEAR: 2017
SOURCE: Proceedings of the 25th ACM international conference on Multimedia
ABSTRACT: The outputs of the higher layers of deep pre-trained convolutional neural networks (CNNs) have consistently been shown to provide a rich representation of an image for use in recognition tasks. This study explores the suitability of such an approach for speech-based emotion recognition tasks. First, we detail a new acoustic feature representation, denoted as deep spectrum features, derived from feeding spectrograms through a very deep image classification CNN and forming a feature vector from the activations of the last fully connected layer. We then compare the performance of our novel features with standardised brute-force and bag-of-audio-words (BoAW) acoustic feature representations for 2- and 5-class speech-based emotion recognition in clean, noisy and denoised conditions. The presented results show that image-based approaches are a promising avenue of research for speech-based recognition tasks. Key results indicate that deep-spectrum features are comparable in performance with the other tested acoustic feature representations in matched for noise type train-test conditions; however, the BoAW paradigm is better suited to cross-noise-type train-test conditions.

294.
TITLE: Low Latency Sound Source Separation Using Convolutional Recurrent Neural Networks
AUTHORS: 2017 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics
YEAR: 2017
SOURCE: G. Naithani, et al.
ABSTRACT: Deep neural networks (DNN) have been successfully employed for the problem of monaural sound source separation achieving state-of-the-art results. In this paper, we propose using convolutional recurrent neural network (CRNN) architecture for tackling this problem. We focus on a scenario where low algorithmic delay (< 10 ms) is paramount, and relatively little training data is available. We show that the proposed architecture can achieve slightly better performance as compared to feedforward DNNs and long short-term memory (LSTM) networks. In addition to reporting separation performance metrics (i.e., source to distortion ratios), we also report extended short term objective intelligibility (ESTOI) scores which better predict intelligibility performance in presence of non-stationary interferers.

295.
TITLE: Deep Recurrent NMF for Speech Separation by Unfolding Iterative Thresholding
AUTHORS: 2017 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics
YEAR: 2017
SOURCE: 2017 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics
ABSTRACT: In this paper, we propose a novel recurrent neural network architecture for speech separation. This architecture is constructed by unfolding the iterations of a sequential iterative soft-thresholding algorithm (ISTA) that solves the optimization problem for sparse nonnegative matrix factorization (NMF) of spectrograms. We name this network architecture deep recurrent NMF (DR-NMF). The proposed DR-NMF network has three distinct advantages. First, DR-NMF provides better interpretability than other deep architectures, since the weights correspond to NMF model parameters, even after training. This interpretability also provides principled initializations that enable faster training and convergence to better solutions compared to conventional random initialization. Second, like many deep networks, DR-NMF is an order of magnitude faster at test time than NMF, since computation of the network output only requires evaluating a few layers at each time step. Third, when a limited amount of training data is available, DR-NMF exhibits stronger generalization and separation performance compared to sparse NMF and state-of-the-art long-short term memory (LSTM) networks. When a large amount of training data is available, DR-NMF achieves lower yet competitive separation performance compared to LSTM networks.

296.
TITLE: Normalized Features for Improving the Generalization of DNN Based Speech Enhancement
AUTHORS: R. Rehr, T. Gerkmann
YEAR: 2017
SOURCE: arXiv
ABSTRACT: Enhancing noisy speech is an important task to restore its quality and to improve its intelligibility. In traditional non-machine-learning (ML) based approaches the parameters required for noise reduction are estimated blindly from the noisy observation while the actual filter functions are derived analytically based on statistical assumptions. Even though such approaches generalize well to many different acoustic conditions, the noise suppression capability in transient noises is low. To amend this shortcoming, machine-learning (ML) methods such as deep learning have been employed for speech enhancement. However, due to their data-driven nature, the generalization of ML based approaches to unknown noise types is still discussed. To improve the generalization of ML based algorithms and to enhance the noise suppression of non-ML based methods, we propose a combination of both approaches. For this, we employ the a priori signal-to-noise ratio (SNR) and the a posteriori SNR estimated as input features in a deep neural network (DNN) based enhancement scheme. We show that this approach allows ML based speech estimators to generalize quickly to unknown noise types even if only few noise conditions have been seen during training. Further, the proposed features outperform a competing approach where an estimate of the noise power spectral density is appended to the noisy spectra. Instrumental measures such as Perceptual Evaluation of Speech Quality (PESQ) and short-time objective intelligibility (STOI) indicate strong improvements in unseen conditions when the proposed features are used. Listening experiments confirm the improved generalization of our proposed combination.

297.
TITLE: A Recurrent Encoder-Decoder Approach with Skip-Filtering Connections for Monaural Singing Voice Separation
AUTHORS: S.I. Mimilakis, K. Drossos, T. Virtanen, G. Schuller
YEAR: 2017
SOURCE: 2017 IEEE 27th International Workshop on Machine Learning for Signal Processing
ABSTRACT: The objective of deep learning methods based on encoder-decoder architectures for music source separation is to approximate either ideal time-frequency masks or spectral representations of the target music source(s). The spectral representations are then used to derive time-frequency masks. In this work we introduce a method to directly learn time-frequency masks from an observed mixture magnitude spectrum. We employ recurrent neural networks and train them using prior knowledge only for the magnitude spectrum of the target source. To assess the performance of the proposed method, we focus on the task of singing voice separation. The results from an objective evaluation show that our proposed method provides comparable results to deep learning based methods which operate over complicated signal representations. Compared to previous methods that approximate time-frequency masks, our method has increased performance of signal to distortion ratio by an average of 3.8 dB.

298.
TITLE: Two-Stage Single-Channel Audio Source Separation Using Deep Neural Networks
AUTHORS: E.M. Grais, G. Roma, A.J.R. Simpson, M.D. Plumbley
YEAR: 2017
SOURCE: IEEE/ACM Transactions on Audio, Speech, and Language Processing
ABSTRACT: Most single channel audio source separation approaches produce separated sources accompanied by interference from other sources and other distortions. To tackle this problem, we propose to separate the sources in two stages. In the first stage, the sources are separated from the mixed signal. In the second stage, the interference between the separated sources and the distortions are reduced using deep neural networks (DNNs). We propose two methods that use DNNs to improve the quality of the separated sources in the second stage. In the first method, each separated source is improved individually using its own trained DNN, while in the second method all the separated sources are improved together using a single DNN. To further improve the quality of the separated sources, the DNNs in the second stage are trained discriminatively to further decrease the interference and the distortions of the separated sources. Our experimental results show that using two stages of separation improves the quality of the separated signals by decreasing the interference between the separated sources and distortions compared to separating the sources using a single stage of separation.

299.
TITLE: Joint Separation and Denoising of Noisy Multi-Talker Speech Using Recurrent Neural Networks and Permutation Invariant Training
AUTHORS: M. Kolbaek, D. Yu, Z.H. Tan, J. Jensen
YEAR: 2017
SOURCE: 2017 IEEE 27th International Workshop on Machine Learning for Signal Processing
ABSTRACT: In this paper we propose to use utterance-level Permutation Invariant Training (uPIT) for speaker independent multi-talker speech separation and denoising, simultaneously. Specifically, we train deep bi-directional Long Short-Term Memory (LSTM) Recurrent Neural Networks (RNNs) using uPIT, for single-channel speaker independent multi-talker speech separation in multiple noisy conditions, including both synthetic and real-life noise signals. We focus our experiments on generalizability and noise robustness of models that rely on various types of a priori knowledge e.g. in terms of noise type and number of simultaneous speakers. We show that deep bi-directional LSTM RNNs trained using uPIT in noisy environments can improve the Signal-to-Distortion Ratio (SDR) as well as the Extended Short-Time Objective Intelligibility (ESTOI) measure, on the speaker independent multi-talker speech separation and denoising task, for various noise types and Signal-to-Noise Ratios (SNRs). Specifically, we first show that LSTM RNNs can achieve large SDR and ESTOI improvements, when evaluated using known noise types, and that a single model is capable of handling multiple noise types with only a slight decrease in performance. Furthermore, we show that a single LSTM RNN can handle both two-speaker and three-speaker noisy mixtures, without a priori knowledge about the exact number of speakers. Finally, we show that LSTM RNNs trained using uPIT generalize well to noise types not seen during training.

300.
TITLE: Deep Recurrent Neural Network Based Monaural Speech Separation Using Recurrent Temporal Restricted Boltzmann Machines
AUTHORS: S. Samui, I. Chakrabati, S. Ghosh
YEAR: 2017
SOURCE: Interspeech 2017
ABSTRACT: This paper presents a single-channel speech separation method implemented with a deep recurrent neural network (DRNN) using recurrent temporal restricted Boltzmann machines (RTRBM). Although deep neural network (DNN) based speech separation (denoising task) methods perform quite well compared to the conventional statistical model based speech enhancement techniques, in DNN-based methods, the temporal correlations across speech frames are often ignored, resulting in loss of spectral detail in the reconstructed output speech. In order to alleviate this issue, one RTRBM is employed for modelling the acoustic features of input (mixture) signal and two RTRBMs are trained for the two training targets (source signals). Each RTRBM attempts to model the abstractions present in the training data at each time step as well as the temporal dependencies in the training data. The entire network (consisting of three RTRBMs and one recurrent neural network) can be fine-tuned by the joint optimization of the DRNN with an extra masking layer which enforces a reconstruction constraint. The proposed method has been evaluated on the IEEE corpus and TIMIT dataset for speech denoising task. Experimental results have established that the proposed approach outperforms NMF and conventional DNN and DRNN-based speech enhancement methods.

301.
TITLE: A Maximum Likelihood Approach to Deep Neural Network Based Nonlinear Spectral Mapping for Single-Channel Speech Separation
AUTHORS: Y. Wang, J. Du, L.R. Dai, C. Lee
YEAR: 2017
SOURCE: Interspeech 2017
ABSTRACT: In contrast to the conventional minimum mean squared error (MMSE) training criterion for nonlinear spectral mapping based on deep neural networks (DNNs), we propose a probabilistic learning framework to estimate the DNN parameters for single channel speech separation. A statistical analysis of the prediction error vector at the DNN output reveals that it follows a unimodal density for each log power spectral component. By characterizing the prediction error vector as a multivariate Gaussian density with zero mean vector and an unknown covariance matrix, we present a maximum likelihood (ML) approach to DNN parameter learning. Our experiments on the Speech Separation Challenge (SSC) corpus show that the proposed learning approach can achieve a better generalization capability and a faster convergence than MMSE-based DNN learning. Furthermore, we demonstrate that the ML-trained DNN consistently outperforms MMSE-trained DNN in all the objective measures of speech quality and intelligibility in single-channel speech separation.

302.
TITLE: MixMax Approximation as a Super-Gaussian Log-Spectral Amplitude Estimator for Speech Enhancement
AUTHORS: R. Rehr, T. Gerkmann
YEAR: 2017
SOURCE: Interspeech 2017
ABSTRACT: For single-channel speech enhancement, most commonly, the noisy observation is described as the sum of the clean speech signal and the noise signal. For machine learning based enhancement schemes where speech and noise are modeled in the log-spectral domain, however, the log-spectrum of the noisy observation can be described as the maximum of the speech and noise log-spectrum to simplify statistical inference. This approximation is referred to as MixMax model or log-max approximation. In this paper, we show how this approximation can be used in combination with non-trained, blind speech and noise power estimators derived in the spectral domain. Our findings allow to interpret the MixMax based clean speech estimator as a super-Gaussian log-spectral amplitude estimator. This MixMax based estimator is embedded in a pre-trained speech enhancement scheme and compared to a log-spectral amplitude estimator based on an additive mixing model. Instrumental measures indicate that the MixMax based estimator causes less musical tones while it virtually yields the same quality for the enhanced speech signal.

303.
TITLE: A Comparison of Perceptually Motivated Loss Functions for Binary Mask Estimation in Speech Separation
AUTHORS: D. Websdale, B.P. Milner
YEAR: 2017
SOURCE: Interspeech 2017
ABSTRACT: This work proposes and compares perceptually motivated loss functions for deep learning based binary mask estimation for speech separation. Previous loss functions have focused on maximising classification accuracy of mask estimation but we now propose loss functions that aim to maximise the hit minus false-alarm (HIT-FA) rate which is known to correlate more closely to speech intelligibility. The baseline loss function is binary cross-entropy (CE), a standard loss function used in binary mask estimation, which maximises classification accuracy. We propose first a loss function that maximises the HIT-FA rate in- stead of classification accuracy. We then propose a second loss function that is a hybrid between CE and HIT-FA, providing a balance between classification accuracy and HIT-FA rate. Evaluations of the perceptually motivated loss functions with the GRID database show improvements to HIT-FA rate and ESTOI across babble and factory noises. Further tests then explore application of the perceptually motivated loss functions to a larger vocabulary dataset.

304.
TITLE: Improving Speech Recognition by Revising Gated Recurrent Units
AUTHORS: M. Ravanelli, P. Brakel, M. Omologo, Y. Bengio
YEAR: 2017
SOURCE: arXiv
ABSTRACT: Speech recognition is largely taking advantage of deep learning, showing that substantial benefits can be obtained by modern Recurrent Neural Networks (RNNs). The most popular RNNs are Long Short-Term Memory (LSTMs), which typically reach state-of-the-art performance in many tasks thanks to their ability to learn long-term dependencies and robustness to vanishing gradients. Nevertheless, LSTMs have a rather complex design with three multiplicative gates, that might impair their efficient implementation. An attempt to simplify LSTMs has recently led to Gated Recurrent Units (GRUs), which are based on just two multiplicative gates. This paper builds on these efforts by further revising GRUs and proposing a simplified architecture potentially more suitable for speech recognition. The contribution of this work is two-fold. First, we suggest to remove the reset gate in the GRU design, resulting in a more efficient single-gate architecture. Second, we propose to replace tanh with ReLU activations in the state update equations. Results show that, in our implementation, the revised architecture reduces the per-epoch training time with more than 30% and consistently improves recognition performance across different tasks, input features, and noisy conditions when compared to a standard GRU.

305.
TITLE: Improving Mask Learning Based Speech Enhancement System with Restoration Layers and Residual Connection
AUTHORS: Z. Chen, Y. Huang, J. Li, Y. Gong
YEAR: 2017
SOURCE: Interspeech 2017
ABSTRACT: For single-channel speech enhancement, mask learning based approach through neural network has been shown to outperform the feature mapping approach, and to be effective as a pre-processor for automatic speech recognition. However, its assumption that the mixture and clean reference must have the correspondent scale doesn’t hold in data collected from real world, and thus leads to significant performance degradation on parallel recorded data. In this paper, we first extend the mask learning based speech enhancement by integrating two types of restoration layer to address the scale mismatch problem. We further propose a novel residual learning based speech enhancement model via adding different shortcut connections to a feature mapping network. We show such a structure can benefit from both the mask learning and the feature mapping. We evaluate the proposed speech enhancement models on CHiME 3 data. Without retraining the acoustic model, the best bidirection LSTM with residue connections yields 24.90% relative WER reduction on real data and 34.57% WER on simulated data.

306.
TITLE: Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments
AUTHORS: Z. Zhang, et al.
YEAR: 2017
SOURCE: arXiv
ABSTRACT: Eliminating the negative effect of non-stationary environmental noise is a long-standing research topic for automatic speech recognition that stills remains an important challenge. Data-driven supervised approaches, including ones based on deep neural networks, have recently emerged as potential alternatives to traditional unsupervised approaches and with sufficient training, can alleviate the shortcomings of the unsupervised methods in various real-life acoustic environments. In this light, we review recently developed, representative deep learning approaches for tackling non-stationary additive and convolutional degradation of speech with the aim of providing guidelines for those involved in the development of environmentally robust speech recognition systems. We separately discuss single- and multi-channel techniques developed for the front-end and back-end of speech recognition systems, as well as joint front-end and back-end training frameworks.

307.
TITLE: Deep Learning Based Binaural Speech Separation in Reverberant Environments
AUTHORS: X. Zhang, D. Wang
YEAR: 2017
SOURCE: IEEE/ACM Transactions on Audio, Speech, and Language Processing
ABSTRACT: Speech signal is usually degraded by room reverberation and additive noises in real environments. This paper focuses on separating target speech signal in reverberant conditions from binaural inputs. Binaural separation is formulated as a supervised learning problem, and we employ deep learning to map from both spatial and spectral features to a training target. With binaural inputs, we first apply a fixed beamformer and then extract several spectral features. A new spatial feature is proposed and extracted to complement the spectral features. The training target is the recently suggested ideal ratio mask. Systematic evaluations and comparisons show that the proposed system achieves very good separation performance and substantially outperforms related algorithms under challenging multisource and reverberant environments.

308.
TITLE: Improving the Perceptual Quality of Ideal Binary Masked Speech
AUTHORS: L. Lightburn, E.D. Sena, A.H. Moore, P. Naylor, M. Brookes
YEAR: 2017
SOURCE: 2017 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: It is known that applying a time-frequency binary mask to very noisy speech can improve its intelligibility but results in poor perceptual quality. In this paper we propose a new approach to applying a binary mask that combines the intelligibility gains of conventional binary masking with the perceptual quality gains of a classical speech enhancer. The binary mask is not applied directly as a time-frequency gain as in most previous studies. Instead, the mask is used to supply prior information to a classical speech enhancer about the probability of speech presence in different time-frequency regions. Using an oracle ideal binary mask, we show that the proposed method results in a higher predicted quality than other methods of applying a binary mask whilst preserving the improvements in predicted intelligibility.

309.
TITLE: A Two-Stage Algorithm for Noisy and Reverberant Speech Enhancement
AUTHORS: Y. Zhao, Z. Wang, D. Wang
YEAR: 2017
SOURCE: 2017 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: In daily listening environments, speech is commonly corrupted by room reverberation and background noise. These distortions are detrimental to speech intelligibility and quality, and also severely degrade the performance of automatic speech and speaker recognition systems. In this paper, we propose a two-stage algorithm to deal with the confounding effects of noise and reverberation separately, where denoising and dereverberation are conducted sequentially using deep neural networks. In addition, we design a new objective function that incorporates clean phase information during training. As the objective function emphasizes more important time-frequency (T-F) units, better estimated magnitude is obtained during testing. By jointly training the two-stage model to optimize the proposed objective function, our algorithm improves objective metrics of speech intelligibility and quality significantly, and substantially outperforms one-stage enhancement baselines.

310.
TITLE: Impact of Low-Precision Deep Regression Networks on Single-Channel Source Separation
AUTHORS: E. Ceolini, S.C. Liu
YEAR: 2017
SOURCE: 2017 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: Recent work on developing training methods for reduced precision Deep Convolutional Networks show that these networks can perform with similar accuracy to full precision networks when tested on a classification task. Reduced precision networks decrease the demand on the memory and computational power capabilities of the computing platform. This paper investigates the impact of reduced precision deep Recurrent Neural Networks (RNNs) when trained on a regression task, in this case, a monaural source separation task. The effect of reduced precision nets is explored for two popular recurrent network architectures: Vanilla RNNs and RNNs using Long-Short Term Memory (LSTM) units. The results show that the performance of the networks as measured by blind source separation metrics and speech intelligibility tests on two datasets, show very little decrease even when the weight precision goes down to 4 bits.

311.
TITLE: Recurrent Deep Stacking Networks for Supervised Speech Separation
AUTHORS: Z. Wang, D. Wang
YEAR: 2017
SOURCE: 2017 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: Supervised speech separation algorithms seldom utilize output patterns. This study proposes a novel recurrent deep stacking approach for time-frequency masking based speech separation, where the output context is explicitly employed to improve the accuracy of mask estimation. The key idea is to incorporate the estimated masks of several previous frames as additional inputs to better estimate the mask of the current frame. Rather than formulating it as a recurrent neural network (RNN), which is potentially much harder to train, we propose to train a deep neural network (DNN) with implicit deep stacking. The estimated masks of the previous frames are updated only at the end of each DNN training epoch, and then the updated estimated masks provide additional inputs to train the DNN in the next epoch. At the test stage, the DNN makes predictions sequentially in a recurrent fashion. In addition, we propose to use the L1 loss for training. Experiments on the CHiME-2 (task-2) dataset demonstrate the effectiveness of our proposed approach.

312.
TITLE: Multiple-Target Deep Learning for LSTM-RNN Based Speech Enhancement
AUTHORS: L. Sun, J. Du, L.R. Dai, C. Lee
YEAR: 2017
SOURCE: 2017 Hands-free Speech Communications and Microphone Arrays 
ABSTRACT: In this study, we explore long short-term memory recurrent neural networks (LSTM-RNNs) for speech enhancement. First, a regression LSTM-RNN approach for a direct mapping from the noisy to clean speech features is presented and verified to be more effective than deep neural network (DNN) based regression techniques in modeling long-term acoustic context. Then, a comprehensive comparison between the proposed direct mapping based LSTM-RNN and ideal ratio mask (IRM) based LSTM-RNNs is conducted. We observe that the direct mapping framework achieves better speech intelligibility at low signal-to-noise ratios (SNRs) while the IRM approach shows its superiority at high SNRs. Accordingly, to fully utilize this complementarity, a novel multiple-target joint learning approach is designed. The experiments under unseen noises show that the proposed framework can consistently and significantly improve the objective measures for both speech quality and intelligibility.

313. 
TITLE: Supervised Independent Vector Analysis Through Pilot Dependent Components
AUTHORS: F. Nesta, Z. Koldovsky
YEAR: 2017
SOURCE: 2017 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: Unknown global permutation of the separated sources, time-varying source activity and under determination are common problems affecting on-line Independent Vector Analysis when applied to real-world speech enhancement. In this work we propose to extend the signal model of IVA by introducing additional supervising components. Pilot signals, which are dependent on the sources, are injected in the multidimensional source representation and act as a prior knowledge. The resulting adaptation still maximizes the multivariate source independence, while simultaneously forcing the estimation of sources dependent on the pilot components. It is also shown as the S-IVA is a generalization over the previously proposed weighted Natural Gradient. Numerical evaluations shows the effectiveness of the proposed method in challenging real-world applications.

314.
TITLE: Deep Clustering and Conventional Networks for Music Separation: Stronger Together
AUTHORS: Y. Luo, Z. Chen, J. Hershey, J. Le Roux, N. Mesgarani
YEAR: 2017
SOURCE: 2017 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: Deep clustering is the first method to handle general audio separation scenarios with multiple sources of the same type and an arbitrary number of sources, performing impressively in speaker-independent speech separation tasks. However, little is known about its effectiveness in other challenging situations such as music source separation. Contrary to conventional networks that directly estimate the source signals, deep clustering generates an embedding for each time-frequency bin, and separates sources by clustering the bins in the embedding space. We show that deep clustering outperforms conventional networks on a singing voice separation task, in both matched and mismatched conditions, even though conventional networks have the advantage of end-to-end training for best signal approximation, presumably because its more flexible objective engenders better regularization. Since the strengths of deep clustering and conventional network architectures appear complementary, we explore combining them in a single hybrid network trained via an approach akin to multi-task learning. Remarkably, the combination significantly outperforms either of its components.

315.
TITLE: Discriminative Enhancement for Single Channel Audio Source Separation Using Deep Neural Networks
AUTHORS: E.M. Grais, G. Roma, A.J.R. Simpson, M.D. Plumbley
YEAR: 2017
SOURCE: arXiv
ABSTRACT: The sources separated by most single channel audio source separation techniques are usually distorted and each separated source contains residual signals from the other sources. To tackle this problem, we propose to enhance the separated sources to decrease the distortion and interference between the separated sources using deep neural networks (DNNs). Two different DNNs are used in this work. The first DNN is used to separate the sources from the mixed signal. The second DNN is used to enhance the separated signals. To consider the interactions between the separated sources, we propose to use a single DNN to enhance all the separated sources together. To reduce the residual signals of one source from the other separated sources (interference), we train the DNN for enhancement discriminatively to maximize the dissimilarity between the predicted sources. The experimental results show that using discriminative enhancement decreases the distortion and interference between the separated sources.

316.
TITLE: Deep Recurrent Networks for Separation and Recognition of Single-Channel Speech in Nonstationary Background Audio
AUTHORS: H. Erdogan, J. Hershey, S. Watanabe, J. Le Roux
YEAR: 2017
SOURCE: New Era for Robust Speech Recognition, Exploiting Deep Learning
ABSTRACT: We investigate the use of deep neural networks and deep recurrent neural networks for separation and recognition of speech in challenging environments. Mask prediction networks received considerable interest recently for speech separation and speech enhancement problems where the background signals are nonstationary and challenging. Initial signal-level enhancement with deep neural networks has also been shown to be useful for noise-robust speech recognition in these environments. We consider using various loss functions for training the networks and illustrate differences among them. We compare the performance of deep computational architectures with conventional statistical techniques as well as variants of nonnegative matrix factorization, and establish that one can achieve impressively superior results with deep-learning-based techniques on this problem.

317.
TITLE: Sound Signal Processing Based on Seq 2 Tree Network
AUTHORS: W. Ma, K. Cao, Z. Ni, X. Ni, S. Chin
YEAR: 2017
SOURCE:
ABSTRACT: Most state-of-the-art solutions to sound signal processing tasks such as the speech and noise separation task and the music style classification task are based on Recurrent Neural Network (RNN) architecture or Hidden Markov Model (HMM). Both RNN and HMM assume that the input is chain-structured so that each element in the chain is equally dependent on all its previous units. However in real-life scenes the units alone do not carry much meaning. Only when several units group to be segments will they be semantically informative. This characteristic of sound signals clearly prefers emphasizing dependencies among units in the same segment, which leads to a natural selection of tree-structured models instead of chain-structured ones. In this paper we introduce Seq2Tree network and two models based on Seq2Tree architecture solving 1) speech and noise separation task and 2) music style classification task, respectively. Experiments show that our Seq2Tree-based models outperform the state-of-the-art systems in both tasks, which agrees with our hypothesis that sound signals have potential tree-structured dependencies among their sound elements. Also the experiment results prove the advancement of the Seq2Tree network architecture in sound signal processing tasks.

318.
TITLE: Variational Recurrent Neural Networks for Speech Separation
AUTHORS: J.T. Chien, K.T. Kuo
YEAR: 2017
SOURCE: Interspeech 2017
ABSTRACT: We present a new stochastic learning machine for speech separation based on the variational recurrent neural network (VRNN). This VRNN is constructed from the perspectives of generative stochastic network and variational auto-encoder. The idea is to faithfully characterize the randomness of hidden state of a recurrent neural network through variational learning. The neural parameters under this latent variable model are estimated by maximizing the variational lower bound of log marginal likelihood. An inference network driven by the variational distribution is trained from a set of mixed signals and the associated source targets. A novel supervised VRNN is developed for speech separation. The proposed VRNN provides a stochastic point of view which accommodates the uncertainty in hidden states and facilitates the analysis of model construction. The masking function is further employed in network outputs for speech separation. The benefit of using VRNN is demonstrated by the experiments on monaural speech separation.

319.
TITLE: Speech Enhancement Using Bayesian Wavenet
AUTHORS: K. Qian, et al.
YEAR: 2017
SOURCE: Interspeech 2017
ABSTRACT: In recent years, deep learning has achieved great success in speech enhancement. However, there are two major limitations regarding existing works. First, the Bayesian framework is not adopted in many such deep-learning-based algorithms. In particular, the prior distribution for speech in the Bayesian framework has been shown useful by regularizing the output to be in the speech space, and thus improving the performance. Second, the majority of the existing methods operate on the frequency domain of the noisy speech, such as spectrogram and its variations. The clean speech is then reconstructed using the approach of overlap-add, which is limited by its inherent performance upper bound. This paper presents a Bayesian speech enhancement framework, called BaWN (Bayesian WaveNet), which directly operates on raw audio samples. It adopts the recently announced WaveNet, which is shown to be effective in modeling conditional distributions of speech samples while generating natural speech. Experiments show that BaWN is able to recover clean and natural speech.

320.
TITLE: Combining Mask Estimates for Single Channel Audio Source Separation Using Deep Neural Networks
AUTHORS: E.M. Grais, G. Roma, A.J.R. Simpson, M.D. Plumbley
YEAR: 2016
SOURCE: Interspeech 2016
ABSTRACT: Deep neural networks (DNNs) are usually used for single channel source separation to predict either soft or binary time frequency masks. The masks are used to separate the sources from the mixed signal. Binary masks produce separated sources with more distortion and less interference than soft masks. In this paper, we propose to use another DNN to combine the estimates of binary and soft masks to achieve the advantages and avoid the disadvantages of using each mask individually. We aim to achieve separated sources with low distortion and low interference between each other. Our experimental results show that combining the estimates of binary and soft masks using DNN achieves lower distortion than using each estimate individually and achieves as low interference as the binary mask.

321.
TITLE: Improved MVDR Beamforming Using Single-Channel Mask Prediction Networks
AUTHORS: H. Erdogan, J. Hershey, S. Watanabe, M.I.Mandel, J. Le Roux
YEAR: 2016
SOURCE: Interspeech 2016
ABSTRACT: Recent studies on multi-microphone speech databases indicate that it is beneficial to perform beamforming to improve speech recognition accuracies, especially when there is a high level of background noise. Minimum variance distortionless response (MVDR) beamforming is an important beamforming method that performs quite well for speech recognition purposes especially if the steering vector is known. However, steering the beamformer to focus on speech in unknown acoustic conditions remains a challenging problem. In this study, we use singlechannel speech enhancement deep networks to form masks that can be used for noise spatial covariance estimation, which steers the MVDR beamforming toward the speech. We analyze how mask prediction affects performance and also discuss various ways to use masks to obtain the speech and noise spatial covariance estimates in a reliable way. We show that using a single mask across microphones for covariance prediction with minima-limited post-masking yields the best result in terms of signal-level quality measures and speech recognition word error rates in a mismatched training condition.

322.
TITLE: Multichannel Music Separation with Deep Neural Networks
AUTHORS: A. Nugraha, A. Liutkus, E. Vincent
YEAR: 2016
SOURCE: 2016 24th European Signal Processing Conference
ABSTRACT: This article addresses the problem of multichannel music separation. We propose a framework where the source spectra are estimated using deep neural networks and combined with spatial covariance matrices to encode the source spatial characteristics. The parameters are estimated in an iterative expectation-maximization fashion and used to derive a multichannel Wiener filter. We evaluate the proposed framework for the task of music separation on a large dataset. Experimental results show that the method we describe performs consistently well in separating singing voice and other instruments from realistic musical mixtures.

323.
TITLE: Monaural Speech Separation on Many Integrated Core Architecture
AUTHORS: W. He, X. Wei-xia, G. Naiyang, Y. Can-qun
YEAR: 2016
SOURCE: Communications in Computer and Information Science
ABSTRACT: Monaural speech separation is a challenging problem in practical audio analysis applications. Non-negative matrix factorization (NMF) is one of the most effective methods to solve this problem because it can learn meaningful features from a speech dataset in a supervised manner. Recently, a semi-supervised method, i.e., transductive NMF (TNMF), has shown great power to separate speeches from different individuals by incorporating both training and testing data in learning the dictionary. However, both NMF-based and TNMF-based monaural speech separation approaches have high computational complexity, and prohibit them from real-time processing. In this paper, we implement TNMF-based monaural speech separation on many integrated core (MIC) architecture to meet the requirement of real-time speech separation. This approach conducts parallelism based on the OpenMP technology, and performs the computing intensitive matrix manipulations on a MIC coprocessor. The experimental results confirm the efficiency of our implementation of monaural speech separation on MIC architecture.

324.
TITLE: Discriminatively Trained Recurrent Neural Networks for Continuous Dimensional Emotion Recognition from Audio
AUTHORS: F. Weninger, F. Ringeval, E. Marchi, B. Schuller
YEAR: 2016
SOURCE: IJCAI 2016
ABSTRACT: Continuous dimensional emotion recognition from audio is a sequential regression problem, where the goal is to maximize correlation between sequences of regression outputs and continuous-valued emotion contours, while minimizing the average deviation. As in other domains, deep neural networks trained on simple acoustic features achieve good performance on this task. Yet, the usual squared error objective functions for neural network training do not fully take into account the above-named goal. Hence, in this paper we introduce a technique for the discriminative training of deep neural networks using the concordance correlation coefficient as cost function, which unites both correlation and mean squared error in a single differentiable function. Results on the MediaEval 2013 and AV+EC 2015 Challenge data sets show that the proposed method can significantly improve the evaluation criteria compared to standard mean squared error training, both in the music and speech domains.

325.
TITLE: Fusion Methods for Speech Enhancement and Audio Source Separation
AUTHORS: X. Jaureguiberry, E. Vincent, G. Richard
YEAR: 2016
SOURCE: IEEE/ACM Transactions on Audio, Speech, and Language Processing
ABSTRACT: A wide variety of audio source separation techniques exist and can already tackle many challenging industrial issues. However, in contrast with other application domains, fusion principles were rarely investigated in audio source separation despite their demonstrated potential in classification tasks. In this paper, we propose a general fusion framework which takes advantage of the diversity of existing separation techniques in order to improve separation quality. We obtain new source estimates by summing the individual estimates given by different separation techniques weighted by a set of fusion coefficients. We investigate three alternative fusion methods which are based on standard nonlinear optimization, Bayesian model averaging, or deep neural networks. Experiments conducted for both speech enhancement and singing voice extraction demonstrate that all the proposed methods outperform traditional model selection. The use of deep neural networks for the estimation of time-varying coefficients notably leads to large quality improvements, up to 3 dB in terms of signal-to-distortion ratio compared to model selection.

326.
TITLE: Group Delay Based Music Source Separation Using Deep Recurrent Neural Networks
AUTHORS: J. Sebastian, H.A. Murphy
YEAR: 2016
SOURCE: 2016 International Conference on Signal Processing and Communications
ABSTRACT: Deep Recurrent Neural Networks (DRNNs) have been most successfully used in solving the challenging task of separating sources from a single channel acoustic mixture. Conventionally, magnitude spectra are being used to learn the characteristics of individual sources in such monaural blind source separation (BSS) task. The phase spectra which inherently contain the timing information is often ignored. In this work, we explore the use of modified group delay (MOD-GD) function for learning the time-frequency masks of the sources in the monaural BSS problem. We demonstrate the use of MOD-GD through two music source separation tasks: singing voice separation on the MIR-1K data set and vocal-violin separation on the Carnatic music data set. We find that it outperforms the state-of-the-art feature in terms of Signal to Interference Ratio (SIR). Moreover, training and testing times are significantly reduced (by 50%) without compromising on the performance for the best performing DRNN configuration.

327.
TITLE: Single Channel Audio Source Separation using Deep Neural Network Ensembles
AUTHORS: E.M. Graise, G. Roma, A.J.R. Simpson, M.D. Plumbley
YEAR: 2016
SOURCE: Journal of The Audio Engineering Society
ABSTRACT: Deep neural networks (DNNs) are often used to tackle the single channel source separation (SCSS) problem by predicting time-frequency masks. The predicted masks are then used to separate the sources from the mixed signal. Different types of masks produce separated sources with different levels of distortion and interference. Some types of masks produce separated sources with low distortion, while other masks produce low interference between the separated sources. In this paper, a combination of different DNNs’ predictions (masks) is used for SCSS to achieve better quality of the separated sources than using each DNN individually. We train four different DNNs by minimizing four different cost functions to predict four different masks. The first and second DNNs are trained to approximate reference binary and soft masks. The third DNN is trained to predict a mask from the reference sources directly. The last DNN is trained similarly to the third DNN but with an additional discriminative constraint to maximize the differences between the estimated sources. Our experimental results show that combining the predictions of different DNNs achieves separated sources with better quality than using each DNN individually.

328.
TITLE: A Deep Ensemble Learning Method for Monaural Speech Separation
AUTHORS: X. Zhang, D. Wang
YEAR: 2106
SOURCE: IEEE/ACM Transactions on Audio, Speech, and Language Processing
ABSTRACT: Monaural speech separation is a fundamental problem in robust speech processing. Recently, deep neural network (DNN)-based speech separation methods, which predict either clean speech or an ideal time-frequency mask, have demonstrated remarkable performance improvement. However, a single DNN with a given window length does not leverage contextual information sufficiently, and the differences between the two optimization objectives are not well understood. In this paper, we propose a deep ensemble method, named multicontext networks, to address monaural speech separation. The first multicontext network averages the outputs of multiple DNNs whose inputs employ different window lengths. The second multicontext network is a stack of multiple DNNs. Each DNN in a module of the stack takes the concatenation of original acoustic features and expansion of the soft output of the lower module as its input, and predicts the ratio mask of the target speaker; the DNNs in the same module employ different contexts. We have conducted extensive experiments with three speech corpora. The results demonstrate the effectiveness of the proposed method. We have also compared the two optimization objectives systematically and found that predicting the ideal time-frequency mask is more efficient in utilizing clean training speech, while predicting clean speech is less sensitive to SNR variations.

329.
TITLE: A Joint Training Framework for Robust Automatic Speech Recognition
AUTHORS: Z. Wang, D. Wang
YEAR: 2016
SOURCE: IEEE/ACM Transactions on Audio, Speech, and Language Processing
ABSTRACT: Robustness against noise and reverberation is critical for ASR systems deployed in real-world environments. In robust ASR, corrupted speech is normally enhanced using speech separation or enhancement algorithms before recognition. This paper presents a novel joint training framework for speech separation and recognition. The key idea is to concatenate a deep neural network (DNN) based speech separation frontend and a DNN-based acoustic model to build a larger neural network, and jointly adjust the weights in each module. This way, the separation frontend is able to provide enhanced speech desired by the acoustic model and the acoustic model can guide the separation frontend to produce more discriminative enhancement. In addition, we apply sequence training to the jointly trained DNN so that the linguistic information contained in the acoustic and language models can be back-propagated to influence the separation frontend at the training stage. To further improve the robustness, we add more noise- and reverberation-robust features for acoustic modeling. At the test stage, utterance-level unsupervised adaptation is performed to adapt the jointly trained network by learning a linear transformation of the input of the separation frontend. The resulting sequence-discriminative jointly-trained multistream system with run-time adaptation achieves 10.63% average word error rate (WER) on the test set of the reverberant and noisy CHiME-2 dataset (task-2), which represents the best performance on this dataset and a 22.75% error reduction over the best existing method.

330.
TITLE: Discriminative Deep Recurrent Neural Networks for Monaural Speech Separation
AUTHORS: G.X. Wang, C.C. Hsu, J.T. Chien
YEAR: 2016
SOURCE: 2016 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: Deep neural network is now a new trend towards solving different problems in speech processing. In this paper, we propose a discriminative deep recurrent neural network (DRNN) model for monaural speech separation. Our idea is to construct DRNN as a regression model to discover the deep structure and regularity for signal reconstruction from a mixture of two source spectra. To reinforce the discrimination capability between two separated spectra, we estimate DRNN separation parameters by minimizing an integrated objective function which consists of two measurements. One is the within-source reconstruction errors due to the individual source spectra while the other conveys the discrimination information which preserves the mutual difference between two source spectra during the supervised training procedure. This discrimination information acts as a kind of regularization so as to maintain between-source separation in monaural source separation. In the experiments, we demonstrate the effectiveness of the proposed method for speech separation compared with the other methods.

331.
TITLE: Exploiting Spectro-Temporal Structures Using NMF for DNN-Based Supervised Speech Separation
AUTHORS: S. Nie, et al.
YEAR: 2016
SOURCE: 2016 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: The targets of speech separation, whether ideal masks or magnitude spectrograms of interest, have prominent spectro-temporal structures. These characteristics are very worthy to be exploited for speech separation, however, they are usually ignored in previous works. In this paper, we use nonnegative matrix factorization (NMF) to exploit the spectro-temporal structures of magnitude spectrograms. With nonnegative constrains, NMF can capture the basis spectra patterns of speech and noise. Then the learned basis spectra are integrated into a deep neural network (DNN) to reconstruct the magnitude spectrograms of speech and noise with their nonnegative linear combination. Using the reconstructed spectrograms, we further explore a discriminative training objective and a joint optimization framework for the proposed model. Systematic experiments show that the proposed model is competitive with the previous methods in monaural speech separation tasks.

332. 
TITLE: Phoneme-Specific Speech Separation
AUTHORS: Z. Wang, Y. Zhao, D. Wang
YEAR: 2016
SOURCE: 2016 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: Speech separation or enhancement algorithms seldom exploit information about phoneme identities. In this study, we propose a novel phoneme-specific speech separation method. Rather than training a single global model to enhance all the frames, we train a separate model for each phoneme to process its corresponding frames. A robust ASR system is employed to identify the phoneme identity of each frame. This way, the information from ASR systems and language models can directly influence speech separation by selecting a phoneme-specific model to use at the test stage. In addition, phoneme-specific models have fewer variations to model and do not exhibit the data imbalance problem. The improved enhancement results can in turn help recognition. Experiments on the corpus of the second CHiME speech separation and recognition challenge (task-2) demonstrate the effectiveness of this method in terms of objective measures of speech intelligibility and quality, as well as recognition performance.

333.
TITLE: A Single Microphone Noise Reduction Algorithm Based on the Detection and Reconstruction of Spectro-Temporal Features
AUTHORS: T. Lee, F.E. Theunissen
YEAR: 2015
SOURCE: Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences
ABSTRACT: Animals throughout the animal kingdom excel at extracting individual sounds from competing background sounds, yet current state-of-the-art signal processing algorithms struggle to process speech in the presence of even modest background noise. Recent psychophysical experiments in humans and electrophysiological recordings in animal models suggest that the brain is adapted to process sounds within the restricted domain of spectro-temporal modulations found in natural sounds. Here, we describe a novel single microphone noise reduction algorithm called spectro-temporal detection–reconstruction (STDR) that relies on an artificial neural network trained to detect, extract and reconstruct the spectro-temporal features found in speech. STDR can significantly reduce the level of the background noise while preserving the foreground speech quality and improving estimates of speech intelligibility. In addition, by leveraging the strong temporal correlations present in speech, the STDR algorithm can also operate on predictions of upcoming speech features, retaining similar performance levels while minimizing inherent throughput delays. STDR performs better than a competing state-of-the-art algorithm for a wide range of signal-to-noise ratios and has the potential for real-time applications such as hearing aids and automatic speech recognition.

334.
TITLE: Robust ASR Using Neural Network Based Speech Enhancement and Feature Simulation
AUTHORS: S. Sivsankaran, et al.
YEAR: 2015
SOURCE: 2015 IEEE Workshop on Automatic Speech Recognition and Understanding
ABSTRACT: We consider the problem of robust automatic speech recognition (ASR) in the context of the CHiME-3 Challenge. The proposed system combines three contributions. First, we propose a deep neural network (DNN) based multichannel speech enhancement technique, where the speech and noise spectra are estimated using a DNN based regressor and the spatial parameters are derived in an expectation-maximization (EM) like fashion. Second, a conditional restricted Boltzmann machine (CRBM) model is trained using the obtained enhanced speech and used to generate simulated training and development datasets. The goal is to increase the similarity between simulated and real data, so as to increase the benefit of multicondition training. Finally, we make some changes to the ASR backend. Our system ranked 4th among 25 entries.

335.
TITLE: Complex Recurrent Neural Networks for Denoising Speech Signals
AUTHORS: K. Osako, R. Singh, B. Raj
YEAR: 2015
SOURCE: 2015 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics 
ABSTRACT: Effective denoising of noise-corrupted speech signals remains a challenging problem. Existing solutions typically employ some combination of noise estimation and noise elimination, either by subtraction or by filtering. The estimation of noise and the denoising are generally treated as independent aspects of the problem. In this paper we propose a new neural-network-based approach for de-noising of speech signals. The approach integrates noise estimation and denoising into a single network design, while maintaining many of the aspects of conventional noise estimation and signal denoising through a recurrent gated structure. The network thus operates as a single integrated process that can be trained to jointly estimate noise and denoise the speech signal with minimal artifacts. Noise reduction experiments on noisy speech, both with digitally added synthetic noise and real car noise, show that the proposed algorithm can recover much of the degradation caused by the noise.

336.
TITLE: Speech Enhancement and Recognition Using Multi-Task Learning of Long Short-Term Memory Recurrent Neural Networks
AUTHORS: Z. Chen, S. Watanabe, H. Erdogan, J. Hershey
YEAR: 2015
SOURCE: Interspeech 2015
ABSTRACT: Long Short-Term Memory (LSTM) recurrent neural network has proven effective in modeling speech and has achieved outstanding performance in both speech enhancement (SE) and automatic speech recognition (ASR). To further improve the performance of noise-robust speech recognition, a combination of speech enhancement and recognition was shown to be promising in earlier work. This paper aims to explore options for consistent integration of SE and ASR using LSTM networks. Since SE and ASR have different objective criteria, it is not clear what kind of integration would finally lead to the best word error rate for noise-robust ASR tasks. In this work, several integration architectures are proposed and tested, including: (1) a pipeline architecture of LSTM-based SE and ASR with sequence training, (2) an alternating estimation architecture, and (3) a multi-task hybrid LSTM network architecture. The proposed models were evaluated on the 2nd CHiME speech separation and recognition challenge task, and show significant improvements relative to prior results.

337.
TITLE: Speech Enhancement with LSTM Recurrent Neural Networks and its Application to Noise-Robust ASR
AUTHORS: F. Weninger, et al.
YEAR: 2015
SOURCE: LVA/ICA 2015
ABSTRACT: We evaluate some recent developments in recurrent neural network RNN based speech enhancement in the light of noise-robust automatic speech recognition ASR. The proposed framework is based on Long Short-Term Memory LSTM RNNs which are discriminatively trained according to an optimal speech reconstruction objective. We demonstrate that LSTM speech enhancement, even when used 'naively' as front-end processing, delivers competitive results on the CHiME-2 speech recognition task. Furthermore, simple, feature-level fusion based extensions to the framework are proposed to improve the integration with the ASR back-end. These yield a best result of 13.76% average word error rate, which is, to our knowledge, the best score to date.

338.
TITLE: Deep NMF for Speech Separation
AUTHORS: J. Le Roux, J. Hershey, F. Weninger
YEAR: 2015
SOURCE: 2015 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: Non-negative matrix factorization (NMF) has been widely used for challenging single-channel audio source separation tasks. However, inference in NMF-based models relies on iterative inference methods, typically formulated as multiplicative updates. We propose “deep NMF”, a novel non-negative deep network architecture which results from unfolding the NMF iterations and untying its parameters. This architecture can be discriminatively trained for optimal separation performance. To optimize its non-negative parameters, we show how a new form of back-propagation, based on multiplicative updates, can be used to preserve non-negativity, without the need for constrained optimization. We show on a challenging speech separation task that deep NMF improves in terms of accuracy upon NMF and is competitive with conventional sigmoid deep neural networks, while requiring a tenth of the number of parameters.

339.
TITLE: Source Separation with Scattering Non-Negative Matrix Factorization
AUTHORS: J. Bruna, P. Sprechmann, Y. LeCun
YEAR: 2015
SOURCE: 2015 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: This paper presents a single-channel source separation method that extends the ideas of Nonnegative Matrix Factorization (NMF). We interpret the approach of audio demixing via NMF as a cascade of a pooled analysis operator, given for example by the magnitude spectrogram, and a synthesis operators given by the matrix decomposition. Instead of imposing the temporal consistency of the decomposition through sophisticated structured penalties in the synthesis stage, we propose to change the analysis operator for a deep scattering representation, where signals are represented at several time resolutions. This new signal representation is invariant to smooth changes in the signal, consistent with its temporal dynamics. We evaluate the proposed approach in a speech separation task obtaining promising results.

340.
TITLE: Phase-Sensitive and Recognition-Boosted Speech Separation Using Deep Recurrent Neural Networks
AUTHORS: H. Erdogan, J. Hershey, S. Watanabe, J. Le Roux
YEAR: 2015
SOURCE: 2015 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: Separation of speech embedded in non-stationary interference is a challenging problem that has recently seen dramatic improvements using deep network-based methods. Previous work has shown that estimating a masking function to be applied to the noisy spectrum is a viable approach that can be improved by using a signal-approximation based objective function. Better modeling of dynamics through deep recurrent networks has also been shown to improve performance. Here we pursue both of these directions. We develop a phase-sensitive objective function based on the signal-to-noise ratio (SNR) of the reconstructed signal, and show that in experiments it yields uniformly better results in terms of signal-to-distortion ratio (SDR). We also investigate improvements to the modeling of dynamics, using bidirectional recurrent networks, as well as by incorporating speech recognition outputs in the form of alignment vectors concatenated with the spectral input features. Both methods yield further improvements, pointing to tighter integration of recognition with separation as a promising future direction.

341.
TITLE: Joint Optimization of Masks and Deep Recurrent Neural Networks for Monaural Source Separation
AUTHORS: P. Huang, M. Kim, M. Hasegawa-Johnson, P. Smaragdis
YEAR: 2015
SOURCE: IEEE/ACM Transactions on Audio, Speech, and Language Processing
ABSTRACT: Monaural source separation is important for many real world applications. It is challenging because, with only a single channel of information available, without any constraints, an infinite number of solutions are possible. In this paper, we explore joint optimization of masking functions and deep recurrent neural networks for monaural source separation tasks, including speech separation, singing voice separation, and speech denoising. The joint optimization of the deep recurrent neural networks with an extra masking layer enforces a reconstruction constraint. Moreover, we explore a discriminative criterion for training neural networks to further enhance the separation performance. We evaluate the proposed system on the TSP, MIR-1K, and TIMIT datasets for speech separation, singing voice separation, and speech denoising tasks, respectively. Our approaches achieve 2.30-4.98 dB SDR gain compared to NMF models in the speech separation task, 2.30-2.48 dB GNSDR gain and 4.32-5.42 dB GSIR gain compared to existing models in the singing voice separation task, and outperform NMF and DNN baselines in the speech denoising task.

342.
TITLE: Latent Variable Analysis and Signal Separation
AUTHORS: J. Kittler, J. Mitchell, M. Naor
YEAR: 2015
SOURCE: LVA/ICA 2015
ABSTRACT: Given an instantaneous mixture of some source signals, the blind signal separation (BSS) problem consists of the identification of both the mixing matrix and the original sources. By itself, it is a non-unique matrix factorization problem, while unique solutions can be obtained by imposing additional assumptions such as statistical independence. By mapping the matrix data to a tensor and by using tensor decompositions afterwards, uniqueness is ensured under certain conditions. Tensor decompositions have been studied thoroughly in literature. We discuss the matrix to tensor step and present tensorization as an important concept on itself, illustrated by a number of stochastic and deterministic tensorization techniques.

343.
TITLE: Supervised Non-Negative Matrix Factorization for Audio Source Separation
AUTHORS: P. Sprechmann, A. Bronstein, G. Sapiro
YEAR: 2015
SOURCE: Applied and Numerical Harmonic Analysis
ABSTRACT: Source separation is a widely studied problem in signal processing. Despite the permanent progress reported in the literature it is still considered a significant challenge. This chapter first reviews the use of non-negative matrix factorization (NMF) algorithms for solving source separation problems, and proposes a new way for the supervised training in NMF. Matrix factorization methods have received a lot of attention in recent year in the audio processing community, producing particularly good results in source separation. Traditionally, NMF algorithms consist of two separate stages: a training stage, in which a generative model is learned; and a testing stage in which the pre-learned model is used in a high level task such as enhancement, separation, or classification. As an alternative, we propose a task-supervised NMF method for the adaptation of the basis spectra learned in the first stage to enhance the performance on the specific task used in the second stage. We cast this problem as a bilevel optimization program efficiently solved via stochastic gradient descent. The proposed approach is general enough to handle sparsity priors of the activations, and allow non-Euclidean data terms such as β-divergences. The framework is evaluated on speech enhancement.

344.
TITLE: Deep Neural Networks Based Binary Classification for Single Channel Speaker Independent Multi-Talker Speech Separation
AUTHORS: N. Saleem, M.I. Khattak
YEAR: 2020
SOURCE: Applied Acoustics
ABSTRACT: Speech separation is an important task of separating a target speech from the mixture signals. Speaker-independent multi-talker speech separation is a challenging task due to unpredictability of the target and interfering speech in the target-interference mixtures. Conventionally, speech separation is used as a signal processing problem, but recently it is formulated as a deep learning problem and discriminative patterns of the speech are learned from the training data. In this paper, we consider the ideal binary mask (IBM) as a supervised binary classification training-target by using fully connected deep neural networks (DNN) for single-channel speaker-independent multi-talker speech separation. The train DNNs is used to estimate IBM training-target. The mean square error (MSE) is used as an objective cost function. Standard backpropagation and Monte-Carlo dropout regularization approaches are used for better generalization and overfitting during training. The estimated training-target is applied to the mixtures to obtain the separated target speech. We have addressed the over-smoothing problem and performed equalization of spectral variances to match the estimated and clean speech features. Our experimental results in various evaluating conditions report that the proposed method outperformed the competing methods in terms of the Perceptual Evaluation of Speech Quality (PESQ), Segmental SNR (SNRSeg), Short-time objective intelligibility (STOI), normalized Frequency weighted SNRSeg (nFwSNRSeg) and HIT-FA rates.

345.
TITLE: Single-Channel Speech Separation with Auxiliary Speaker Embeddings
AUTHORS: S. Liu, G. Keren, B. Schuller
YEAR: 2019
SOURCE: arXiv
ABSTRACT: We present a novel source separation model to decompose asingle-channel speech signal into two speech segments belonging to two different speakers. The proposed model is a neural network based on residual blocks, and uses learnt speaker embeddings created from additional clean context recordings of the two speakers as input to assist in attributing the different time-frequency bins to the two speakers. In experiments, we show that the proposed model yields good performance in the source separation task, and outperforms the state-of-the-art baselines. Specifically, separating speech from the challenging VoxCeleb dataset, the proposed model yields 4.79dB signal-to-distortion ratio, 8.44dB signal-to-artifacts ratio and 7.11dB signal-to-interference ratio.

346.
TITLE: Spectrograms Fusion with Minimum Difference Masks Estimation for Monaural Speech Dereverberation
AUTHORS: H. Shi, L. Wang, M. Ge, S. Li, J. Du
YEAR: 2020
SOURCE: 2020 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: Spectrograms fusion is an effective method for incorporating complementary speech dereverberation systems. Previous linear spectrograms fusion by averaging multiple spectrograms shows outstanding performance. However, various systems with different features cannot apply this simple method. In this study, we design the minimum difference masks (MDMs) to classify the time-frequency (T-F) bins in spectrograms according to the nearest distances from labels. Then, we propose a two-stage nonlinear spectrograms fusion system for speech dereverberation. First, we conduct a multitarget learning-based speech dereverberation front-end model to get spectrograms simultaneously. Then, MDMs are estimated to take the best parts of different spectrograms. We are using spectrograms in the first stage and MDMs in the second stage to recombine T-F bins. The experiments on the REVERB challenge show that a strong feature complementarity between spectrograms and MDMs. Moreover, the proposed framework can consistently and significantly improve PESQ and SRMR, both real and simulated data, e.g., an average PESQ gain of 0.1 in all simulated data and an average SRMR gain of 1.22 in all real data.

347.
TITLE: Single Channel Speech Enhancement Using Temporal Convolutional Recurrent Neural Networks
AUTHORS: J. Li, H. Zhang, X. Zhang, C. Li
YEAR: 2019
SOURCE: 2019 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference
ABSTRACT: In recent decades, neural network based methods have significantly improved the performance of speech enhancement. Most of them estimate time-frequency (T-F) representation of target speech directly or indirectly, then resynthesize waveform using the estimated T-F representation. In this work, we proposed the temporal convolutional recurrent network (TCRN), an end-to-end model that directly map noisy waveform to clean waveform. The TCRN, which is combined convolution and recurrent neural network, is able to efficiently and effectively leverage short-term ang long-term information. Furthermore, we present the architecture that iterately downsample and upsample speech during forward propagation. We show that our model is able to improve the performance of model, compared with existing convolutional recurrent networks. Furthermore, We present several key techniques to stabilize the training process. The experimental results show that our model consistently outperforms existing speech enhancement approaches, in terms of speech intelligibility and quality.

348.
TITLE: Looking to Listen at the Cocktail Party
AUTHORS: A. Ephrat, et al.
YEAR: 2018
SOURCE: ACM Transactions on Graphics
ABSTRACT: We present a joint audio-visual model for isolating a single speech signal from a mixture of sounds such as other speakers and background noise. Solving this task using only audio as input is extremely challenging and does not provide an association of the separated speech signals with speakers in the video. In this paper, we present a deep network-based model that incorporates both visual and auditory signals to solve this task. The visual features are used to "focus" the audio on desired speakers in a scene and to improve the speech separation quality. To train our joint audio-visual model, we introduce AVSpeech, a new dataset comprised of thousands of hours of video segments from the Web. We demonstrate the applicability of our method to classic speech separation tasks, as well as real-world scenarios involving heated interviews, noisy bars, and screaming children, only requiring the user to specify the face of the person in the video whose speech they want to isolate. Our method shows clear advantage over state-of-the-art audio-only speech separation in cases of mixed speech. In addition, our model, which is speaker-independent (trained once, applicable to any speaker), produces better results than recent audio-visual speech separation methods that are speaker-dependent (require training a separate model for each speaker of interest).

349.
TITLE: Conv-TasNet: Surpassing Ideal Time–Frequency Magnitude Masking for Speech Separation
AUTHORS: Y. Luo, N. Mesgarani
YEAR: 2019
SOURCE: IEEE/ACM Transactions on Audio, Speech, and Language Processing
ABSTRACT: Single-channel, speaker-independent speech separation methods have recently seen great progress. However, the accuracy, latency, and computational cost of such methods remain insufficient. The majority of the previous methods have formulated the separation problem through the time–frequency representation of the mixed signal, which has several drawbacks, including the decoupling of the phase and magnitude of the signal, the suboptimality of time–frequency representation for speech separation, and the long latency in calculating the spectrograms. To address these shortcomings, we propose a fully convolutional time-domain audio separation network (Conv-TasNet), a deep learning framework for end-to-end time-domain speech separation. Conv-TasNet uses a linear encoder to generate a representation of the speech waveform optimized for separating individual speakers. Speaker separation is achieved by applying a set of weighting functions (masks) to the encoder output. The modified encoder representations are then inverted back to the waveforms using a linear decoder. The masks are found using a temporal convolutional network consisting of stacked one-dimensional dilated convolutional blocks, which allows the network to model the long-term dependencies of the speech signal while maintaining a small model size. The proposed Conv-TasNet system significantly outperforms previous time–frequency masking methods in separating two- and three-speaker mixtures. Additionally, Conv-TasNet surpasses several ideal time–frequency magnitude masks in two-speaker speech separation as evaluated by both objective distortion measures and subjective quality assessment by human listeners. Finally, Conv-TasNet has a significantly smaller model size and a shorter minimum latency, making it a suitable solution for both offline and real-time speech separation applications. This study, therefore, represents a major step toward the realization of speech separation systems for real-world speech processing technologies.

350.
TITLE: The Sound of Pixels
AUTHORS: H. Zhao, et al.
YEAR: 2018
SOURCE: arXiv
ABSTRACT: We introduce PixelPlayer, a system that, by leveraging large amounts of unlabeled videos, learns to locate image regions which produce sounds and separate the input sounds into a set of components that represents the sound from each pixel. Our approach capitalizes on the natural synchronization of the visual and audio modalities to learn models that jointly parse sounds and images, without requiring additional manual supervision. Experimental results on a newly collected MUSIC dataset show that our proposed Mix-and-Separate framework outperforms baseline approaches for grounding sounds into images. Several qualitative results suggest our model learns to ground sounds in vision, enabling applications such as independently adjusting the volume of sound sources.

351.
TITLE: The Conversation: Deep Audio-Visual Speech Enhancement
AUTHORS: T. Afouras, J.S. Chung, A. Zisserman
YEAR: 2018
SOURCE: arXiv
ABSTRACT: Our goal is to isolate individual speakers from multi-talker simultaneous speech in videos. Existing works in this area have focussed on trying to separate utterances from known speakers in controlled environments. In this paper, we propose a deep audio-visual speech enhancement network that is able to separate a speaker's voice given lip regions in the corresponding video, by predicting both the magnitude and the phase of the target signal. The method is applicable to speakers unheard and unseen during training, and for unconstrained environments. We demonstrate strong quantitative and qualitative results, isolating extremely challenging real-world examples.

352.
TITLE: Exploring Speech Enhancement with Generative Adversarial Networks for Robust Speech Recognition
AUTHORS: C. Donahue, B. Li, R. Prabhavalkar
YEAR: 2018
SOURCE: 2018 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: We investigate the effectiveness of generative adversarial networks (GANs) for speech enhancement, in the context of improving noise robustness of automatic speech recognition (ASR) systems. Prior work [1] demonstrates that GANs can effectively suppress additive noise in raw waveform speech signals, improving perceptual quality metrics; however this technique was not justified in the context of ASR. In this work, we conduct a detailed study to measure the effectiveness of GANs in enhancing speech contaminated by both additive and reverberant noise. Motivated by recent advances in image processing [2], we propose operating GANs on log-Mel filterbank spectra instead of waveforms, which requires less computation and is more robust to reverberant noise. While GAN enhancement improves the performance of a clean-trained ASR system on noisy speech, it falls short of the performance achieved by conventional multi-style training (MTR). By appending the GAN-enhanced features to the noisy inputs and retraining, we achieve a 7% WER improvement relative to the MTR system.

353.
TITLE: A Convolutional Recurrent Neural Network for Real-Time Speech Enhancement
AUTHORS: K. Tan, D. Wang
YEAR: 2018
SOURCE: Interspeech 2018
ABSTRACT: Many real-world applications of speech enhancement, such as hearing aids and cochlear implants, desire real-time processing, with no or low latency. In this paper, we propose a novel convolutional recurrent network (CRN) to address real-time monaural speech enhancement. We incorporate a convolutional encoder decoder (CED) and long short-term memory (LSTM) into the CRN architecture, which leads to a causal system that is naturally suitable for real-time processing. Moreover, the proposed model is noise and speaker-independent, i.e. noise types and speakers can be different between training and test. Our experiments suggest that the CRN leads to consistently better objective intelligibility and perceptual quality than an existing LSTM based model. Moreover, the CRN has much fewer trainable parameters.

354.
TITLE: Deep Neural Network Concepts for Background Subtraction: A Systematic Review and Comparative Evaluation
AUTHORS: T. Bouwmans, S. Javed, M. Sultana, S.K. Jung
YEAR: 2019
SOURCE: Neural Networks: The Official Journal of the International Neural Network Society
ABSTRACT: Conventional neural networks have been demonstrated to be a powerful framework for background subtraction in video acquired by static cameras. Indeed, the well-known Self-Organizing Background Subtraction (SOBS) method and its variants based on neural networks have long been the leading methods on the large-scale CDnet 2012 dataset during a long time. Convolutional neural networks, which are used in deep learning, have been recently and excessively employed for background initialization, foreground detection, and deep learned features. The top background subtraction methods currently used in CDnet 2014 are based on deep neural networks, and have demonstrated a large performance improvement in comparison to conventional unsupervised approaches based on multi-feature or multi-cue strategies. Furthermore, since the seminal work of Braham and Van Droogenbroeck in 2016, a large number of studies on convolutional neural networks applied to background subtraction have been published, and a continual gain of performance has been achieved. In this context, we provide the first review of deep neural network concepts in background subtraction for novices and experts in order to analyze this success and to provide further directions. To do so, we first surveyed the background initialization and background subtraction methods based on deep neural networks concepts, and also deep learned features. We then discuss the adequacy of deep neural networks for the task of background subtraction. Finally, experimental results are presented for the CDnet 2014 dataset.

355.
TITLE: Single Channel Target Speaker Extraction and Recognition with Speaker Beam
AUTHORS: M. Delcroix, K. Zmolikova, K. Kinoshita, A. Ogawa, T. Nakatani
YEAR: 2018
SOURCE: 2018 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: This paper addresses the problem of single channel speech recognition of a target speaker in a mixture of speech signals. We propose to exploit auxiliary speaker information provided by an adaptation utterance from the target speaker to extract and recognize only that speaker. Using such auxiliary information, we can build a speaker extraction neural network (NN) that is independent of the number of sources in the mixture, and that can track speakers across different utterances, which are two challenging issues occurring with conventional approaches for speech recognition of mixtures. We call such an informed speaker extraction scheme “SpeakerBeam”. SpeakerBeam exploits a recently developed context adaptive deep NN (CADNN) that allows tracking speech from a target speaker using a speaker adaptation layer, whose parameters are adjusted depending on auxiliary features representing the target speaker characteristics. SpeakerBeam was previously investigated for speaker extraction using a microphone array. In this paper, we demonstrate that it is also efficient for single channel speaker extraction. The speaker adaptation layer can be employed either to build a speaker adaptive acoustic model that recognizes only the target speaker or a mask-based speaker extraction network that extracts the target speech from the speech mixture signal prior to recognition. We also show that the latter speaker extraction network can be optimized jointly with an acoustic model to further improve ASR performance.

356.
TITLE: Deep Extractor Network for Target Speaker Recovery From Single Channel Speech Mixtures
AUTHORS: J. Wang, et al.
YEAR: 2018
SOURCE: Interspeech 2018
ABSTRACT: Speaker-aware source separation methods are promising workarounds for major difficulties such as arbitrary source permutation and unknown number of sources. However, it remains challenging to achieve satisfying performance provided a very short available target speaker utterance (anchor). Here we present a novel "deep extractor network" which creates an extractor point for the target speaker in a canonical high dimensional embedding space, and pulls together the time-frequency bins corresponding to the target speaker. The proposed model is different from prior works in that the canonical embedding space encodes knowledges of both the anchor and the mixture during an end-to-end training phase: First, embeddings for the anchor and mixture speech are separately constructed in a primary embedding space, and then combined as an input to feed-forward layers to transform to a canonical embedding space which we discover more stable than the primary one. Experimental results show that given a very short utterance, the proposed model can efficiently recover high quality target speech from a mixture, which outperforms various baseline models, with 5.2% and 6.6% relative improvements in SDR and PESQ respectively compared with a baseline oracle deep attracor model. Meanwhile, we show it can be generalized well to more than one interfering speaker.

357.
TITLE: Deep Learning Based Phase Reconstruction for Speaker Separation: A Trigonometric Perspective
AUTHORS: Z. Wang, K. Tan, D. Wang
YEAR: 2019
SOURCE: 2019 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: This study investigates phase reconstruction for deep learning based monaural talker-independent speaker separation in the short-time Fourier transform (STFT) domain. The key observation is that, for a mixture of two sources, with their magnitudes accurately estimated and under a geometric constraint, the absolute phase difference between each source and the mixture can be uniquely determined; in addition, the source phases at each time-frequency  unit can be narrowed down to only two candidates. To pick the right candidate, we propose three algorithms based on iterative phase reconstruction, group delay estimation, and phase-difference sign prediction. State-of-the-art results are obtained on the publicly available wsj0-2mix and 3 mix corpus.

358.
TITLE: Combining Spectral and Spatial Features for Deep Learning Based Blind Speaker Separation
AUTHORS: Z. Wang, D. Wang
YEAR: 2019
SOURCE: IEEE/ACM Transactions on Audio, Speech, and Language Processing
ABSTRACT: This study tightly integrates complementary spectral and spatial features for deep learning based multi-channel speaker separation in reverberant environments. The key idea is to localize individual speakers so that an enhancement network can be trained on spatial as well as spectral features to extract the speaker from an estimated direction and with specific spectral structures. The spatial and spectral features are designed in a way such that the trained models are blind to the number of microphones and microphone geometry. To determine the direction of the speaker of interest, we identify time-frequency (T-F) units dominated by that speaker and only use them for direction estimation. The T-F unit level speaker dominance is determined by a two-channel chimera++ network, which combines deep clustering and permutation invariant training at the objective function level, and integrates spectral and interchannel phase patterns at the input feature level. In addition, T-F masking based beamforming is tightly integrated in the system by leveraging the magnitudes and phases produced by beamforming. Strong separation performance has been observed on reverberant talker-independent speaker separation, which separates reverberant speaker mixtures based on a random number of microphones arranged in arbitrary linear-array geometry.

359.
TITLE: TCNN: Temporal Convolutional Neural Network for Real-time Speech Enhancement in the Time Domain
AUTHORS: A. Pandey, D. Wang
YEAR: 2019
SOURCE: 2019 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: This work proposes a fully convolutional neural network (CNN) for real-time speech enhancement in the time domain. The proposed CNN is an encoder-decoder based architecture with an additional temporal convolutional module (TCM) inserted between the encoder and the decoder. We call this architecture a Temporal Convolutional Neural Network (TCNN). The encoder in the TCNN creates a low dimensional representation of a noisy input frame. The TCM uses causal and dilated convolutional layers to utilize the encoder output of the current and previous frames. The decoder uses the TCM output to reconstruct the enhanced frame. The proposed model is trained in a speaker- and noise-independent way. Experimental results demonstrate that the proposed model gives consistently better enhancement results than a state-of-the-art real-time convolutional recurrent model. Moreover, since the model is fully convolutional, it has much fewer trainable parameters than earlier models.

360.
TITLE: Training Supervised Speech Separation System to Improve STOI and PESQ Directly
AUTHORS: H.B. Zhang, X. Zhang, G. Gao
YEAR: 2018
SOURCE: 2018 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: Supervised speech separation methods train learning machine to cast the noisy speech to the target clean speech. Most of them use mean-square error (MSE) as loss function. However, MSE is not the perfect choice because it doesn't match the human auditory perception. Short-time objective intelligibility (STOI) and perceptual evaluation of speech quality (PESQ) are closely related to the human auditory perception and widely used in speech separation research as evaluation criteria. Therefore, STOI and PESQ may be better choices for the loss function. However, they are nondifferentiable functions which cannot be optimized by the conventional gradient descent algorithm. In this work, a gradient approximation method is used to calculate the gradients of the STOI and PESQ. Then the calculated gradients are used in the gradient descent algorithm to optimize the STOI and PESQ directly. Experimental results show the speech separation performance can be improved by the proposed method.

361.
TITLE: Complex Ratio Masking for Monaural Speech Separation
AUTHORS: D.S. Williamson, Y. Wang, D. Wang
YEAR: 2016
SOURCE: IEEE/ACM Transactions on Audio, Speech, and Language Processing
ABSTRACT: Speech separation systems usually operate on the short-time Fourier transform (STFT) of noisy speech, and enhance only the magnitude spectrum while leaving the phase spectrum unchanged. This is done because there was a belief that the phase spectrum is unimportant for speech enhancement. Recent studies, however, suggest that phase is important for perceptual quality, leading some researchers to consider magnitude and phase spectrum enhancements. We present a supervised monaural speech separation approach that simultaneously enhances the magnitude and phase spectra by operating in the complex domain. Our approach uses a deep neural network to estimate the real and imaginary components of the ideal ratio mask defined in the complex domain. We report separation results for the proposed method and compare them to related systems. The proposed approach improves over other methods when evaluated with several objective metrics, including the perceptual evaluation of speech quality (PESQ), and a listening test where subjects prefer the proposed approach with at least a 69% rate.

362. 
TITLE: Single-Channel Multi-Speaker Separation Using Deep Clustering
AUTHORS: Y. Isik, J. Le Roux, Z. Chen, S. Watanabe, J. Hershey
YEAR: 2016
SOURCE: Interspeech 2016
ABSTRACT: Deep clustering is a recently introduced deep learning architecture that uses discriminatively trained embeddings as the basis for clustering. It was recently applied to spectrogram segmentation, resulting in impressive results on speaker-independent multi-speaker separation. In this paper we extend the baseline system with an end-to-end signal approximation objective that greatly improves performance on a challenging speech separation. We first significantly improve upon the baseline system performance by incorporating better regularization, larger temporal context, and a deeper architecture, culminating in an overall improvement in signal to distortion ratio (SDR) of 10.3 dB compared to the baseline of 6.0 dB for two-speaker separation, as well as a 7.1 dB SDR improvement for three-speaker separation. We then extend the model to incorporate an enhancement layer to refine the signal estimates, and perform end-to-end training through both the clustering and enhancement stages to maximize signal fidelity. We evaluate the results using automatic speech recognition. The new signal approximation objective, combined with end-to-end training, produces unprecedented performance, reducing the word error rate (WER) from 89.1% down to 30.8%. This represents a major advancement towards solving the cocktail party problem.

363.
TITLE: Multitalker Speech Separation With Utterance-Level Permutation Invariant Training of Deep Recurrent Neural Networks
AUTHORS: M. Kolbaek, D. Yu, Z. Tan, J. Jensen
YEAR: 2017
SOURCE: IEEE/ACM Transactions on Audio, Speech, and Language Processing
ABSTRACT: In this paper, we propose the utterance-level permutation invariant training (uPIT) technique. uPIT is a practically applicable, end-to-end, deep-learning-based solution for speaker independent multitalker speech separation. Specifically, uPIT extends the recently proposed permutation invariant training (PIT) technique with an utterance-level cost function, hence eliminating the need for solving an additional permutation problem during inference, which is otherwise required by frame-level PIT. We achieve this using recurrent neural networks (RNNs) that, during training, minimize the utterance-level separation error, hence forcing separated frames belonging to the same speaker to be aligned to the same output stream. In practice, this allows RNNs, trained with uPIT, to separate multitalker mixed speech without any prior knowledge of signal duration, number of speakers, speaker identity, or gender. We evaluated uPIT on the WSJ0 and Danish two- and three-talker mixed-speech separation tasks and found that uPIT outperforms techniques based on nonnegative matrix factorization and computational auditory scene analysis, and compares favorably with deep clustering, and the deep attractor network. Furthermore, we found that models trained with uPIT generalize well to unseen speakers and languages. Finally, we found that a single model, trained with uPIT, can handle both two-speaker, and three-speaker speech mixtures.

364.
TITLE: TaSNet: Time-Domain Audio Separation Network for Real-Time, Single-Channel Speech Separation
AUTHORS: Y. Luo, N. Mesgarani
YEAR: 2018
SOURCE: 2018 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: Robust speech processing in multi-talker environments requires effective speech separation. Recent deep learning systems have made significant progress toward solving this problem, yet it remains challenging particularly in real-time, short latency applications. Most methods attempt to construct a mask for each source in time-frequency representation of the mixture signal which is not necessarily an optimal representation for speech separation. In addition, time-frequency decomposition results in inherent problems such as phase/magnitude decoupling and long time window which is required to achieve sufficient frequency resolution. We propose Time-domain Audio Separation Network (TasNet) to overcome these limitations. We directly model the signal in the time-domain using an encoder-decoder framework and perform the source separation on nonnegative encoder outputs. This method removes the frequency decomposition step and reduces the separation problem to estimation of source masks on encoder outputs which is then synthesized by the decoder. Our system outperforms the current state-of-the-art causal and noncausal speech separation algorithms, reduces the computational cost of speech separation, and significantly reduces the minimum required latency of the output. This makes TasNet suitable for applications where low-power, real-time implementation is desirable such as in hearable and telecommunication devices.

365.
TITLE: Alternative Objective Functions for Deep Clustering
AUTHORS: Z. Wang, J. Le Roux, J. Hershey
YEAR: 2018
SOURCE: 2018 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: The recently proposed deep clustering framework represents a significant step towards solving the cocktail party problem. This study proposes and compares a variety of alternative objective functions for training deep clustering networks. In addition, whereas the original deep clustering work relied on k-means clustering for test-time inference, here we investigate inference methods that are matched to the training objective. Furthermore, we explore the use of an improved chimera network architecture for speech separation, which combines deep clustering with mask-inference networks in a multiobjective training scheme. The deep clustering loss acts as a regularizer while training the end-to-end mask inference network for best separation. With further iterative phase reconstruction, our best proposed method achieves a state-of-the-art 11.5 dB signal-to-distortion ratio (SDR) result on the publicly available wsj0-2mix dataset, with a much simpler architecture than the previous best approach.

366.
TITLE: Advances in Phase-Aware Signal Processing in Speech Communication
AUTHORS: P.M.B. Mahale, R. Saeidi, Y. Stylianou
YEAR: 2016
SOURCE: Speech Communication
ABSTRACT: During the past three decades, the issue of processing spectral phase has been largely neglected in speech applications. There is no doubt that the interest of speech processing community towards the use of phase information in a big spectrum of speech technologies, from automatic speech and speaker recognition to speech synthesis, from speech enhancement and source separation to speech coding, is constantly increasing. In this paper, we elaborate on why phase was believed to be unimportant in each application. We provide an overview of advancements in phase-aware signal processing with applications to speech, showing that considering phase-aware speech processing can be beneficial in many cases, while it can complement the possible solutions that magnitude-only methods suggest. Our goal is to show that phase-aware signal processing is an important emerging field with high potential in the current speech communication applications. The paper provides an extended and up-to-date bibliography on the topic of phase aware speech processing aiming at providing the necessary background to the interested readers for following the recent advancements in the area. Our review expands the step initiated by our organized special session and exemplifies the usefulness of spectral phase information in a wide range of speech processing applications. Finally, the overview will provide some future work directions.

367.
TITLE: Complex Spectrogram Enhancement by Convolutional Neural Network with Multi-Metrics Learning
AUTHORS: S. Fu, T. Hu, Y. Tsao, X. Lu
YEAR: 2017
SOURCE: 2017 IEEE 27th International Workshop on Machine Learning for Signal Processing
ABSTRACT: This paper aims to address two issues existing in the current speech enhancement methods: 1) the difficulty of phase estimations; 2) a single objective function cannot consider multiple metrics simultaneously. To solve the first problem, we propose a novel convolutional neural network (CNN) model for complex spectrogram enhancement, namely estimating clean real and imaginary (RI) spectrograms from noisy ones. The reconstructed RI spectrograms are directly used to synthesize enhanced speech waveforms. In addition, since log-power spectrogram (LPS) can be represented as a function of RI spectrograms, its reconstruction is also considered as another target. Thus a unified objective function, which combines these two targets (reconstruction of RI spectrograms and LPS), is equivalent to simultaneously optimizing two commonly used objective metrics: segmental signal-to-noise ratio (SSNR) and log-spectral distortion (LSD). Therefore, the learning process is called multi-metrics learning (MML). Experimental results confirm the effectiveness of the proposed CNN with RI spectrograms and MML in terms of improved standardized evaluation metrics on a speech enhancement task.

368.
TITLE: End-to-End Multi-Speaker Speech Recognition
AUTHORS: S. Settle, J. Le Roux, T. Hori, S. Watanabe, J. Hershey
YEAR: 2018
SOURCE: 2018 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: Current advances in deep learning have resulted in a convergence of methods across a wide range of tasks, opening the door for tighter integration of modules that were previously developed and optimized in isolation. Recent ground-breaking works have produced end-to-end deep network methods for both speech separation and end-to-end automatic speech recognition (ASR). Speech separation methods such as deep clustering address the challenging cocktail-party problem of distinguishing multiple simultaneous speech signals. This is an enabling technology for real-world human machine interaction (HMI). However, speech separation requires ASR to interpret the speech for any HMI task. Likewise, ASR requires speech separation to work in an unconstrained environment. Although these two components can be trained in isolation and connected after the fact, this paradigm is likely to be sub-optimal, since it relies on artificially mixed data. In this paper, we develop the first fully end-to-end, jointly trained deep learning system for separation and recognition of overlapping speech signals. The joint training framework synergistically adapts the separation and recognition to each other. As an additional benefit, it enables training on more realistic data that contains only mixed signals and their transcriptions, and thus is suited to large scale training on existing transcribed data.

369.
TITLE: Divide and Conquer: A Deep CASA Approach to Talker-Independent Monaural Speaker Separation
AUTHORS: Y. Liu, D. Wang
YEAR: 2019
SOURCE: IEEE/ACM Transactions on Audio, Speech, and Language Processing
ABSTRACT: We address talker-independent monaural speaker separation from the perspectives of deep learning and computational auditory scene analysis (CASA). Specifically, we decompose the multi-speaker separation task into the stages of simultaneous grouping and sequential grouping. Simultaneous grouping is first performed in each time frame by separating the spectra of different speakers with a permutation-invariantly trained neural network. In the second stage, the frame-level separated spectra are sequentially grouped to different speakers by a clustering network. The proposed deep CASA approach optimizes frame-level separation and speaker tracking in turn, and produces excellent results for both objectives. Experimental results on the benchmark WSJ0-2mix database show that the new approach achieves the state-of-the-art results with a modest model size.

370.
TITLE: Listening to Each Speaker One by One with Recurrent Selective Hearing Networks
AUTHORS: K. Kinoshita, L. Drude, M. Delcroix, T. Nakatani
YEAR: 2018
SOURCE: 2018 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: Deep learning-based single-channel source separation algorithms are currently being actively investigated. Among them, Deep Clustering (DC) and Deep Attractor Networks (DANs) have made it possible to separate an arbitrary number of speakers. In particular, they cleverly combine a neural network and a K-means clustering algorithm to obtain source separation masks with the assumption that the correct number of speakers at the test time is known in advance. Unlike DC and DAN, Permutation Invariant Training (PIT) was proposed as a purely neural network-based mask estimator. Essentially, however, PIT can deal with only a fixed number of speakers, given the strong relationship between the dimensions of the output nodes and the assumed number of sources. Considering these limitations and merits of such conventional methods, this paper proposes a purely neural-network based mask estimator that can handle an arbitrary number of sources, and simultaneously estimate the number of sources in the test signal. To accomplish this, while the conventional methods deal with the source separation problem as a one-pass problem, we cast the problem as a recursive multi-pass source extraction problem based on a recurrent neural network (RNN) that can learn and determine how many computational steps/iterations have to be performed depending on the input signals. In this paper, we describe our proposed method in detail, and experimentally show its efficacy in terms of source separation and source counting performance.

371.
TITLE: CBLDNN-Based Speaker-Independent Speech Separation Via Generative Adversarial Training
AUTHORS: 2018 IEEE International Conference on Acoustics, Speech and Signal Processing
YEAR: 2018
SOURCE: 2018 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: In this paper, we propose a speaker-independent multi-speaker monaural speech separation system (CBLDNN-GAT) based on convolutional, bidirectional long short-term memory, deep feedforward neural network (CBLDNN) with generative adversarial training (GAT). Our system aims at obtaining better speech quality instead of only minimizing a mean square error (MSE). In the initial phase, we utilize log-mel filterbank and pitch features to warm up our CBLDNN in a multi-task manner. Thus, the information that contributes to separating speech and improving speech quality is integrated into the model. We execute GAT throughout the training, which makes the separated speech indistinguishable from the real one. We evaluate CBLDNN-GAT on WSJ0-2mix dataset. The experimental results show that the proposed model achieves 11.0d-B signal-to-distortion ratio (SDR) improvement, which is the new state-of-the-art result.

372.
TITLE: Phase Reconstruction with Learned Time-Frequency Representations for Single-Channel Speech Separation
AUTHORS: G. Wichern, J. Le Roux
YEAR: 2018
SOURCE: 2018 16th International Workshop on Acoustic Signal Enhancement
ABSTRACT: Progress in solving the cocktail party problem, i.e., separating the speech from multiple overlapping speakers, has recently accelerated with the invention of techniques such as deep clustering and permutation free mask inference. These approaches typically focus on estimating target STFT magnitudes and ignore problems of phase inconsistency. In this paper, we explicitly integrate phase reconstruction into our separation algorithm using a loss function defined on time-domain signals. A deep neural network structure is defined by unfolding a phase reconstruction algorithm and treating each iteration as a layer in our network. Furthermore, instead of using fixed STFT/iSTFT time-frequency representations, we allow our network to learn a modified version of these representations from data. We compare several variants of these unfolded phase reconstruction networks achieving state of the art results on the publicly available wsj0-2mix dataset, and show improved performance when the STFT/iSTFT-like representations are allowed to adapt.

373.
TITLE: MaD TwinNet: Masker-Denoiser Architecture with Twin Networks for Monaural Sound Source Separation
AUTHORS: K. Drossos, et al.
YEAR: 2018
SOURCE: 2018 International Joint Conference on Neural Networks (IJCNN)
ABSTRACT: Monaural singing voice separation task focuses on the prediction of the singing voice from a single channel music mixture signal. Current state of the art (SOTA) results in monaural singing voice separation are obtained with deep learning based methods. In this work we present a novel recurrent neural approach that learns long-term temporal patterns and structures of a musical piece. We build upon the recently proposed Masker-Denoiser (MaD) architecture and we enhance it with the Twin Networks, a technique to regularize a recurrent generative network using a backward running copy of the network. We evaluate our method using the Demixing Secret Dataset and we obtain an increment to signal-to-distortion ratio (SDR) of 0.37 dB and to signal-to-interference ratio (SIR) of 0.23 dB, compared to previous SOTA results. 

374.
TITLE: LibriMix: An Open-Source Dataset for Generalizable Speech Separation.
AUTHORS: J. Cosentino, M. Pariente, S. Cornell, A. Deleforge, E. Vincent
YEAR: 2020
SOURCE: arXiv
ABSTRACT: In recent years, wsj0-2mix has become the reference dataset for single-channel speech separation. Most deep learning-based speech separation models today are benchmarked on it. However, recent studies have shown important performance drops when models trained on wsj0-2mix are evaluated on other, similar datasets. To address this generalization issue, we created LibriMix, an open-source alternative to wsj0-2mix, and to its noisy extension, WHAM!. Based on LibriSpeech, LibriMix consists of two- or three-speaker mixtures combined with ambient noise samples from WHAM!. Using Conv-TasNet, we achieve competitive performance on all LibriMix versions. In order to fairly evaluate across datasets, we introduce a third test set based on VCTK for speech and WHAM! for noise. Our experiments show that the generalization error is smaller for models trained with LibriMix than with WHAM!, in both clean and noisy conditions. Aiming towards evaluation in more realistic, conversation-like scenarios, we also release a sparsely overlapping version of LibriMix's test set.

375.
TITLE: Dual-Path RNN: Efficient Long Sequence Modeling for Time-Domain Single-Channel Speech Separation
AUTHORS: Y. Luo, Z. Chen, T./ Yoshioka
YEAR: 2020
SOURCE: 2020 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: Recent studies in deep learning-based speech separation have proven the superiority of time-domain approaches to conventional time-frequency-based methods. Unlike the time-frequency domain approaches, the time-domain separation systems often receive input sequences consisting of a huge number of time steps, which introduces challenges for modeling extremely long sequences. Conventional recurrent neural networks (RNNs) are not effective for modeling such long sequences due to optimization difficulties, while one-dimensional convolutional neural networks (1-D CNNs) cannot perform utterance-level sequence modeling when its receptive field is smaller than the sequence length. In this paper, we propose dual-path recurrent neural network (DPRNN), a simple yet effective method for organizing RNN layers in a deep structure to model extremely long sequences. DPRNN splits the long sequential input into smaller chunks and applies intra- and inter-chunk operations iteratively, where the input length can be made proportional to the square root of the original sequence length in each operation. Experiments show that by replacing 1-D CNN with DPRNN and apply sample-level modeling in the time-domain audio separation network (TasNet), a new state-of-the-art performance on WSJ0-2mix is achieved with a 20 times smaller model than the previous best system.

376.
TITLE: A Comprehensive Study of Speech Separation: Spectrogram vs Waveform Separation
AUTHORS: F. Bahmaninezhad, et al.
YEAR: 2019
SOURCE: Interspeech 2019
ABSTRACT: Speech separation has been studied widely for single-channel close-talk recordings over the past few years; developed solutions are mostly in frequency-domain. Recently, a raw audio waveform separation network (TasNet) introduced for single-channel data, with achieving high Si-SNR (scale-invariant source-to-noise ratio) and SDR (source-to-distortion ratio) comparing against the state-of-the-art solution in frequency-domain. In this study, we incorporate effective components of TasNet into a frequency-domain separation method. We compare both for alternative scenarios. We introduce a solution for directly optimizing the separation criterion in frequency-domain networks. In addition to speech separation objective and subjective measurements, we evaluate the separation performance on a speech recognition task as well. We study the speech separation problem for far-filed data (more similar to naturalistic audio streams) and develop multi-channel solutions for both frequency and time-domain separators with utilizing spectral, spatial and speaker location information. For our experiments, we simulated multi-channel spatialized reverberate WSJ0-2mix dataset. Our experimental results show that spectrogram separation can achieve competitive performance with better network design. With multi-channel framework as well, we can obtain relatively up to +35.5% and +46% improvement in terms of WER and SDR, respectively.

377.
TITLE: FurcaNeXt: End-to-End Monaural Speech Separation with Dynamic Gated Dilated Temporal Convolutional Networks
AUTHORS: Z. Shi, H. Lin, L. Liu, R. Liu, J. Han
YEAR: 2020
SOURCE: arXiv
ABSTRACT: Deep dilated temporal convolutional networks (TCN) have been proved to be very effective in sequence modeling. In this paper we propose several improvements of TCN for end-to-end approach to monaural speech separation, which consists of 1) multi-scale dynamic weighted gated dilated convolutional pyramids network (FurcaPy), 2) gated TCN with intra-parallel convolutional components (FurcaPa), 3) weight-shared multi-scale gated TCN (FurcaSh), 4) dilated TCN with gated difference-convolutional component (FurcaSu), that all these networks take the mixed utterance of two speakers and maps it to two separated utterances, where each utterance contains only one speaker's voice. For the objective, we propose to train the network by directly optimizing utterance level signal-to-distortion ratio (SDR) in a permutation invariant training (PIT) style. Our experiments on the the public WSJ0-2mix data corpus results in 18.4dB SDR improvement, which shows our proposed networks can leads to performance improvement on the speaker separation task.

378.
TITLE: Improving Universal Sound Separation Using Sound Classification
AUTHORS: E. Tzinis, S. Wisdom, J. Hershey, A. Jansen, D. Ellis
YEAR: 2020
SOURCE: 2020 IEEE International Conference on Acoustics, Speech and Signal Processing 
ABSTRACT: Deep learning approaches have recently achieved impressive performance on both audio source separation and sound classification. Most audio source separation approaches focus only on separating sources belonging to a restricted domain of source classes, such as speech and music. However, recent work has demonstrated the possibility of "universal sound separation", which aims to separate acoustic sources from an open domain, regardless of their class. In this paper, we utilize the semantic information learned by sound classifier networks trained on a vast amount of diverse sounds to improve universal sound separation. In particular, we show that semantic embeddings extracted from a sound classifier can be used to condition a separation network, providing it with useful additional information. This approach is especially useful in an iterative setup, where source estimates from an initial separation stage and their corresponding classifier-derived embeddings are fed to a second separation network. By performing a thorough hyperparameter search consisting of over a thousand experiments, we find that classifier embeddings from oracle clean sources provide nearly one dB of SNR gain, and our best iterative models achieve a significant fraction of this oracle performance, establishing a new state-of-the-art for universal sound separation.

379.
TITLE: FurcaNet: An End-to-End Deep Gated Convolutional, Long Short-Term Memory, Deep Neural Networks for Single Channel Speech Separation
AUTHORS: Z. Shi, H. Lin, L. Liu, R. Liu, J. Han
YEAR: 2019
SOURCE: arXiv
ABSTRACT: Deep gated convolutional networks have been proved to be very effective in single channel speech separation. However current state-of-the-art framework often considers training the gated convolutional networks in time-frequency (TF) domain. Such an approach will result in limited perceptual score, such as signal-to-distortion ratio (SDR) upper bound of separated utterances and also fail to exploit an end-to-end framework. In this paper we present an integrated simple and effective end-to-end approach to monaural speech separation, which consists of deep gated convolutional neural networks (GCNN) that takes the mixed utterance of two speakers and maps it to two separated utterances, where each utterance contains only one speaker's voice. In addition long short-term memory (LSTM) is employed for long term temporal modeling. For the objective, we propose to train the network by directly optimizing utterance level SDR in a permutation invariant training (PIT) style. Our experiments on the public WSJ0-2mix data corpus demonstrate that this new scheme can produce more discriminative separated utterances and leading to performance improvement on the speaker separation task.

380.
TITLE: Filterbank Design for End-to-end Speech Separation
AUTHORS: M. Pariente, S. Cornell, A. Deleforge, E. Vincent
YEAR: 2020
SOURCE: 2020 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: Single-channel speech separation has recently made great progress thanks to learned filterbanks as used in ConvTasNet. In parallel, parameterized filterbanks have been proposed for speaker recognition where only center frequencies and bandwidths are learned. In this work, we extend real-valued learned and parameterized filterbanks into complex-valued analytic filterbanks and define a set of corresponding representations and masking strategies. We evaluate these filterbanks on a newly released noisy speech separation dataset (WHAM). The results show that the proposed analytic learned filterbank consistently outperforms the real-valued filterbank of ConvTasNet. Also, we validate the use of parameterized filterbanks and show that complex-valued representations and masks are beneficial in all conditions. Finally, we show that the STFT achieves its best performance for 2 ms windows.

381.
TITLE: Improved Speech Separation with Time-and-Frequency Cross-domain Joint Embedding and Clustering
AUTHORS: G.P. Yang, C. Tuan, H.Y. Lee, L. Lee
YEAR: 2019
SOURCE: arXiv
ABSTRACT: Speech separation has been very successful with deep learning techniques. Substantial effort has been reported based on approaches over spectrogram, which is well known as the standard time-and-frequency cross-domain representation for speech signals. It is highly correlated to the phonetic structure of speech, or "how the speech sounds" when perceived by human, but primarily frequency domain features carrying temporal behaviour. Very impressive work achieving speech separation over time domain was reported recently, probably because waveforms in time domain may describe the different realizations of speech in a more precise way than spectrogram. In this paper, we propose a framework properly integrating the above two directions, hoping to achieve both purposes. We construct a time-and-frequency feature map by concatenating the 1-dim convolution encoded feature map (for time domain) and the spectrogram (for frequency domain), which was then processed by an embedding network and clustering approaches very similar to those used in time and frequency domain prior works. In this way, the information in the time and frequency domains, as well as the interactions between them, can be jointly considered during embedding and clustering. Very encouraging results (state-of-the-art to our knowledge) were obtained with WSJ0-2mix dataset in preliminary experiments.

382.
TITLE: PHASEN: A Phase-and-Harmonics-Aware Speech Enhancement Network
AUTHORS: D. Yin, C. Luo, Z. XIong, W.J. Zeng
YEAR: 2020
SOURCE: AAAI 2020
ABSTRACT: Time-frequency (T-F) domain masking is a mainstream approach for single-channel speech enhancement. Recently, focuses have been put to phase prediction in addition to amplitude prediction. In this paper, we propose a phase-and-harmonics-aware deep neural network (DNN), named PHASEN, for this task. Unlike previous methods that directly use a complex ideal ratio mask to supervise the DNN learning, we design a two-stream network, where amplitude stream and phase stream are dedicated to amplitude and phase prediction. We discover that the two streams should communicate with each other, and this is crucial to phase prediction. In addition, we propose frequency transformation blocks to catch long-range correlations along the frequency axis. The visualization shows that the learned transformation matrix spontaneously captures the harmonic correlation, which has been proven to be helpful for T-F spectrogram reconstruction. With these two innovations, PHASEN acquires the ability to handle detailed phase patterns and to utilize harmonic patterns, getting 1.76dB SDR improvement on AVSpeech + AudioSet dataset. It also achieves significant gains over Google's network on this dataset. On Voice Bank + DEMAND dataset, PHASEN outperforms previous methods by a large margin on four metrics.

383.
TITLE: Multi-Modal Multi-Channel Target Speech Separation
AUTHORS: R. Gu, S. Zhang, Y. Xu, L. Chen, Y. Zou, D. Yu
YEAR: 2020
SOURCE: IEEE Journal of Selected Topics in Signal Processing
ABSTRACT: Target speech separation refers to extracting a target speaker's voice from an overlapped audio of simultaneous talkers. Previously the use of visual modality for target speech separation has demonstrated great potentials. This work proposes a general multi-modal framework for target speech separation by utilizing all the available information of the target speaker, including his/her spatial location, voice characteristics and lip movements. Also, under this framework, we investigate on the fusion methods for multi-modal joint modeling. A factorized attention-based fusion method is proposed to aggregate the high-level semantic information of multi-modalities at embedding level. This method firstly factorizes the mixture audio into a set of acoustic subspaces, then leverages the target's information from other modalities to enhance these subspace acoustic embeddings with a learnable attention scheme. To validate the robustness of proposed multi-modal separation model in practical scenarios, the system was evaluated under the condition that one of the modalities is temporarily missing, invalid or corrupted. Experiments are conducted on a large-scale audio-visual dataset collected from YouTube (to be released) that spatialized by simulated room impulse responses (RIRs). Experiment results illustrate that our proposed multi-modal framework significantly outperforms single-modal and bi-modal speech separation approaches, while can still support real-time processing.

384.
TITLE: Continuous Speech Separation: Dataset and Analysis
AUTHORS: Z. Chen, et al.
YEAR: 2020
SOURCE: 2020 IEEE International Conference on Acoustics, Speech and Signal Processing 
ABSTRACT: This paper describes a dataset and protocols for evaluating continuous speech separation algorithms. Most prior speech separation studies use pre-segmented audio signals, which are typically generated by mixing speech utterances on computers so that they fully overlap. Also, the separation algorithms have often been evaluated based on signal-based metrics such as signal-to-distortion ratio. However, in natural conversations, speech signals are continuous and contain both overlapped and overlap-free regions. In addition, the signal-based metrics only have weak correlation with automatic speech recognition (ASR) accuracy. Not only does this make it hard to assess the practical relevance of the tested algorithms, it also hinders researchers from developing systems that can be readily applied to real scenarios. In this paper, we define continuous speech separation (CSS) as a task of generating a set of non-overlapped speech signals from a continuous audio stream that contains multiple utterances that are partially overlapped by a varying degree. A new real recording dataset, called LibriCSS, is derived from LibriSpeech by concatenating the corpus utterances to simulate conversations and capturing the audio replays with far-field microphones. A Kaldi-based ASR evaluation protocol is established by using a well-trained multi-conditional acoustic model. A recently proposed speaker-independent CSS algorithm is investigated by using LibriCSS. The dataset and evaluation scripts are made available to facilitate the research in this direction.

385.
TITLE: Two-Step Sound Source Separation: Training On Learned Latent Targets
AUTHORS: E. Tzinis, et al.
YEAR: 2020
SOURCE: 2020 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: In this paper, we propose a two-step training procedure for source separation via a deep neural network. In the first step we learn a transform (and it’s inverse) to a latent space where masking-based separation performance using oracles is optimal. For the second step, we train a separation module that operates on the previously learned space. In order to do so, we also make use of a scale-invariant signal to distortion ratio (SI-SDR) loss function that works in the latent space, and we prove that it lower-bounds the SI-SDR in the time domain. We run various sound separation experiments that show how this approach can obtain better performance as compared to systems that learn the transform and the separation module jointly. The proposed methodology is general enough to be applicable to a large class of neural network end-to-end separation systems.

386.
TITLE: A Multi-Phase Gammatone Filterbank for Speech Separation Via Tasnet
AUTHORS: D. Ditto, T. Gerkmann
YEAR: 2020
SOURCE: 2020 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: In this work, we investigate if the learned encoder of the end-to-end convolutional time domain audio separation network (Conv-TasNet) is the key to its recent success, or if the encoder can just as well be replaced by a deterministic hand-crafted filterbank. Motivated by the resemblance of the trained encoder of Conv-TasNet to auditory filterbanks, we propose to employ a deterministic gammatone filterbank. In contrast to a common gammatone filterbank, our filters are restricted to 2 ms length to allow for low-latency processing. Inspired by the encoder learned by Conv-TasNet, in addition to the logarithmically spaced filters, the proposed filterbank holds multiple gammatone filters at the same center frequency with varying phase shifts. We show that replacing the learned encoder with our proposed multi-phase gammatone filterbank (MP-GTF) even leads to a scale-invariant source-to-noise ratio (SI-SNR) improvement of 0.7 dB. Furthermore, in contrast to using the learned encoder we show that the number of filters can be reduced from 512 to 128 without loss of performance.

387.
TITLE: Demystifying TasNet: A Dissecting Approach
AUTHORS: J. Heitkaemper, D. Jacobeit, C. Boddeker, L. Drude, R. Haeb-Umbach
YEAR: 2020
SOURCE: 2020 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: In recent years time domain speech separation has excelled over frequency domain separation in single channel scenarios and noise-free environments. In this paper we dissect the gains of the time-domain audio separation network (TasNet) approach by gradually replacing components of an utterance-level permutation invariant training (u-PIT) based separation system in the frequency domain until the TasNet system is reached, thus blending components of frequency domain approaches with those of time domain approaches. Some of the intermediate variants achieve comparable signal-to-distortion ratio (SDR) gains to TasNet, but retain the advantage of frequency domain processing: compatibility with classic signal processing tools such as frequency-domain beamforming and the human interpretability of the masks. Furthermore, we show that the scale invariant signal-to-distortion ratio (si-SDR) criterion used as loss function in TasNet is related to a logarithmic mean square error criterion and that it is this criterion which contributes most reliable to the performance advantage of TasNet. Finally, we critically assess which gains in a noise-free single channel environment generalize to more realistic reverberant conditions.

388.
TITLE: Learning With Learned Loss Function: Speech Enhancement With Quality-Net to Improve Perceptual Evaluation of Speech Quality
AUTHORS: S. Fu, C.F. Liao, Y. Tsao
YEAR: 2020
SOURCE: IEEE Signal Processing Letters
ABSTRACT: Utilizing a human-perception-related objective function to train a speech enhancement model has become a popular topic recently. The main reason is that the conventional mean squared error (MSE) loss cannot represent auditory perception well. One of the typical human-perception-related metrics, which is the perceptual evaluation of speech quality (PESQ), has been proven to provide a high correlation to the quality scores rated by humans. Owing to its complex and non-differentiable properties, however, the PESQ function may not be used to optimize speech enhancement models directly. In this study, we propose optimizing the enhancement model with an approximated PESQ function, which is differentiable and learned from the training data. The experimental results show that the learned surrogate function can guide the enhancement model to further boost the PESQ score (increase of 0.18 points compared to the results trained with MSE loss) and maintain the speech intelligibility.

389.
TITLE: End-to-End Training of Time Domain Audio Separation and Recognition
AUTHORS: T. Von Neumann, et al.
YEAR: 2020
SOURCE: 2020 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: The rising interest in single-channel multi-speaker speech separation sparked development of End-to-End (E2E) approaches to multi-speaker speech recognition. However, up until now, state-of-the-art neural network–based time domain source separation has not yet been combined with E2E speech recognition. We here demonstrate how to combine a separation module based on a Convolutional Time domain Audio Separation Network (Conv-TasNet) with an E2E speech recognizer and how to train such a model jointly by distributing it over multiple GPUs or by approximating truncated back-propagation for the convolutional front-end. To put this work into perspective and illustrate the complexity of the design space, we provide a compact overview of single-channel multi-speaker recognition systems. Our experiments show a word error rate of 11.0% on WSJ0-2mix and indicate that our joint time domain model can yield substantial improvements over cascade DNN-HMM and monolithic E2E frequency domain systems proposed so far.

390.
TITLE: Time-Domain Speaker Extraction Network
AUTHORS: C. Xu, W. Rao, C.E. Siong, H. Li
YEAR: 2019
SOURCE: 2019 IEEE Automatic Speech Recognition and Understanding Workshop
ABSTRACT: Speaker extraction is to extract a target speaker's voice from multi-talker speech. It simulates humans' cocktail party effect or the selective listening ability. The prior work mostly performs speaker extraction in frequency domain, then reconstructs the signal with some phase approximation. The inaccuracy of phase estimation is inherent to the frequency domain processing, that affects the quality of signal reconstruction. In this paper, we propose a time-domain speaker extraction network (TseNet) that doesn't decompose the speech signal into magnitude and phase spectrums, therefore, doesn't require phase estimation. The TseNet consists of a stack of dilated depthwise separable convolutional networks, that capture the long-range dependency of the speech signal with a manageable number of parameters. It is also conditioned on a reference voice from the target speaker, that is characterized by speaker i-vector, to perform the selective listening to the target speaker. Experiments show that the proposed TseNet achieves 16.3% and 7.0% relative improvements over the baseline in terms of signal-to-distortion ratio (SDR) and perceptual evaluation of speech quality (PESQ) under open evaluation condition.

391.
TITLE: Spatial and Spectral Deep Attention Fusion for Multi-Channel Speech Separation Using Deep Embedding Features
AUTHORS: C. Fan, B. Liu, J. Tao, J. Yi, Z. Wen 
YEAR: 2020
SOURCE: arXiv
ABSTRACT: Multi-channel deep clustering (MDC) has acquired a good performance for speech separation. However, MDC only applies the spatial features as the additional information. So it is difficult to learn mutual relationship between spatial and spectral features. Besides, the training objective of MDC is defined at embedding vectors, rather than real separated sources, which may damage the separation performance. In this work, we propose a deep attention fusion method to dynamically control the weights of the spectral and spatial features and combine them deeply. In addition, to solve the training objective problem of MDC, the real separated sources are used as the training objectives. Specifically, we apply the deep clustering network to extract deep embedding features. Instead of using the unsupervised K-means clustering to estimate binary masks, another supervised network is utilized to learn soft masks from these deep embedding features. Our experiments are conducted on a spatialized reverberant version of WSJ0-2mix dataset. Experimental results show that the proposed method outperforms MDC baseline and even better than the oracle ideal binary mask (IBM).

392.
TITLE: CochleaNet: A Robust Language-independent Audio-Visual Model for Speech Enhancement
AUTHORS: M. Gogate, K. Dashtipour, 
YEAR: 2019
SOURCE: arXiv
ABSTRACT: Noisy situations cause huge problems for suffers of hearing loss as hearing aids often make the signal more audible but do not always restore the intelligibility. In noisy settings, humans routinely exploit the audio-visual (AV) nature of the speech to selectively suppress the background noise and to focus on the target speaker. In this paper, we present a causal, language, noise and speaker independent AV deep neural network (DNN) architecture for speech enhancement (SE). The model exploits the noisy acoustic cues and noise robust visual cues to focus on the desired speaker and improve the speech intelligibility. To evaluate the proposed SE framework a first of its kind AV binaural speech corpus, called ASPIRE, is recorded in real noisy environments including cafeteria and restaurant. We demonstrate superior performance of our approach in terms of objective measures and subjective listening tests over the state-of-the-art SE approaches as well as recent DNN based SE models. In addition, our work challenges a popular belief that a scarcity of multi-language large vocabulary AV corpus and wide variety of noises is a major bottleneck to build a robust language, speaker and noise independent SE systems. We show that a model trained on synthetic mixture of Grid corpus (with 33 speakers and a small English vocabulary) and ChiME 3 Noises (consisting of only bus, pedestrian, cafeteria, and street noises) generalise well not only on large vocabulary corpora but also on completely unrelated languages (such as Mandarin), wide variety of speakers and noises.

393.
TITLE: End-to-End Microphone Permutation and Number Invariant Multi-Channel Speech Separation
AUTHORS: Y. Luo, Z. Chen, N. Mesgarani, T. Yoshioka
YEAR: 2020. 
SOURCE: 2020 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: An important problem in ad-hoc microphone speech separation is how to guarantee the robustness of a system with respect to the locations and numbers of microphones. The former requires the system to be invariant to different indexing of the microphones with the same locations, while the latter requires the system to be able to process inputs with varying dimensions. Conventional optimization-based beamforming techniques satisfy these requirements by definition, while for deep learning-based end-to-end systems those constraints are not fully addressed. In this paper, we propose transform-average-concatenate (TAC), a simple design paradigm for channel permutation and number invariant multi-channel speech separation. Based on the filter-and-sum network (FaSNet), a recently proposed end-to-end time-domain beamforming system, we show how TAC significantly improves the separation performance across various numbers of microphones in noisy reverberant separation tasks with ad-hoc arrays. Moreover, we show that TAC also significantly improves the separation performance with fixed geometry array configuration, further proving the effectiveness of the proposed paradigm in the general problem of multi-microphone speech separation.

394.
TITLE: SpEx: Multi-Scale Time Domain Speaker Extraction Network
AUTHORS: C. Xu, W. Rao, C.E. Siong, H. Li
YEAR: 2020
SOURCE: IEEE/ACM Transactions on Audio, Speech, and Language Processing
ABSTRACT: Speaker extraction aims to mimic humans’ selective auditory attention by extracting a target speaker's voice from a multi-talker environment. It is common to perform the extraction in frequency-domain, and reconstruct the time-domain signal from the extracted magnitude and estimated phase spectra. However, such an approach is adversely affected by the inherent difficulty of phase estimation. Inspired by Conv-TasNet, we propose a time-domain speaker extraction network (SpEx) that converts the mixture speech into multi-scale embedding coefficients instead of decomposing the speech signal into magnitude and phase spectra. In this way, we avoid phase estimation. The SpEx network consists of four network components, namely speaker encoder, speech encoder, speaker extractor, and speech decoder. Specifically, the speech encoder converts the mixture speech into multi-scale embedding coefficients, the speaker encoder learns to represent the target speaker with a speaker embedding. The speaker extractor takes the multi-scale embedding coefficients and target speaker embedding as input and estimates a receptive mask. Finally, the speech decoder reconstructs the target speaker's speech from the masked embedding coefficients. We also propose a multi-task learning framework and a multi-scale embedding implementation. Experimental results show that the proposed SpEx achieves 37.3%, 37.7% and 15.0% relative improvements over the best baseline in terms of signal-to-distortion ratio (SDR), scale-invariant SDR (SI-SDR), and perceptual evaluation of speech quality (PESQ) under an open evaluation condition.

395.
TITLE: Deep Casa for Talker-Independent Monaural Speech Separation
AUTHORS: Y. Liu, M. Delfarah, D. Wang
YEAR: 2020
SOURCE: 2020 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: Monaural speech separation is the task of separating target speech from interference in single-channel recordings. Although substantial progress has been made recently in deep learning based speech separation, previous studies usually focus on a single type of interference, either background noise or competing speakers. In this study, we address both speech and nonspeech interference, i.e., monaural speaker separation in noise, in a talker-independent fashion. We extend a recently proposed deep CASA system to deal with noisy speaker mixtures. To facilitate speech enhancement, a denoising module is added to deep CASA as a front-end processor. The proposed systems achieve state-of-the-art results on a benchmark noisy two-speaker separation dataset. The denoising module leads to substantial performance gain across various noise types, and even better generalization in noise-free conditions.

396.
TITLE: Meta-Learning Extractors for Music Source Separation
AUTHORS: D.Samuel, A. Ganeshan, J. Naradowsky
YEAR: 2020
SOURCE: 2020 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: We propose a hierarchical meta-learning-inspired model for music source separation (Meta-TasNet) in which a generator model is used to predict the weights of individual extractor models. This enables efficient parameter-sharing, while still allowing for instrument-specific parameterization. Meta-TasNet is shown to be more effective than the models trained independently or in a multi-task setting, and achieve performance comparable with state-of-the-art methods. In comparison to the latter, our extractors contain fewer parameters and have faster run-time performance. We discuss important architectural considerations, and explore the costs and benefits of this approach.

397.
TITLE: Deep Attention Fusion Feature for Speech Separation with End-to-End Post-Filter Method
AUTHORS: C. Fan, J. Tao, B. Liu, J. Yi, Z. Wen, X. Liu
YEAR: 2020
SOURCE: arXiv
ABSTRACT: In this paper, we propose an end-to-end post-filter method with deep attention fusion features for monaural speaker-independent speech separation. At first, a time-frequency domain speech separation method is applied as the pre-separation stage. The aim of pre-separation stage is to separate the mixture preliminarily. Although this stage can separate the mixture, it still contains the residual interference. In order to enhance the pre-separated speech and improve the separation performance further, the end-to-end post-filter (E2EPF) with deep attention fusion features is proposed. The E2EPF can make full use of the prior knowledge of the pre-separated speech, which contributes to speech separation. It is a fully convolutional speech separation network and uses the waveform as the input features. Firstly, the 1-D convolutional layer is utilized to extract the deep representation features for the mixture and pre-separated signals in the time domain. Secondly, to pay more attention to the outputs of the pre-separation stage, an attention module is applied to acquire deep attention fusion features, which are extracted by computing the similarity between the mixture and the pre-separated speech. These deep attention fusion features are conducive to reduce the interference and enhance the pre-separated speech. Finally, these features are sent to the post-filter to estimate each target signals. Experimental results on the WSJ0-2mix dataset show that the proposed method outperforms the state-of-the-art speech separation method. Compared with the pre-separation method, our proposed method can acquire 64.1%, 60.2%, 25.6% and 7.5% relative improvements in scale-invariant source-to-noise ratio (SI-SNR), the signal-to-distortion ratio (SDR), the perceptual evaluation of speech quality (PESQ) and the short-time objective intelligibility (STOI) measures, respectively.

398.
TITLE: Asteroid: The PyTorch-Based Audio Source Separation Toolkit for Researchers
AUTHORS: M. Pariente, et al. 
YEAR: 2020
SOURCE: arXiv
ABSTRACT: This paper describes Asteroid, the PyTorch-based audio source separation toolkit for researchers. Inspired by the most successful neural source separation systems, it provides all neural building blocks required to build such a system. To improve reproducibility, Kaldi-style recipes on common audio source separation datasets are also provided. This paper describes the software architecture of Asteroid and its most important features. By showing experimental results obtained with Asteroid's recipes, we show that our implementations are at least on par with most results reported in reference papers. The toolkit is publicly available.

399.
TITLE: Tackling Real Noisy Reverberant Meetings with All-Neural Source Separation, Counting, and Diarization System
AUTHORS: K. Kinoshita, M. Delcroix, S. Araki, T. Nakatani
YEAR: 2020
SOURCE: 2020 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: Automatic meeting analysis is an essential fundamental technology required to let, e.g. smart devices follow and respond to our conversations. To achieve an optimal automatic meeting analysis, we previously proposed an all-neural approach that jointly solves source separation, speaker diarization and source counting problems in an optimal way (in a sense that all the 3 tasks can be jointly optimized through error back-propagation). It was shown that the method could well handle simulated clean (noiseless and anechoic) dialog-like data, and achieved very good performance in comparison with several conventional methods. However, it was not clear whether such all-neural approach would be successfully generalized to more complicated real meeting data containing more spontaneously-speaking speakers, severe noise and reverberation, and how it performs in comparison with the state-of-the-art systems in such scenarios. In this paper, we first consider practical issues required for improving the robustness of the all-neural approach, and then experimentally show that, even in real meeting scenarios, the all-neural approach can perform effective speech enhancement, and simultaneously outperform state-of-the-art systems.

400.
TITLE: Improving Speaker Discrimination of Target Speech Extraction With Time-Domain Speakerbeam
AUTHORS: M. Delcroix, et al.
YEAR: 2020
SOURCE: 2020 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: Target speech extraction, which extracts a single target source in a mixture given clues about the target speaker, has attracted increasing attention. We have recently proposed SpeakerBeam, which exploits an adaptation utterance of the target speaker to extract his/her voice characteristics that are then used to guide a neural network towards extracting speech of that speaker. SpeakerBeam presents a practical alternative to speech separation as it enables tracking speech of a target speaker across utterances, and achieves promising speech extraction performance. However, it sometimes fails when speakers have similar voice characteristics, such as in same-gender mixtures, because it is difficult to discriminate the target speaker from the interfering speakers. In this paper, we investigate strategies for improving the speaker discrimination capability of SpeakerBeam. First, we propose a time-domain implementation of SpeakerBeam similar to that proposed for a time-domain audio separation network (TasNet), which has achieved state-of-the-art performance for speech separation. Besides, we investigate (1) the use of spatial features to better discriminate speakers when microphone array recordings are available, (2) adding an auxiliary speaker identification loss for helping to learn more discriminative voice characteristics. We show experimentally that these strategies greatly improve speech extraction performance, especially for same-gender mixtures, and outperform TasNet in terms of target speech extraction.

401.
TITLE: MITAS: A Compressed Time-Domain Audio Separation Network with Parameter Sharing
AUTHORS: C. Tuan, Y.K. Wu, H.Y. Lee, Y. Tsao
YEAR: 2019
SOURCE: arXiv
ABSTRACT: Deep learning methods have brought substantial advancements in speech separation (SS). Nevertheless, it remains challenging to deploy deep-learning-based models on edge devices. Thus, identifying an effective way to compress these large models without hurting SS performance has become an important research topic. Recently, TasNet and Conv-TasNet have been proposed. They achieved state-of-the-art results on several standardized SS tasks. Moreover, their low latency natures make them definitely suitable for real-time on-device applications. In this study, we propose two parameter-sharing schemes to lower the memory consumption on TasNet and Conv-TasNet. Accordingly, we derive a novel so-called MiTAS (Mini TasNet). Our experimental results first confirmed the robustness of our MiTAS on two types of perturbations in mixed audio. We also designed a series of ablation experiments to analyze the relation between SS performance and the amount of parameters in the model. The results show that MiTAS is able to reduce the model size by a factor of four while maintaining comparable SS performance with improved stability as compared to TasNet and Conv-TasNet. This suggests that MiTAS is more suitable for real-time low latency applications.

402.
TITLE: Unsupervised Learning of Semantic Audio Representations
AUTHORS: A. Jansen, et al.
YEAR: 2018
SOURCE: Proceedings of ICASSP 2018
ABSTRACT: Even in the absence of any explicit semantic annotation, vast collections of audio recordings provide valuable information for learning the categorical structure of sounds. We consider several class-agnostic semantic constraints that apply to unlabeled nonspeech audio: (i) noise and translations in time do not change the underlying sound category, (ii) a mixture of two sound events inherits the categories of the constituents, and (iii) the categories of events in close temporal proximity are likely to be the same or related. Without labels to ground them, these constraints are incompatible with classification loss functions. However, they may still be leveraged to identify geometric inequalities needed for triplet loss-based training of convolutional neural networks. The result is low-dimensional embeddings of the input spectrograms that recover 41% and 84% of the performance of their fully-supervised counterparts when applied to downstream query-by-example sound retrieval and sound event classification tasks, respectively. Moreover, in limited-supervision settings, our unsupervised embeddings double the state-of-the-art classification performance.

403.
TITLE: Deep Clustering: Discriminative Embeddings for Segmentation and Separation
AUTHORS: J. Hershey, Z. Chen, J. Le Roux, S. Watanabe
YEAR: 2016
SOURCE: 2016 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: We address the problem of "cocktail-party" source separation in a deep learning framework called deep clustering. Previous deep network approaches to separation have shown promising performance in scenarios with a fixed number of sources, each belonging to a distinct signal class, such as speech and noise. However, for arbitrary source classes and number, "class-based" methods are not suitable. Instead, we train a deep network to assign contrastive embedding vectors to each time-frequency region of the spectrogram in order to implicitly predict the segmentation labels of the target spectrogram from the input mixtures. This yields a deep network-based analogue to spectral clustering, in that the embeddings form a low-rank pair-wise affinity matrix that approximates the ideal affinity matrix, while enabling much faster performance. At test time, the clustering step "decodes" the segmentation implicit in the embeddings by optimizing K-means with respect to the unknown assignments. Preliminary experiments on single-channel mixtures from multiple speakers show that a speaker-independent model trained on two-speaker mixtures can improve signal quality for mixtures of held-out speakers by an average of 6dB. More dramatically, the same model does surprisingly well with three-speaker mixtures.

404.
TITLE: Performance Measurement in Blind Audio Source Separation
AUTHORS: E. Vincent, R. Gribonval, C. Fevotte
YEAR: 2006
SOURCE: IEEE Transactions on Audio, Speech, and Language Processing
ABSTRACT: In this paper, we discuss the evaluation of blind audio source separation (BASS) algorithms. Depending on the exact application, different distortions can be allowed between an estimated source and the wanted true source. We consider four different sets of such allowed distortions, from time-invariant gains to time-varying filters. In each case, we decompose the estimated source into a true source part plus error terms corresponding to interferences, additive noise, and algorithmic artifacts. Then, we derive a global performance measure using an energy ratio, plus a separate performance measure for each error term. These measures are computed and discussed on the results of several BASS problems with various difficulty levels.

405.
TITLE: Single Channel Speech Separation with Constrained Utterance Level Permutation Invariant Training Using Grid LSTM
AUTHORS: C. Xu, W. Rao, X. Xiao, C.E. Siong, H. Li
YEAR: 2018
SOURCE: 2018 IEEE International Conference on Acoustics, Speech and Signal Processing
ABSTRACT: Utterance level permutation invariant training (uPIT) technique is a state-of-the-art deep learning architecture for speaker independent multi-talker separation. uPIT solves the label ambiguity problem by minimizing the mean square error (MSE) over all permutations between outputs and targets. However, uPIT may be sub-optimal at segmental level because the optimization is not calculated over the individual frames. In this paper, we propose a constrained uPIT (cuPIT) to solve this problem by computing a weighted MSE loss using dynamic information (i.e., delta and acceleration). The weighted loss ensures the temporal continuity of output frames with the same speaker. Inspired by the heuristics (i.e., vocal tract continuity) in computational auditory scene analysis, we then extend the model by adding a Grid LSTM layer, that we name it as cuPIT-Grid LSTM, to automatically learn both temporal and spectral patterns over the input magnitude spectrum simultaneously. The experimental results show 9.6% and 8.5% relative improvements on WSJ0-2mix dataset under both closed and open conditions comparing with the uPIT baseline.

406.
TITLE: WaveNet: A Generative Model for Raw Audio
AUTHORS: A. Oord, et al.
YEAR: 2016
SOURCE: arXiv
ABSTRACT: This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.

407.
TITLE: Sudo rm -rf: Efficient Networks for Universal Audio Source Separation
AUTHORS: E. Tzinis, Z. Wang, P. Smaragdis
YEAR: 2020
SOURCE: arXiv
ABSTRACT: In this paper, we present an efficient neural network for end-to-end general purpose audio source separation. Specifically, the backbone structure of this convolutional network is the SUccessive DOwnsampling and Resampling of Multi-Resolution Features (SuDoRMRF) as well as their aggregation which is performed through simple one-dimensional convolutions. In this way, we are able to obtain high quality audio source separation with limited number of floating point operations, memory requirements, number of parameters and latency. Our experiments on both speech and environmental sound separation datasets show that SuDoRMRF performs comparably and even surpasses various state-of-the-art approaches with significantly higher computational resource requirements.

408.
TITLE: Unsupervised Sound Separation Using Mixtures of Mixtures
AUTHORS: S. Wisdom, et al.
YEAR: 2020
SOURCE: arXiv
ABSTRACT: In recent years, rapid progress has been made on the problem of single-channel sound separation using supervised training of deep neural networks. In such supervised approaches, the model is trained to predict the component sources from synthetic mixtures created by adding up isolated ground-truth sources. The reliance on this synthetic training data is problematic because good performance depends upon the degree of match between the training data and real-world audio, especially in terms of the acoustic conditions and distribution of sources. The acoustic properties can be challenging to accurately simulate, and the distribution of sound types may be hard to replicate. In this paper, we propose a completely unsupervised method, mixture invariant training (MixIT), that requires only single-channel acoustic mixtures. In MixIT, training examples are constructed by mixing together existing mixtures, and the model separates them into a variable number of latent sources, such that the separated sources can be remixed to approximate the original mixtures. We show that MixIT can achieve competitive performance compared to supervised methods on speech separation. Using MixIT in a semi-supervised learning setting enables unsupervised domain adaptation and learning from large amounts of real-world data without ground-truth source waveforms. In particular, we significantly improve reverberant speech separation performance by incorporating reverberant mixtures, train a speech enhancement system from noisy mixtures, and improve universal sound separation by incorporating a large amount of in-the-wild data.

409.
TITLE: Listen to What You Want: Neural Network-Based Universal Sound Selector
AUTHORS: T. Ochiai, et al.
YEAR: 2020 
SOURCE: arXiv
ABSTRACT: Being able to control the acoustic events (AEs) to which we want to listen would allow the development of more controllable hearable devices. This paper addresses the AE sound selection (or removal) problems, that we define as the extraction (or suppression) of all the sounds that belong to one or multiple desired AE classes. Although this problem could be addressed with a combination of source separation followed by AE classification, this is a sub-optimal way of solving the problem. Moreover, source separation usually requires knowing the maximum number of sources, which may not be practical when dealing with AEs. In this paper, we propose instead a universal sound selection neural network that enables to directly select AE sounds from a mixture given user-specified target AE classes. The proposed framework can be explicitly optimized to simultaneously select sounds from multiple desired AE classes, independently of the number of sources in the mixture. We experimentally show that the proposed method achieves promising AE sound selection performance and could be generalized to mixtures with a number of sources that are unseen during training.

410.
TITLE: Speech Separation Based on Multi-Stage Elaborated Dual-Path Deep BiLSTM with Auxiliary Identity Loss
AUTHORS: Z. Shi, R. Liu, J. Han
YEAR: 2020
SOURCE: arXiv
ABSTRACT: Deep neural network with dual-path bi-directional long short-term memory (BiLSTM) block has been proved to be very effective in sequence modeling, especially in speech separation. This work investigates how to extend dual-path BiLSTM to result in a new state-of-the-art approach, called TasTas, for multi-talker monaural speech separation (a.k.a cocktail party problem). TasTas introduces two simple but effective improvements, one is an iterative multi-stage refinement scheme, and the other is to correct the speech with imperfect separation through a loss of speaker identity consistency between the separated speech and original speech, to boost the performance of dual-path BiLSTM based networks. TasTas takes the mixed utterance of two speakers and maps it to two separated utterances, where each utterance contains only one speaker's voice. Our experiments on the notable benchmark WSJ0-2mix data corpus result in 20.55dB SDR improvement, 20.35dB SI-SDR improvement, 3.69 of PESQ, and 94.86\% of ESTOI, which shows that our proposed networks can lead to big performance improvement on the speaker separation task. We have open sourced our re-implementation of the DPRNN-TasNet here (this https URL), and our TasTas is realized based on this implementation of DPRNN-TasNet, it is believed that the results in this paper can be reproduced with ease.

411.
TITLE: Identify Speakers in Cocktail Parties with End-to-End Attention
AUTHORS: J. Zhu, M. Hasegawa-Johnson, L. Sari
YEAR: 2020
SOURCE: arXiv
ABSTRACT: In scenarios where multiple speakers talk at the same time, it is important to be able to identify the talkers accurately. This paper presents an end-to-end system that integrates speech source extraction and speaker identification, and proposes a new way to jointly optimize these two parts by max-pooling the speaker predictions along the channel dimension. Residual attention permits us to learn spectrogram masks that are optimized for the purpose of speaker identification, while residual forward connections permit dilated convolution with a sufficiently large context window to guarantee correct streaming across syllable boundaries. End-to-end training results in a system that recognizes one speaker in a two-speaker broadcast speech mixture with 99.9% accuracy and both speakers with 93.9% accuracy, and that recognizes all speakers in three-speaker scenarios with 81.2% accuracy.

412.
TITLE: Bio-Inspired Modality Fusion for Active Speaker Detection
AUTHORS: G. Assuncao, N. Goncalves, P. Menezes
YEAR: 2020
SOURCE: arXiv
ABSTRACT: Human beings have developed fantastic abilities to integrate information from various sensory sources exploring their inherent complementarity. Perceptual capabilities are therefore heightened enabling, for instance, the well known "cocktail party" and McGurk effects, i.e. speech disambiguation from a panoply of sound signals. This fusion ability is also key in refining the perception of sound source location, as in distinguishing whose voice is being heard in a group conversation. Furthermore, Neuroscience has successfully identified the superior colliculus region in the brain as the one responsible for this modality fusion, with a handful of biological models having been proposed to approach its underlying neurophysiological process. Deriving inspiration from one of these models, this paper presents a methodology for effectively fusing correlated auditory and visual information for active speaker detection. Such an ability can have a wide range of applications, from teleconferencing systems to social robotics. The detection approach initially routes auditory and visual information through two specialized neural network structures. The resulting embeddings are fused via a novel layer based on the superior colliculus, whose topological structure emulates spatial neuron cross-mapping of unimodal perceptual fields. The validation process employed two publicly available datasets, with achieved results confirming and greatly surpassing initial expectations.








